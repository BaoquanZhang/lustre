diff -rup --new-file linux.mcp2/fs/ext3/Makefile linux_tmp/fs/ext3/Makefile
--- linux.mcp2/fs/ext3/Makefile	1969-12-31 16:00:00.000000000 -0800
+++ linux_tmp/fs/ext3/Makefile	2001-12-21 09:41:55.000000000 -0800
@@ -0,0 +1,16 @@
+#
+# Makefile for the linux ext2-filesystem routines.
+#
+# Note! Dependencies are done automagically by 'make dep', which also
+# removes any old dependencies. DON'T put your own dependencies here
+# unless it's something special (ie not a .c file).
+#
+# Note 2! The CFLAGS definitions are now in the main makefile...
+
+O_TARGET := ext3.o
+
+obj-y    := balloc.o bitmap.o dir.o file.o fsync.o ialloc.o inode.o \
+		ioctl.o namei.o super.o symlink.o
+obj-m    := $(O_TARGET)
+
+include $(TOPDIR)/Rules.make
diff -rup --new-file linux.mcp2/fs/ext3/balloc.c linux_tmp/fs/ext3/balloc.c
--- linux.mcp2/fs/ext3/balloc.c	1969-12-31 16:00:00.000000000 -0800
+++ linux_tmp/fs/ext3/balloc.c	2002-08-02 17:39:45.000000000 -0700
@@ -0,0 +1,999 @@
+/*
+ *  linux/fs/ext3/balloc.c
+ *
+ * Copyright (C) 1992, 1993, 1994, 1995
+ * Remy Card (card@masi.ibp.fr)
+ * Laboratoire MASI - Institut Blaise Pascal
+ * Universite Pierre et Marie Curie (Paris VI)
+ *
+ *  Enhanced block allocation by Stephen Tweedie (sct@redhat.com), 1993
+ *  Big-endian to little-endian byte-swapping/bitmaps by
+ *        David S. Miller (davem@caip.rutgers.edu), 1995
+ */
+
+#include <linux/config.h>
+#include <linux/sched.h>
+#include <linux/fs.h>
+#include <linux/jbd.h>
+#include <linux/ext3_fs.h>
+#include <linux/ext3_jbd.h>
+#include <linux/locks.h>
+#include <linux/quotaops.h>
+
+/*
+ * balloc.c contains the blocks allocation and deallocation routines
+ */
+
+/*
+ * The free blocks are managed by bitmaps.  A file system contains several
+ * blocks groups.  Each group contains 1 bitmap block for blocks, 1 bitmap
+ * block for inodes, N blocks for the inode table and data blocks.
+ *
+ * The file system contains group descriptors which are located after the
+ * super block.  Each descriptor contains the number of the bitmap block and
+ * the free blocks count in the block.  The descriptors are loaded in memory
+ * when a file system is mounted (see ext3_read_super).
+ */
+
+
+#define in_range(b, first, len)	((b) >= (first) && (b) <= (first) + (len) - 1)
+
+struct ext3_group_desc * ext3_get_group_desc(struct super_block * sb,
+					     unsigned int block_group,
+					     struct buffer_head ** bh)
+{
+	unsigned long group_desc;
+	unsigned long desc;
+	struct ext3_group_desc * gdp;
+
+	if (block_group >= sb->u.ext3_sb.s_groups_count) {
+		ext3_error (sb, "ext3_get_group_desc",
+			    "block_group >= groups_count - "
+			    "block_group = %d, groups_count = %lu",
+			    block_group, sb->u.ext3_sb.s_groups_count);
+
+		return NULL;
+	}
+	
+	group_desc = block_group / EXT3_DESC_PER_BLOCK(sb);
+	desc = block_group % EXT3_DESC_PER_BLOCK(sb);
+	if (!sb->u.ext3_sb.s_group_desc[group_desc]) {
+		ext3_error (sb, "ext3_get_group_desc",
+			    "Group descriptor not loaded - "
+			    "block_group = %d, group_desc = %lu, desc = %lu",
+			     block_group, group_desc, desc);
+		return NULL;
+	}
+	
+	gdp = (struct ext3_group_desc *) 
+	      sb->u.ext3_sb.s_group_desc[group_desc]->b_data;
+	if (bh)
+		*bh = sb->u.ext3_sb.s_group_desc[group_desc];
+	return gdp + desc;
+}
+
+/*
+ * Read the bitmap for a given block_group, reading into the specified 
+ * slot in the superblock's bitmap cache.
+ *
+ * Return >=0 on success or a -ve error code.
+ */
+
+static int read_block_bitmap (struct super_block * sb,
+			       unsigned int block_group,
+			       unsigned long bitmap_nr)
+{
+	struct ext3_group_desc * gdp;
+	struct buffer_head * bh = NULL;
+	int retval = -EIO;
+	
+	gdp = ext3_get_group_desc (sb, block_group, NULL);
+	if (!gdp)
+		goto error_out;
+	retval = 0;
+	bh = sb_bread(sb, le32_to_cpu(gdp->bg_block_bitmap));
+	if (!bh) {
+		ext3_error (sb, "read_block_bitmap",
+			    "Cannot read block bitmap - "
+			    "block_group = %d, block_bitmap = %lu",
+			    block_group, (unsigned long) gdp->bg_block_bitmap);
+		retval = -EIO;
+	}
+	/*
+	 * On IO error, just leave a zero in the superblock's block pointer for
+	 * this group.  The IO will be retried next time.
+	 */
+error_out:
+	sb->u.ext3_sb.s_block_bitmap_number[bitmap_nr] = block_group;
+	sb->u.ext3_sb.s_block_bitmap[bitmap_nr] = bh;
+	return retval;
+}
+
+/*
+ * load_block_bitmap loads the block bitmap for a blocks group
+ *
+ * It maintains a cache for the last bitmaps loaded.  This cache is managed
+ * with a LRU algorithm.
+ *
+ * Notes:
+ * 1/ There is one cache per mounted file system.
+ * 2/ If the file system contains less than EXT3_MAX_GROUP_LOADED groups,
+ *    this function reads the bitmap without maintaining a LRU cache.
+ * 
+ * Return the slot used to store the bitmap, or a -ve error code.
+ */
+static int __load_block_bitmap (struct super_block * sb,
+			        unsigned int block_group)
+{
+	int i, j, retval = 0;
+	unsigned long block_bitmap_number;
+	struct buffer_head * block_bitmap;
+
+	if (block_group >= sb->u.ext3_sb.s_groups_count)
+		ext3_panic (sb, "load_block_bitmap",
+			    "block_group >= groups_count - "
+			    "block_group = %d, groups_count = %lu",
+			    block_group, sb->u.ext3_sb.s_groups_count);
+
+	if (sb->u.ext3_sb.s_groups_count <= EXT3_MAX_GROUP_LOADED) {
+		if (sb->u.ext3_sb.s_block_bitmap[block_group]) {
+			if (sb->u.ext3_sb.s_block_bitmap_number[block_group] ==
+			    block_group)
+				return block_group;
+			ext3_error (sb, "__load_block_bitmap",
+				    "block_group != block_bitmap_number");
+		}
+		retval = read_block_bitmap (sb, block_group, block_group);
+		if (retval < 0)
+			return retval;
+		return block_group;
+	}
+
+	for (i = 0; i < sb->u.ext3_sb.s_loaded_block_bitmaps &&
+		    sb->u.ext3_sb.s_block_bitmap_number[i] != block_group; i++)
+		;
+	if (i < sb->u.ext3_sb.s_loaded_block_bitmaps &&
+  	    sb->u.ext3_sb.s_block_bitmap_number[i] == block_group) {
+		block_bitmap_number = sb->u.ext3_sb.s_block_bitmap_number[i];
+		block_bitmap = sb->u.ext3_sb.s_block_bitmap[i];
+		for (j = i; j > 0; j--) {
+			sb->u.ext3_sb.s_block_bitmap_number[j] =
+				sb->u.ext3_sb.s_block_bitmap_number[j - 1];
+			sb->u.ext3_sb.s_block_bitmap[j] =
+				sb->u.ext3_sb.s_block_bitmap[j - 1];
+		}
+		sb->u.ext3_sb.s_block_bitmap_number[0] = block_bitmap_number;
+		sb->u.ext3_sb.s_block_bitmap[0] = block_bitmap;
+
+		/*
+		 * There's still one special case here --- if block_bitmap == 0
+		 * then our last attempt to read the bitmap failed and we have
+		 * just ended up caching that failure.  Try again to read it.
+		 */
+		if (!block_bitmap)
+			retval = read_block_bitmap (sb, block_group, 0);
+	} else {
+		if (sb->u.ext3_sb.s_loaded_block_bitmaps<EXT3_MAX_GROUP_LOADED)
+			sb->u.ext3_sb.s_loaded_block_bitmaps++;
+		else
+			brelse (sb->u.ext3_sb.s_block_bitmap
+					[EXT3_MAX_GROUP_LOADED - 1]);
+		for (j = sb->u.ext3_sb.s_loaded_block_bitmaps - 1;
+					j > 0;  j--) {
+			sb->u.ext3_sb.s_block_bitmap_number[j] =
+				sb->u.ext3_sb.s_block_bitmap_number[j - 1];
+			sb->u.ext3_sb.s_block_bitmap[j] =
+				sb->u.ext3_sb.s_block_bitmap[j - 1];
+		}
+		retval = read_block_bitmap (sb, block_group, 0);
+	}
+	return retval;
+}
+
+/*
+ * Load the block bitmap for a given block group.  First of all do a couple
+ * of fast lookups for common cases and then pass the request onto the guts
+ * of the bitmap loader.
+ *
+ * Return the slot number of the group in the superblock bitmap cache's on
+ * success, or a -ve error code.
+ *
+ * There is still one inconsistency here --- if the number of groups in this
+ * filesystems is <= EXT3_MAX_GROUP_LOADED, then we have no way of 
+ * differentiating between a group for which we have never performed a bitmap
+ * IO request, and a group for which the last bitmap read request failed.
+ */
+static inline int load_block_bitmap (struct super_block * sb,
+				     unsigned int block_group)
+{
+	int slot;
+	
+	/*
+	 * Do the lookup for the slot.  First of all, check if we're asking
+	 * for the same slot as last time, and did we succeed that last time?
+	 */
+	if (sb->u.ext3_sb.s_loaded_block_bitmaps > 0 &&
+	    sb->u.ext3_sb.s_block_bitmap_number[0] == block_group &&
+	    sb->u.ext3_sb.s_block_bitmap[0]) {
+		return 0;
+	}
+	/*
+	 * Or can we do a fast lookup based on a loaded group on a filesystem
+	 * small enough to be mapped directly into the superblock?
+	 */
+	else if (sb->u.ext3_sb.s_groups_count <= EXT3_MAX_GROUP_LOADED && 
+		 sb->u.ext3_sb.s_block_bitmap_number[block_group]==block_group
+			&& sb->u.ext3_sb.s_block_bitmap[block_group]) {
+		slot = block_group;
+	}
+	/*
+	 * If not, then do a full lookup for this block group.
+	 */
+	else {
+		slot = __load_block_bitmap (sb, block_group);
+	}
+
+	/*
+	 * <0 means we just got an error
+	 */
+	if (slot < 0)
+		return slot;
+	
+	/*
+	 * If it's a valid slot, we may still have cached a previous IO error,
+	 * in which case the bh in the superblock cache will be zero.
+	 */
+	if (!sb->u.ext3_sb.s_block_bitmap[slot])
+		return -EIO;
+	
+	/*
+	 * Must have been read in OK to get this far.
+	 */
+	return slot;
+}
+
+/* Free given blocks, update quota and i_blocks field */
+void ext3_free_blocks (handle_t *handle, struct inode * inode,
+			unsigned long block, unsigned long count)
+{
+	struct buffer_head *bitmap_bh;
+	struct buffer_head *gd_bh;
+	unsigned long block_group;
+	unsigned long bit;
+	unsigned long i;
+	int bitmap_nr;
+	unsigned long overflow;
+	struct super_block * sb;
+	struct ext3_group_desc * gdp;
+	struct ext3_super_block * es;
+	int err = 0, ret;
+	int dquot_freed_blocks = 0;
+
+	sb = inode->i_sb;
+	if (!sb) {
+		printk ("ext3_free_blocks: nonexistent device");
+		return;
+	}
+	lock_super (sb);
+	es = sb->u.ext3_sb.s_es;
+	if (block < le32_to_cpu(es->s_first_data_block) || 
+	    (block + count) > le32_to_cpu(es->s_blocks_count)) {
+		ext3_error (sb, "ext3_free_blocks",
+			    "Freeing blocks not in datazone - "
+			    "block = %lu, count = %lu", block, count);
+		goto error_return;
+	}
+
+	ext3_debug ("freeing block %lu\n", block);
+
+do_more:
+	overflow = 0;
+	block_group = (block - le32_to_cpu(es->s_first_data_block)) /
+		      EXT3_BLOCKS_PER_GROUP(sb);
+	bit = (block - le32_to_cpu(es->s_first_data_block)) %
+		      EXT3_BLOCKS_PER_GROUP(sb);
+	/*
+	 * Check to see if we are freeing blocks across a group
+	 * boundary.
+	 */
+	if (bit + count > EXT3_BLOCKS_PER_GROUP(sb)) {
+		overflow = bit + count - EXT3_BLOCKS_PER_GROUP(sb);
+		count -= overflow;
+	}
+	bitmap_nr = load_block_bitmap (sb, block_group);
+	if (bitmap_nr < 0)
+		goto error_return;
+	
+	bitmap_bh = sb->u.ext3_sb.s_block_bitmap[bitmap_nr];
+	gdp = ext3_get_group_desc (sb, block_group, &gd_bh);
+	if (!gdp)
+		goto error_return;
+
+	if (in_range (le32_to_cpu(gdp->bg_block_bitmap), block, count) ||
+	    in_range (le32_to_cpu(gdp->bg_inode_bitmap), block, count) ||
+	    in_range (block, le32_to_cpu(gdp->bg_inode_table),
+		      sb->u.ext3_sb.s_itb_per_group) ||
+	    in_range (block + count - 1, le32_to_cpu(gdp->bg_inode_table),
+		      sb->u.ext3_sb.s_itb_per_group))
+		ext3_error (sb, "ext3_free_blocks",
+			    "Freeing blocks in system zones - "
+			    "Block = %lu, count = %lu",
+			    block, count);
+
+	/*
+	 * We are about to start releasing blocks in the bitmap,
+	 * so we need undo access.
+	 */
+	/* @@@ check errors */
+	BUFFER_TRACE(bitmap_bh, "getting undo access");
+	err = ext3_journal_get_undo_access(handle, bitmap_bh);
+	if (err)
+		goto error_return;
+	
+	/*
+	 * We are about to modify some metadata.  Call the journal APIs
+	 * to unshare ->b_data if a currently-committing transaction is
+	 * using it
+	 */
+	BUFFER_TRACE(gd_bh, "get_write_access");
+	err = ext3_journal_get_write_access(handle, gd_bh);	
+	if (err)
+		goto error_return;
+
+	BUFFER_TRACE(sb->u.ext3_sb.s_sbh, "get_write_access");
+	err = ext3_journal_get_write_access(handle, sb->u.ext3_sb.s_sbh);
+	if (err)
+		goto error_return;
+
+	for (i = 0; i < count; i++) {
+		/*
+		 * An HJ special.  This is expensive...
+		 */
+#ifdef CONFIG_JBD_DEBUG
+		{
+			struct buffer_head *debug_bh;
+			debug_bh = sb_get_hash_table(sb, block + i);
+			if (debug_bh) {
+				BUFFER_TRACE(debug_bh, "Deleted!");
+				if (!bh2jh(bitmap_bh)->b_committed_data)
+					BUFFER_TRACE(debug_bh,
+						"No commited data in bitmap");
+				BUFFER_TRACE2(debug_bh, bitmap_bh, "bitmap");
+				__brelse(debug_bh);
+			}
+		}
+#endif
+		BUFFER_TRACE(bitmap_bh, "clear bit");
+		if (!ext3_clear_bit (bit + i, bitmap_bh->b_data)) {
+			ext3_error (sb, __FUNCTION__,
+				      "bit already cleared for block %lu", 
+				      block + i);
+			BUFFER_TRACE(bitmap_bh, "bit already cleared");
+		} else {
+			dquot_freed_blocks++;
+			gdp->bg_free_blocks_count =
+			  cpu_to_le16(le16_to_cpu(gdp->bg_free_blocks_count)+1);
+			es->s_free_blocks_count =
+			  cpu_to_le32(le32_to_cpu(es->s_free_blocks_count)+1);
+		}
+		/* @@@ This prevents newly-allocated data from being
+		 * freed and then reallocated within the same
+		 * transaction. 
+		 * 
+		 * Ideally we would want to allow that to happen, but to
+		 * do so requires making journal_forget() capable of
+		 * revoking the queued write of a data block, which
+		 * implies blocking on the journal lock.  *forget()
+		 * cannot block due to truncate races.
+		 *
+		 * Eventually we can fix this by making journal_forget()
+		 * return a status indicating whether or not it was able
+		 * to revoke the buffer.  On successful revoke, it is
+		 * safe not to set the allocation bit in the committed
+		 * bitmap, because we know that there is no outstanding
+		 * activity on the buffer any more and so it is safe to
+		 * reallocate it.  
+		 */
+		BUFFER_TRACE(bitmap_bh, "clear in b_committed_data");
+		J_ASSERT_BH(bitmap_bh,
+				bh2jh(bitmap_bh)->b_committed_data != NULL);
+		ext3_set_bit(bit + i, bh2jh(bitmap_bh)->b_committed_data);
+	}
+
+	/* We dirtied the bitmap block */
+	BUFFER_TRACE(bitmap_bh, "dirtied bitmap block");
+	err = ext3_journal_dirty_metadata(handle, bitmap_bh);
+
+	/* And the group descriptor block */
+	BUFFER_TRACE(gd_bh, "dirtied group descriptor block");
+	ret = ext3_journal_dirty_metadata(handle, gd_bh);
+	if (!err) err = ret;
+
+	/* And the superblock */
+	BUFFER_TRACE(sb->u.ext3_sb.s_sbh, "dirtied superblock");
+	ret = ext3_journal_dirty_metadata(handle, sb->u.ext3_sb.s_sbh);
+	if (!err) err = ret;
+
+	if (overflow && !err) {
+		block += count;
+		count = overflow;
+		goto do_more;
+	}
+	sb->s_dirt = 1;
+error_return:
+	ext3_std_error(sb, err);
+	unlock_super(sb);
+	if (dquot_freed_blocks)
+		DQUOT_FREE_BLOCK(inode, dquot_freed_blocks);
+	return;
+}
+
+/* For ext3 allocations, we must not reuse any blocks which are
+ * allocated in the bitmap buffer's "last committed data" copy.  This
+ * prevents deletes from freeing up the page for reuse until we have
+ * committed the delete transaction.
+ *
+ * If we didn't do this, then deleting something and reallocating it as
+ * data would allow the old block to be overwritten before the
+ * transaction committed (because we force data to disk before commit).
+ * This would lead to corruption if we crashed between overwriting the
+ * data and committing the delete. 
+ *
+ * @@@ We may want to make this allocation behaviour conditional on
+ * data-writes at some point, and disable it for metadata allocations or
+ * sync-data inodes.
+ */
+static int ext3_test_allocatable(int nr, struct buffer_head *bh)
+{
+	if (ext3_test_bit(nr, bh->b_data))
+		return 0;
+	if (!buffer_jbd(bh) || !bh2jh(bh)->b_committed_data)
+		return 1;
+	return !ext3_test_bit(nr, bh2jh(bh)->b_committed_data);
+}
+
+/*
+ * Find an allocatable block in a bitmap.  We honour both the bitmap and
+ * its last-committed copy (if that exists), and perform the "most
+ * appropriate allocation" algorithm of looking for a free block near
+ * the initial goal; then for a free byte somewhere in the bitmap; then
+ * for any free bit in the bitmap.
+ */
+static int find_next_usable_block(int start,
+			struct buffer_head *bh, int maxblocks)
+{
+	int here, next;
+	char *p, *r;
+	
+	if (start > 0) {
+		/*
+		 * The goal was occupied; search forward for a free 
+		 * block within the next XX blocks.
+		 *
+		 * end_goal is more or less random, but it has to be
+		 * less than EXT3_BLOCKS_PER_GROUP. Aligning up to the
+		 * next 64-bit boundary is simple..
+		 */
+		int end_goal = (start + 63) & ~63;
+		here = ext3_find_next_zero_bit(bh->b_data, end_goal, start);
+		if (here < end_goal && ext3_test_allocatable(here, bh))
+			return here;
+		
+		ext3_debug ("Bit not found near goal\n");
+		
+	}
+	
+	here = start;
+	if (here < 0)
+		here = 0;
+	
+	/*
+	 * There has been no free block found in the near vicinity of
+	 * the goal: do a search forward through the block groups,
+	 * searching in each group first for an entire free byte in the
+	 * bitmap and then for any free bit.
+	 * 
+	 * Search first in the remainder of the current group 
+	 */
+	p = ((char *) bh->b_data) + (here >> 3);
+	r = memscan(p, 0, (maxblocks - here + 7) >> 3);
+	next = (r - ((char *) bh->b_data)) << 3;
+	
+	if (next < maxblocks && ext3_test_allocatable(next, bh))
+		return next;
+	
+	/* The bitmap search --- search forward alternately
+	 * through the actual bitmap and the last-committed copy
+	 * until we find a bit free in both. */
+
+	while (here < maxblocks) {
+		next  = ext3_find_next_zero_bit ((unsigned long *) bh->b_data, 
+						 maxblocks, here);
+		if (next >= maxblocks)
+			return -1;
+		if (ext3_test_allocatable(next, bh))
+			return next;
+
+		J_ASSERT_BH(bh, bh2jh(bh)->b_committed_data);
+		here = ext3_find_next_zero_bit
+			((unsigned long *) bh2jh(bh)->b_committed_data, 
+			 maxblocks, next);
+	}
+	return -1;
+}
+
+/*
+ * ext3_new_block uses a goal block to assist allocation.  If the goal is
+ * free, or there is a free block within 32 blocks of the goal, that block
+ * is allocated.  Otherwise a forward search is made for a free block; within 
+ * each block group the search first looks for an entire free byte in the block
+ * bitmap, and then for any free bit if that fails.
+ * This function also updates quota and i_blocks field.
+ */
+int ext3_new_block (handle_t *handle, struct inode * inode,
+		unsigned long goal, u32 * prealloc_count,
+		u32 * prealloc_block, int * errp)
+{
+	struct buffer_head * bh, *bhtmp;
+	struct buffer_head * bh2;
+#if 0
+	char * p, * r;
+#endif
+	int i, j, k, tmp, alloctmp;
+	int bitmap_nr;
+	int fatal = 0, err;
+	int performed_allocation = 0;
+	struct super_block * sb;
+	struct ext3_group_desc * gdp;
+	struct ext3_super_block * es;
+#ifdef EXT3FS_DEBUG
+	static int goal_hits = 0, goal_attempts = 0;
+#endif
+	*errp = -ENOSPC;
+	sb = inode->i_sb;
+	if (!sb) {
+		printk ("ext3_new_block: nonexistent device");
+		return 0;
+	}
+
+	/*
+	 * Check quota for allocation of this block.
+	 */
+	if (DQUOT_ALLOC_BLOCK(inode, 1)) {
+		*errp = -EDQUOT;
+		return 0;
+	}
+
+	lock_super (sb);
+	es = sb->u.ext3_sb.s_es;
+	if (le32_to_cpu(es->s_free_blocks_count) <=
+			le32_to_cpu(es->s_r_blocks_count) &&
+	    ((sb->u.ext3_sb.s_resuid != current->fsuid) &&
+	     (sb->u.ext3_sb.s_resgid == 0 ||
+	      !in_group_p (sb->u.ext3_sb.s_resgid)) && 
+	     !capable(CAP_SYS_RESOURCE)))
+		goto out;
+
+	ext3_debug ("goal=%lu.\n", goal);
+
+	/*
+	 * First, test whether the goal block is free.
+	 */
+	if (goal < le32_to_cpu(es->s_first_data_block) ||
+	    goal >= le32_to_cpu(es->s_blocks_count))
+		goal = le32_to_cpu(es->s_first_data_block);
+	i = (goal - le32_to_cpu(es->s_first_data_block)) /
+			EXT3_BLOCKS_PER_GROUP(sb);
+	gdp = ext3_get_group_desc (sb, i, &bh2);
+	if (!gdp)
+		goto io_error;
+
+	if (le16_to_cpu(gdp->bg_free_blocks_count) > 0) {
+		j = ((goal - le32_to_cpu(es->s_first_data_block)) %
+				EXT3_BLOCKS_PER_GROUP(sb));
+#ifdef EXT3FS_DEBUG
+		if (j)
+			goal_attempts++;
+#endif
+		bitmap_nr = load_block_bitmap (sb, i);
+		if (bitmap_nr < 0)
+			goto io_error;
+		
+		bh = sb->u.ext3_sb.s_block_bitmap[bitmap_nr];
+
+		ext3_debug ("goal is at %d:%d.\n", i, j);
+
+		if (ext3_test_allocatable(j, bh)) {
+#ifdef EXT3FS_DEBUG
+			goal_hits++;
+			ext3_debug ("goal bit allocated.\n");
+#endif
+			goto got_block;
+		}
+
+		j = find_next_usable_block(j, bh, EXT3_BLOCKS_PER_GROUP(sb));
+		if (j >= 0)
+			goto search_back;
+	}
+
+	ext3_debug ("Bit not found in block group %d.\n", i);
+
+	/*
+	 * Now search the rest of the groups.  We assume that 
+	 * i and gdp correctly point to the last group visited.
+	 */
+	for (k = 0; k < sb->u.ext3_sb.s_groups_count; k++) {
+		i++;
+		if (i >= sb->u.ext3_sb.s_groups_count)
+			i = 0;
+		gdp = ext3_get_group_desc (sb, i, &bh2);
+		if (!gdp) {
+			*errp = -EIO;
+			goto out;
+		}
+		if (le16_to_cpu(gdp->bg_free_blocks_count) > 0) {
+			bitmap_nr = load_block_bitmap (sb, i);
+			if (bitmap_nr < 0)
+				goto io_error;
+	
+			bh = sb->u.ext3_sb.s_block_bitmap[bitmap_nr];
+			j = find_next_usable_block(-1, bh, 
+						   EXT3_BLOCKS_PER_GROUP(sb));
+			if (j >= 0) 
+				goto search_back;
+		}
+	}
+
+	/* No space left on the device */
+	goto out;
+
+search_back:
+	/* 
+	 * We have succeeded in finding a free byte in the block
+	 * bitmap.  Now search backwards up to 7 bits to find the
+	 * start of this group of free blocks.
+	 */
+	for (	k = 0;
+		k < 7 && j > 0 && ext3_test_allocatable(j - 1, bh);
+		k++, j--)
+		;
+	
+got_block:
+
+	ext3_debug ("using block group %d(%d)\n", i, gdp->bg_free_blocks_count);
+
+	/* Make sure we use undo access for the bitmap, because it is
+           critical that we do the frozen_data COW on bitmap buffers in
+           all cases even if the buffer is in BJ_Forget state in the
+           committing transaction.  */
+	BUFFER_TRACE(bh, "get undo access for marking new block");
+	fatal = ext3_journal_get_undo_access(handle, bh);
+	if (fatal) goto out;
+	
+	BUFFER_TRACE(bh2, "get_write_access");
+	fatal = ext3_journal_get_write_access(handle, bh2);
+	if (fatal) goto out;
+
+	BUFFER_TRACE(sb->u.ext3_sb.s_sbh, "get_write_access");
+	fatal = ext3_journal_get_write_access(handle, sb->u.ext3_sb.s_sbh);
+	if (fatal) goto out;
+
+	tmp = j + i * EXT3_BLOCKS_PER_GROUP(sb)
+				+ le32_to_cpu(es->s_first_data_block);
+
+	if (tmp == le32_to_cpu(gdp->bg_block_bitmap) ||
+	    tmp == le32_to_cpu(gdp->bg_inode_bitmap) ||
+	    in_range (tmp, le32_to_cpu(gdp->bg_inode_table),
+		      sb->u.ext3_sb.s_itb_per_group))
+		ext3_error (sb, "ext3_new_block",
+			    "Allocating block in system zone - "
+			    "block = %u", tmp);
+
+	/* The superblock lock should guard against anybody else beating
+	 * us to this point! */
+	J_ASSERT_BH(bh, !ext3_test_bit(j, bh->b_data));
+	BUFFER_TRACE(bh, "setting bitmap bit");
+	ext3_set_bit(j, bh->b_data);
+	performed_allocation = 1;
+
+#ifdef CONFIG_JBD_DEBUG
+	{
+		struct buffer_head *debug_bh;
+
+		/* Record bitmap buffer state in the newly allocated block */
+		debug_bh = sb_get_hash_table(sb, tmp);
+		if (debug_bh) {
+			BUFFER_TRACE(debug_bh, "state when allocated");
+			BUFFER_TRACE2(debug_bh, bh, "bitmap state");
+			brelse(debug_bh);
+		}
+	}
+#endif
+	if (buffer_jbd(bh) && bh2jh(bh)->b_committed_data)
+		J_ASSERT_BH(bh, !ext3_test_bit(j, bh2jh(bh)->b_committed_data));
+	bhtmp = bh;
+	alloctmp = j;
+
+	ext3_debug ("found bit %d\n", j);
+
+	/*
+	 * Do block preallocation now if required.
+	 */
+#ifdef EXT3_PREALLOCATE
+	/*
+	 * akpm: this is not enabled for ext3.  Need to use
+	 * ext3_test_allocatable()
+	 */
+	/* Writer: ->i_prealloc* */
+	if (prealloc_count && !*prealloc_count) {
+		int	prealloc_goal;
+		unsigned long next_block = tmp + 1;
+
+		prealloc_goal = es->s_prealloc_blocks ?
+			es->s_prealloc_blocks : EXT3_DEFAULT_PREALLOC_BLOCKS;
+
+		*prealloc_block = next_block;
+		/* Writer: end */
+		for (k = 1;
+		     k < prealloc_goal && (j + k) < EXT3_BLOCKS_PER_GROUP(sb);
+		     k++, next_block++) {
+			if (DQUOT_PREALLOC_BLOCK(inode, 1))
+				break;
+			/* Writer: ->i_prealloc* */
+			if (*prealloc_block + *prealloc_count != next_block ||
+			    ext3_set_bit (j + k, bh->b_data)) {
+				/* Writer: end */
+				DQUOT_FREE_BLOCK(inode, 1);
+ 				break;
+			}
+			(*prealloc_count)++;
+			/* Writer: end */
+		}	
+		/*
+		 * As soon as we go for per-group spinlocks we'll need these
+		 * done inside the loop above.
+		 */
+		gdp->bg_free_blocks_count =
+			cpu_to_le16(le16_to_cpu(gdp->bg_free_blocks_count) -
+			       (k - 1));
+		es->s_free_blocks_count =
+			cpu_to_le32(le32_to_cpu(es->s_free_blocks_count) -
+			       (k - 1));
+		ext3_debug ("Preallocated a further %lu bits.\n",
+			       (k - 1));
+	}
+#endif
+
+	j = tmp;
+
+	BUFFER_TRACE(bh, "journal_dirty_metadata for bitmap block");
+	err = ext3_journal_dirty_metadata(handle, bh);
+	if (!fatal) fatal = err;
+	
+	if (j >= le32_to_cpu(es->s_blocks_count)) {
+		ext3_error (sb, "ext3_new_block",
+			    "block(%d) >= blocks count(%d) - "
+			    "block_group = %d, es == %p ",j,
+			le32_to_cpu(es->s_blocks_count), i, es);
+		goto out;
+	}
+
+	/*
+	 * It is up to the caller to add the new buffer to a journal
+	 * list of some description.  We don't know in advance whether
+	 * the caller wants to use it as metadata or data.
+	 */
+
+	ext3_debug ("allocating block %d. "
+		    "Goal hits %d of %d.\n", j, goal_hits, goal_attempts);
+
+	gdp->bg_free_blocks_count =
+			cpu_to_le16(le16_to_cpu(gdp->bg_free_blocks_count) - 1);
+	es->s_free_blocks_count =
+			cpu_to_le32(le32_to_cpu(es->s_free_blocks_count) - 1);
+
+	BUFFER_TRACE(bh2, "journal_dirty_metadata for group descriptor");
+	err = ext3_journal_dirty_metadata(handle, bh2);
+	if (!fatal) fatal = err;
+	
+	BUFFER_TRACE(bh, "journal_dirty_metadata for superblock");
+	err = ext3_journal_dirty_metadata(handle, sb->u.ext3_sb.s_sbh);
+	if (!fatal) fatal = err;
+
+	sb->s_dirt = 1;
+	if (fatal)
+		goto out;
+
+	unlock_super (sb);
+	*errp = 0;
+	return j;
+	
+io_error:
+	*errp = -EIO;
+out:
+	if (fatal) {
+		*errp = fatal;
+		ext3_std_error(sb, fatal);
+	}
+	unlock_super (sb);
+	/*
+	 * Undo the block allocation
+	 */
+	if (!performed_allocation)
+		DQUOT_FREE_BLOCK(inode, 1);
+	return 0;
+	
+}
+
+unsigned long ext3_count_free_blocks (struct super_block * sb)
+{
+#ifdef EXT3FS_DEBUG
+	struct ext3_super_block * es;
+	unsigned long desc_count, bitmap_count, x;
+	int bitmap_nr;
+	struct ext3_group_desc * gdp;
+	int i;
+	
+	lock_super (sb);
+	es = sb->u.ext3_sb.s_es;
+	desc_count = 0;
+	bitmap_count = 0;
+	gdp = NULL;
+	for (i = 0; i < sb->u.ext3_sb.s_groups_count; i++) {
+		gdp = ext3_get_group_desc (sb, i, NULL);
+		if (!gdp)
+			continue;
+		desc_count += le16_to_cpu(gdp->bg_free_blocks_count);
+		bitmap_nr = load_block_bitmap (sb, i);
+		if (bitmap_nr < 0)
+			continue;
+		
+		x = ext3_count_free (sb->u.ext3_sb.s_block_bitmap[bitmap_nr],
+				     sb->s_blocksize);
+		printk ("group %d: stored = %d, counted = %lu\n",
+			i, le16_to_cpu(gdp->bg_free_blocks_count), x);
+		bitmap_count += x;
+	}
+	printk("ext3_count_free_blocks: stored = %lu, computed = %lu, %lu\n",
+	       le32_to_cpu(es->s_free_blocks_count), desc_count, bitmap_count);
+	unlock_super (sb);
+	return bitmap_count;
+#else
+	return le32_to_cpu(sb->u.ext3_sb.s_es->s_free_blocks_count);
+#endif
+}
+
+static inline int block_in_use (unsigned long block,
+				struct super_block * sb,
+				unsigned char * map)
+{
+	return ext3_test_bit ((block -
+		le32_to_cpu(sb->u.ext3_sb.s_es->s_first_data_block)) %
+			 EXT3_BLOCKS_PER_GROUP(sb), map);
+}
+
+static inline int test_root(int a, int b)
+{
+	if (a == 0)
+		return 1;
+	while (1) {
+		if (a == 1)
+			return 1;
+		if (a % b)
+			return 0;
+		a = a / b;
+	}
+}
+
+int ext3_group_sparse(int group)
+{
+	return (test_root(group, 3) || test_root(group, 5) ||
+		test_root(group, 7));
+}
+
+/**
+ *	ext3_bg_has_super - number of blocks used by the superblock in group
+ *	@sb: superblock for filesystem
+ *	@group: group number to check
+ *
+ *	Return the number of blocks used by the superblock (primary or backup)
+ *	in this group.  Currently this will be only 0 or 1.
+ */
+int ext3_bg_has_super(struct super_block *sb, int group)
+{
+	if (EXT3_HAS_RO_COMPAT_FEATURE(sb,EXT3_FEATURE_RO_COMPAT_SPARSE_SUPER)&&
+	    !ext3_group_sparse(group))
+		return 0;
+	return 1;
+}
+
+/**
+ *	ext3_bg_num_gdb - number of blocks used by the group table in group
+ *	@sb: superblock for filesystem
+ *	@group: group number to check
+ *
+ *	Return the number of blocks used by the group descriptor table
+ *	(primary or backup) in this group.  In the future there may be a
+ *	different number of descriptor blocks in each group.
+ */
+unsigned long ext3_bg_num_gdb(struct super_block *sb, int group)
+{
+	if (EXT3_HAS_RO_COMPAT_FEATURE(sb,EXT3_FEATURE_RO_COMPAT_SPARSE_SUPER)&&
+	    !ext3_group_sparse(group))
+		return 0;
+	return EXT3_SB(sb)->s_gdb_count;
+}
+
+#ifdef CONFIG_EXT3_CHECK
+/* Called at mount-time, super-block is locked */
+void ext3_check_blocks_bitmap (struct super_block * sb)
+{
+	struct buffer_head * bh;
+	struct ext3_super_block * es;
+	unsigned long desc_count, bitmap_count, x, j;
+	unsigned long desc_blocks;
+	int bitmap_nr;
+	struct ext3_group_desc * gdp;
+	int i;
+
+	es = sb->u.ext3_sb.s_es;
+	desc_count = 0;
+	bitmap_count = 0;
+	gdp = NULL;
+	for (i = 0; i < sb->u.ext3_sb.s_groups_count; i++) {
+		gdp = ext3_get_group_desc (sb, i, NULL);
+		if (!gdp)
+			continue;
+		desc_count += le16_to_cpu(gdp->bg_free_blocks_count);
+		bitmap_nr = load_block_bitmap (sb, i);
+		if (bitmap_nr < 0)
+			continue;
+
+		bh = EXT3_SB(sb)->s_block_bitmap[bitmap_nr];
+
+		if (ext3_bg_has_super(sb, i) && !ext3_test_bit(0, bh->b_data))
+			ext3_error(sb, __FUNCTION__,
+				   "Superblock in group %d is marked free", i);
+
+		desc_blocks = ext3_bg_num_gdb(sb, i);
+		for (j = 0; j < desc_blocks; j++)
+			if (!ext3_test_bit(j + 1, bh->b_data))
+				ext3_error(sb, __FUNCTION__,
+					   "Descriptor block #%ld in group "
+					   "%d is marked free", j, i);
+
+		if (!block_in_use (le32_to_cpu(gdp->bg_block_bitmap),
+						sb, bh->b_data))
+			ext3_error (sb, "ext3_check_blocks_bitmap",
+				    "Block bitmap for group %d is marked free",
+				    i);
+
+		if (!block_in_use (le32_to_cpu(gdp->bg_inode_bitmap),
+						sb, bh->b_data))
+			ext3_error (sb, "ext3_check_blocks_bitmap",
+				    "Inode bitmap for group %d is marked free",
+				    i);
+
+		for (j = 0; j < sb->u.ext3_sb.s_itb_per_group; j++)
+			if (!block_in_use (le32_to_cpu(gdp->bg_inode_table) + j,
+							sb, bh->b_data))
+				ext3_error (sb, "ext3_check_blocks_bitmap",
+					    "Block #%d of the inode table in "
+					    "group %d is marked free", j, i);
+
+		x = ext3_count_free (bh, sb->s_blocksize);
+		if (le16_to_cpu(gdp->bg_free_blocks_count) != x)
+			ext3_error (sb, "ext3_check_blocks_bitmap",
+				    "Wrong free blocks count for group %d, "
+				    "stored = %d, counted = %lu", i,
+				    le16_to_cpu(gdp->bg_free_blocks_count), x);
+		bitmap_count += x;
+	}
+	if (le32_to_cpu(es->s_free_blocks_count) != bitmap_count)
+		ext3_error (sb, "ext3_check_blocks_bitmap",
+			"Wrong free blocks count in super block, "
+			"stored = %lu, counted = %lu",
+			(unsigned long)le32_to_cpu(es->s_free_blocks_count),
+			bitmap_count);
+}
+#endif
diff -rup --new-file linux.mcp2/fs/ext3/bitmap.c linux_tmp/fs/ext3/bitmap.c
--- linux.mcp2/fs/ext3/bitmap.c	1969-12-31 16:00:00.000000000 -0800
+++ linux_tmp/fs/ext3/bitmap.c	2001-11-09 14:25:04.000000000 -0800
@@ -0,0 +1,26 @@
+/*
+ *  linux/fs/ext3/bitmap.c
+ *
+ * Copyright (C) 1992, 1993, 1994, 1995
+ * Remy Card (card@masi.ibp.fr)
+ * Laboratoire MASI - Institut Blaise Pascal
+ * Universite Pierre et Marie Curie (Paris VI)
+ */
+
+#include <linux/fs.h>
+
+
+static int nibblemap[] = {4, 3, 3, 2, 3, 2, 2, 1, 3, 2, 2, 1, 2, 1, 1, 0};
+
+unsigned long ext3_count_free (struct buffer_head * map, unsigned int numchars)
+{
+	unsigned int i;
+	unsigned long sum = 0;
+	
+	if (!map) 
+		return (0);
+	for (i = 0; i < numchars; i++)
+		sum += nibblemap[map->b_data[i] & 0xf] +
+			nibblemap[(map->b_data[i] >> 4) & 0xf];
+	return (sum);
+}
diff -rup --new-file linux.mcp2/fs/ext3/dir.c linux_tmp/fs/ext3/dir.c
--- linux.mcp2/fs/ext3/dir.c	1969-12-31 16:00:00.000000000 -0800
+++ linux_tmp/fs/ext3/dir.c	2001-11-09 14:25:04.000000000 -0800
@@ -0,0 +1,190 @@
+/*
+ *  linux/fs/ext3/dir.c
+ *
+ * Copyright (C) 1992, 1993, 1994, 1995
+ * Remy Card (card@masi.ibp.fr)
+ * Laboratoire MASI - Institut Blaise Pascal
+ * Universite Pierre et Marie Curie (Paris VI)
+ *
+ *  from
+ *
+ *  linux/fs/minix/dir.c
+ *
+ *  Copyright (C) 1991, 1992  Linus Torvalds
+ *
+ *  ext3 directory handling functions
+ *
+ *  Big-endian to little-endian byte-swapping/bitmaps by
+ *        David S. Miller (davem@caip.rutgers.edu), 1995
+ */
+
+#include <linux/fs.h>
+#include <linux/jbd.h>
+#include <linux/ext3_fs.h>
+
+static unsigned char ext3_filetype_table[] = {
+	DT_UNKNOWN, DT_REG, DT_DIR, DT_CHR, DT_BLK, DT_FIFO, DT_SOCK, DT_LNK
+};
+
+static int ext3_readdir(struct file *, void *, filldir_t);
+
+struct file_operations ext3_dir_operations = {
+	read:		generic_read_dir,
+	readdir:	ext3_readdir,		/* BKL held */
+	ioctl:		ext3_ioctl,		/* BKL held */
+	fsync:		ext3_sync_file,		/* BKL held */
+};
+
+int ext3_check_dir_entry (const char * function, struct inode * dir,
+			  struct ext3_dir_entry_2 * de,
+			  struct buffer_head * bh,
+			  unsigned long offset)
+{
+	const char * error_msg = NULL;
+ 	const int rlen = le16_to_cpu(de->rec_len);
+
+	if (rlen < EXT3_DIR_REC_LEN(1))
+		error_msg = "rec_len is smaller than minimal";
+	else if (rlen % 4 != 0)
+		error_msg = "rec_len % 4 != 0";
+	else if (rlen < EXT3_DIR_REC_LEN(de->name_len))
+		error_msg = "rec_len is too small for name_len";
+	else if (((char *) de - bh->b_data) + rlen > dir->i_sb->s_blocksize)
+		error_msg = "directory entry across blocks";
+	else if (le32_to_cpu(de->inode) >
+			le32_to_cpu(dir->i_sb->u.ext3_sb.s_es->s_inodes_count))
+		error_msg = "inode out of bounds";
+
+	if (error_msg != NULL)
+		ext3_error (dir->i_sb, function,
+			"bad entry in directory #%lu: %s - "
+			"offset=%lu, inode=%lu, rec_len=%d, name_len=%d",
+			dir->i_ino, error_msg, offset,
+			(unsigned long) le32_to_cpu(de->inode),
+			rlen, de->name_len);
+	return error_msg == NULL ? 1 : 0;
+}
+
+static int ext3_readdir(struct file * filp,
+			 void * dirent, filldir_t filldir)
+{
+	int error = 0;
+	unsigned long offset, blk;
+	int i, num, stored;
+	struct buffer_head * bh, * tmp, * bha[16];
+	struct ext3_dir_entry_2 * de;
+	struct super_block * sb;
+	int err;
+	struct inode *inode = filp->f_dentry->d_inode;
+
+	sb = inode->i_sb;
+
+	stored = 0;
+	bh = NULL;
+	offset = filp->f_pos & (sb->s_blocksize - 1);
+
+	while (!error && !stored && filp->f_pos < inode->i_size) {
+		blk = (filp->f_pos) >> EXT3_BLOCK_SIZE_BITS(sb);
+		bh = ext3_bread (0, inode, blk, 0, &err);
+		if (!bh) {
+			ext3_error (sb, "ext3_readdir",
+				"directory #%lu contains a hole at offset %lu",
+				inode->i_ino, (unsigned long)filp->f_pos);
+			filp->f_pos += sb->s_blocksize - offset;
+			continue;
+		}
+
+		/*
+		 * Do the readahead
+		 */
+		if (!offset) {
+			for (i = 16 >> (EXT3_BLOCK_SIZE_BITS(sb) - 9), num = 0;
+			     i > 0; i--) {
+				tmp = ext3_getblk (NULL, inode, ++blk, 0, &err);
+				if (tmp && !buffer_uptodate(tmp) &&
+						!buffer_locked(tmp))
+					bha[num++] = tmp;
+				else
+					brelse (tmp);
+			}
+			if (num) {
+				ll_rw_block (READA, num, bha);
+				for (i = 0; i < num; i++)
+					brelse (bha[i]);
+			}
+		}
+		
+revalidate:
+		/* If the dir block has changed since the last call to
+		 * readdir(2), then we might be pointing to an invalid
+		 * dirent right now.  Scan from the start of the block
+		 * to make sure. */
+		if (filp->f_version != inode->i_version) {
+			for (i = 0; i < sb->s_blocksize && i < offset; ) {
+				de = (struct ext3_dir_entry_2 *) 
+					(bh->b_data + i);
+				/* It's too expensive to do a full
+				 * dirent test each time round this
+				 * loop, but we do have to test at
+				 * least that it is non-zero.  A
+				 * failure will be detected in the
+				 * dirent test below. */
+				if (le16_to_cpu(de->rec_len) <
+						EXT3_DIR_REC_LEN(1))
+					break;
+				i += le16_to_cpu(de->rec_len);
+			}
+			offset = i;
+			filp->f_pos = (filp->f_pos & ~(sb->s_blocksize - 1))
+				| offset;
+			filp->f_version = inode->i_version;
+		}
+		
+		while (!error && filp->f_pos < inode->i_size 
+		       && offset < sb->s_blocksize) {
+			de = (struct ext3_dir_entry_2 *) (bh->b_data + offset);
+			if (!ext3_check_dir_entry ("ext3_readdir", inode, de,
+						   bh, offset)) {
+				/* On error, skip the f_pos to the
+                                   next block. */
+				filp->f_pos = (filp->f_pos |
+						(sb->s_blocksize - 1)) + 1;
+				brelse (bh);
+				return stored;
+			}
+			offset += le16_to_cpu(de->rec_len);
+			if (le32_to_cpu(de->inode)) {
+				/* We might block in the next section
+				 * if the data destination is
+				 * currently swapped out.  So, use a
+				 * version stamp to detect whether or
+				 * not the directory has been modified
+				 * during the copy operation.
+				 */
+				unsigned long version = filp->f_version;
+				unsigned char d_type = DT_UNKNOWN;
+
+				if (EXT3_HAS_INCOMPAT_FEATURE(sb,
+						EXT3_FEATURE_INCOMPAT_FILETYPE)
+						&& de->file_type < EXT3_FT_MAX)
+					d_type =
+					  ext3_filetype_table[de->file_type];
+				error = filldir(dirent, de->name,
+						de->name_len,
+						filp->f_pos,
+						le32_to_cpu(de->inode),
+						d_type);
+				if (error)
+					break;
+				if (version != filp->f_version)
+					goto revalidate;
+				stored ++;
+			}
+			filp->f_pos += le16_to_cpu(de->rec_len);
+		}
+		offset = 0;
+		brelse (bh);
+	}
+	UPDATE_ATIME(inode);
+	return 0;
+}
diff -rup --new-file linux.mcp2/fs/ext3/file.c linux_tmp/fs/ext3/file.c
--- linux.mcp2/fs/ext3/file.c	1969-12-31 16:00:00.000000000 -0800
+++ linux_tmp/fs/ext3/file.c	2001-11-15 13:37:55.000000000 -0800
@@ -0,0 +1,94 @@
+/*
+ *  linux/fs/ext3/file.c
+ *
+ * Copyright (C) 1992, 1993, 1994, 1995
+ * Remy Card (card@masi.ibp.fr)
+ * Laboratoire MASI - Institut Blaise Pascal
+ * Universite Pierre et Marie Curie (Paris VI)
+ *
+ *  from
+ *
+ *  linux/fs/minix/file.c
+ *
+ *  Copyright (C) 1991, 1992  Linus Torvalds
+ *
+ *  ext3 fs regular file handling primitives
+ *
+ *  64-bit file support on 64-bit platforms by Jakub Jelinek
+ *	(jj@sunsite.ms.mff.cuni.cz)
+ */
+
+#include <linux/sched.h>
+#include <linux/fs.h>
+#include <linux/locks.h>
+#include <linux/jbd.h>
+#include <linux/ext3_fs.h>
+#include <linux/ext3_jbd.h>
+#include <linux/smp_lock.h>
+
+/*
+ * Called when an inode is released. Note that this is different
+ * from ext3_file_open: open gets called at every open, but release
+ * gets called only when /all/ the files are closed.
+ */
+static int ext3_release_file (struct inode * inode, struct file * filp)
+{
+	if (filp->f_mode & FMODE_WRITE)
+		ext3_discard_prealloc (inode);
+	return 0;
+}
+
+/*
+ * Called when an inode is about to be opened.
+ * We use this to disallow opening RW large files on 32bit systems if
+ * the caller didn't specify O_LARGEFILE.  On 64bit systems we force
+ * on this flag in sys_open.
+ */
+static int ext3_open_file (struct inode * inode, struct file * filp)
+{
+	if (!(filp->f_flags & O_LARGEFILE) &&
+	    inode->i_size > 0x7FFFFFFFLL)
+		return -EFBIG;
+	return 0;
+}
+
+/*
+ * ext3_file_write().
+ *
+ * Most things are done in ext3_prepare_write() and ext3_commit_write().
+ */
+
+static ssize_t
+ext3_file_write(struct file *file, const char *buf, size_t count, loff_t *ppos)
+{
+	struct inode *inode = file->f_dentry->d_inode;
+
+	/*
+	 * Nasty: if the file is subject to synchronous writes then we need
+	 * to force generic_osync_inode() to call ext3_write_inode().
+	 * We do that by marking the inode dirty.  This adds much more
+	 * computational expense than we need, but we're going to sync
+	 * anyway.
+	 */
+	if (IS_SYNC(inode) || (file->f_flags & O_SYNC))
+		mark_inode_dirty(inode);
+
+	return generic_file_write(file, buf, count, ppos);
+}
+
+struct file_operations ext3_file_operations = {
+	llseek:		generic_file_llseek,	/* BKL held */
+	read:		generic_file_read,	/* BKL not held.  Don't need */
+	write:		ext3_file_write,	/* BKL not held.  Don't need */
+	ioctl:		ext3_ioctl,		/* BKL held */
+	mmap:		generic_file_mmap,
+	open:		ext3_open_file,		/* BKL not held.  Don't need */
+	release:	ext3_release_file,	/* BKL not held.  Don't need */
+	fsync:		ext3_sync_file,		/* BKL held */
+};
+
+struct inode_operations ext3_file_inode_operations = {
+	truncate:	ext3_truncate,		/* BKL held */
+	setattr:	ext3_setattr,		/* BKL held */
+};
+
diff -rup --new-file linux.mcp2/fs/ext3/fsync.c linux_tmp/fs/ext3/fsync.c
--- linux.mcp2/fs/ext3/fsync.c	1969-12-31 16:00:00.000000000 -0800
+++ linux_tmp/fs/ext3/fsync.c	2001-11-20 21:34:13.000000000 -0800
@@ -0,0 +1,70 @@
+/*
+ *  linux/fs/ext3/fsync.c
+ *
+ *  Copyright (C) 1993  Stephen Tweedie (sct@redhat.com)
+ *  from
+ *  Copyright (C) 1992  Remy Card (card@masi.ibp.fr)
+ *                      Laboratoire MASI - Institut Blaise Pascal
+ *                      Universite Pierre et Marie Curie (Paris VI)
+ *  from
+ *  linux/fs/minix/truncate.c   Copyright (C) 1991, 1992  Linus Torvalds
+ * 
+ *  ext3fs fsync primitive
+ *
+ *  Big-endian to little-endian byte-swapping/bitmaps by
+ *        David S. Miller (davem@caip.rutgers.edu), 1995
+ * 
+ *  Removed unnecessary code duplication for little endian machines
+ *  and excessive __inline__s. 
+ *        Andi Kleen, 1997
+ *
+ * Major simplications and cleanup - we only need to do the metadata, because
+ * we can depend on generic_block_fdatasync() to sync the data blocks.
+ */
+
+#include <linux/sched.h>
+#include <linux/fs.h>
+#include <linux/jbd.h>
+#include <linux/ext3_fs.h>
+#include <linux/ext3_jbd.h>
+#include <linux/jbd.h>
+#include <linux/smp_lock.h>
+
+/*
+ * akpm: A new design for ext3_sync_file().
+ *
+ * This is only called from sys_fsync(), sys_fdatasync() and sys_msync().
+ * There cannot be a transaction open by this task. (AKPM: quotas?)
+ * Another task could have dirtied this inode.  Its data can be in any
+ * state in the journalling system.
+ *
+ * What we do is just kick off a commit and wait on it.  This will snapshot the
+ * inode to disk.
+ *
+ * Note that there is a serious optimisation we can make here: if the current
+ * inode is not part of j_running_transaction or j_committing_transaction
+ * then we have nothing to do.  That would require implementation of t_ilist,
+ * which isn't too hard.
+ */
+
+int ext3_sync_file(struct file * file, struct dentry *dentry, int datasync)
+{
+	struct inode *inode = dentry->d_inode;
+	int ret;
+
+	J_ASSERT(ext3_journal_current_handle() == 0);
+
+	/*
+	 * fsync_inode_buffers() just walks i_dirty_buffers and waits
+	 * on them.  It's a no-op for full data journalling because
+	 * i_dirty_buffers will be ampty.
+	 * Really, we only need to start I/O on the dirty buffers -
+	 * we'll end up waiting on them in commit.
+	 */
+	ret = fsync_inode_buffers(inode);
+	ret |= fsync_inode_data_buffers(inode);
+
+	ext3_force_commit(inode->i_sb);
+
+	return ret;
+}
diff -rup --new-file linux.mcp2/fs/ext3/ialloc.c linux_tmp/fs/ext3/ialloc.c
--- linux.mcp2/fs/ext3/ialloc.c	1969-12-31 16:00:00.000000000 -0800
+++ linux_tmp/fs/ext3/ialloc.c	2002-02-25 11:38:08.000000000 -0800
@@ -0,0 +1,663 @@
+/*
+ *  linux/fs/ext3/ialloc.c
+ *
+ * Copyright (C) 1992, 1993, 1994, 1995
+ * Remy Card (card@masi.ibp.fr)
+ * Laboratoire MASI - Institut Blaise Pascal
+ * Universite Pierre et Marie Curie (Paris VI)
+ *
+ *  BSD ufs-inspired inode and directory allocation by
+ *  Stephen Tweedie (sct@redhat.com), 1993
+ *  Big-endian to little-endian byte-swapping/bitmaps by
+ *        David S. Miller (davem@caip.rutgers.edu), 1995
+ */
+
+#include <linux/sched.h>
+#include <linux/fs.h>
+#include <linux/jbd.h>
+#include <linux/ext3_fs.h>
+#include <linux/ext3_jbd.h>
+#include <linux/stat.h>
+#include <linux/string.h>
+#include <linux/locks.h>
+#include <linux/quotaops.h>
+
+#include <asm/bitops.h>
+#include <asm/byteorder.h>
+
+/*
+ * ialloc.c contains the inodes allocation and deallocation routines
+ */
+
+/*
+ * The free inodes are managed by bitmaps.  A file system contains several
+ * blocks groups.  Each group contains 1 bitmap block for blocks, 1 bitmap
+ * block for inodes, N blocks for the inode table and data blocks.
+ *
+ * The file system contains group descriptors which are located after the
+ * super block.  Each descriptor contains the number of the bitmap block and
+ * the free blocks count in the block.  The descriptors are loaded in memory
+ * when a file system is mounted (see ext3_read_super).
+ */
+
+
+/*
+ * Read the inode allocation bitmap for a given block_group, reading
+ * into the specified slot in the superblock's bitmap cache.
+ *
+ * Return >=0 on success or a -ve error code.
+ */
+static int read_inode_bitmap (struct super_block * sb,
+			       unsigned long block_group,
+			       unsigned int bitmap_nr)
+{
+	struct ext3_group_desc * gdp;
+	struct buffer_head * bh = NULL;
+	int retval = 0;
+
+	gdp = ext3_get_group_desc (sb, block_group, NULL);
+	if (!gdp) {
+		retval = -EIO;
+		goto error_out;
+	}
+	bh = sb_bread(sb, le32_to_cpu(gdp->bg_inode_bitmap));
+	if (!bh) {
+		ext3_error (sb, "read_inode_bitmap",
+			    "Cannot read inode bitmap - "
+			    "block_group = %lu, inode_bitmap = %lu",
+			    block_group, (unsigned long) gdp->bg_inode_bitmap);
+		retval = -EIO;
+	}
+	/*
+	 * On IO error, just leave a zero in the superblock's block pointer for
+	 * this group.  The IO will be retried next time.
+	 */
+error_out:
+	sb->u.ext3_sb.s_inode_bitmap_number[bitmap_nr] = block_group;
+	sb->u.ext3_sb.s_inode_bitmap[bitmap_nr] = bh;
+	return retval;
+}
+
+/*
+ * load_inode_bitmap loads the inode bitmap for a blocks group
+ *
+ * It maintains a cache for the last bitmaps loaded.  This cache is managed
+ * with a LRU algorithm.
+ *
+ * Notes:
+ * 1/ There is one cache per mounted file system.
+ * 2/ If the file system contains less than EXT3_MAX_GROUP_LOADED groups,
+ *    this function reads the bitmap without maintaining a LRU cache.
+ *
+ * Return the slot used to store the bitmap, or a -ve error code.
+ */
+static int load_inode_bitmap (struct super_block * sb,
+			      unsigned int block_group)
+{
+	struct ext3_sb_info *sbi = EXT3_SB(sb);
+	unsigned long inode_bitmap_number;
+	struct buffer_head * inode_bitmap;
+	int i, j, retval = 0;
+
+	if (block_group >= sbi->s_groups_count)
+		ext3_panic (sb, "load_inode_bitmap",
+			    "block_group >= groups_count - "
+			    "block_group = %d, groups_count = %lu",
+			    block_group, sbi->s_groups_count);
+	if (sbi->s_loaded_inode_bitmaps > 0 &&
+	    sbi->s_inode_bitmap_number[0] == block_group &&
+	    sbi->s_inode_bitmap[0] != NULL)
+		return 0;
+	if (sbi->s_groups_count <= EXT3_MAX_GROUP_LOADED) {
+		if (sbi->s_inode_bitmap[block_group]) {
+			if (sbi->s_inode_bitmap_number[block_group] !=
+						block_group)
+				ext3_panic(sb, "load_inode_bitmap",
+					"block_group != inode_bitmap_number");
+			return block_group;
+		}
+		retval = read_inode_bitmap(sb, block_group, block_group);
+		if (retval < 0)
+			return retval;
+		return block_group;
+	}
+
+	for (i = 0; i < sbi->s_loaded_inode_bitmaps &&
+		    sbi->s_inode_bitmap_number[i] != block_group; i++)
+		/* do nothing */;
+	if (i < sbi->s_loaded_inode_bitmaps &&
+	    sbi->s_inode_bitmap_number[i] == block_group) {
+		inode_bitmap_number = sbi->s_inode_bitmap_number[i];
+		inode_bitmap = sbi->s_inode_bitmap[i];
+		for (j = i; j > 0; j--) {
+			sbi->s_inode_bitmap_number[j] =
+				sbi->s_inode_bitmap_number[j - 1];
+			sbi->s_inode_bitmap[j] = sbi->s_inode_bitmap[j - 1];
+		}
+		sbi->s_inode_bitmap_number[0] = inode_bitmap_number;
+		sbi->s_inode_bitmap[0] = inode_bitmap;
+
+		/*
+		 * There's still one special case here --- if inode_bitmap == 0
+		 * then our last attempt to read the bitmap failed and we have
+		 * just ended up caching that failure.  Try again to read it.
+		 */
+		if (!inode_bitmap)
+			retval = read_inode_bitmap (sb, block_group, 0);
+	} else {
+		if (sbi->s_loaded_inode_bitmaps < EXT3_MAX_GROUP_LOADED)
+			sbi->s_loaded_inode_bitmaps++;
+		else
+			brelse(sbi->s_inode_bitmap[EXT3_MAX_GROUP_LOADED - 1]);
+		for (j = sbi->s_loaded_inode_bitmaps - 1; j > 0; j--) {
+			sbi->s_inode_bitmap_number[j] =
+				sbi->s_inode_bitmap_number[j - 1];
+			sbi->s_inode_bitmap[j] = sbi->s_inode_bitmap[j - 1];
+		}
+		retval = read_inode_bitmap (sb, block_group, 0);
+	}
+	return retval;
+}
+
+/*
+ * NOTE! When we get the inode, we're the only people
+ * that have access to it, and as such there are no
+ * race conditions we have to worry about. The inode
+ * is not on the hash-lists, and it cannot be reached
+ * through the filesystem because the directory entry
+ * has been deleted earlier.
+ *
+ * HOWEVER: we must make sure that we get no aliases,
+ * which means that we have to call "clear_inode()"
+ * _before_ we mark the inode not in use in the inode
+ * bitmaps. Otherwise a newly created file might use
+ * the same inode number (not actually the same pointer
+ * though), and then we'd have two inodes sharing the
+ * same inode number and space on the harddisk.
+ */
+void ext3_free_inode (handle_t *handle, struct inode * inode)
+{
+	struct super_block * sb = inode->i_sb;
+	int is_directory;
+	unsigned long ino;
+	struct buffer_head * bh;
+	struct buffer_head * bh2;
+	unsigned long block_group;
+	unsigned long bit;
+	int bitmap_nr;
+	struct ext3_group_desc * gdp;
+	struct ext3_super_block * es;
+	int fatal = 0, err;
+
+	if (!inode->i_dev) {
+		printk ("ext3_free_inode: inode has no device\n");
+		return;
+	}
+	if (atomic_read(&inode->i_count) > 1) {
+		printk ("ext3_free_inode: inode has count=%d\n",
+					atomic_read(&inode->i_count));
+		return;
+	}
+	if (inode->i_nlink) {
+		printk ("ext3_free_inode: inode has nlink=%d\n",
+			inode->i_nlink);
+		return;
+	}
+	if (!sb) {
+		printk("ext3_free_inode: inode on nonexistent device\n");
+		return;
+	}
+
+	ino = inode->i_ino;
+	ext3_debug ("freeing inode %lu\n", ino);
+
+	/*
+	 * Note: we must free any quota before locking the superblock,
+	 * as writing the quota to disk may need the lock as well.
+	 */
+	DQUOT_INIT(inode);
+	DQUOT_FREE_INODE(inode);
+	DQUOT_DROP(inode);
+
+	is_directory = S_ISDIR(inode->i_mode);
+
+	/* Do this BEFORE marking the inode not in use or returning an error */
+	clear_inode (inode);
+
+	lock_super (sb);
+	es = sb->u.ext3_sb.s_es;
+	if (ino < EXT3_FIRST_INO(sb) || ino > le32_to_cpu(es->s_inodes_count)) {
+		ext3_error (sb, "ext3_free_inode",
+			    "reserved or nonexistent inode %lu", ino);
+		goto error_return;
+	}
+	block_group = (ino - 1) / EXT3_INODES_PER_GROUP(sb);
+	bit = (ino - 1) % EXT3_INODES_PER_GROUP(sb);
+	bitmap_nr = load_inode_bitmap (sb, block_group);
+	if (bitmap_nr < 0)
+		goto error_return;
+
+	bh = sb->u.ext3_sb.s_inode_bitmap[bitmap_nr];
+
+	BUFFER_TRACE(bh, "get_write_access");
+	fatal = ext3_journal_get_write_access(handle, bh);
+	if (fatal)
+		goto error_return;
+
+	/* Ok, now we can actually update the inode bitmaps.. */
+	if (!ext3_clear_bit (bit, bh->b_data))
+		ext3_error (sb, "ext3_free_inode",
+			      "bit already cleared for inode %lu", ino);
+	else {
+		gdp = ext3_get_group_desc (sb, block_group, &bh2);
+
+		BUFFER_TRACE(bh2, "get_write_access");
+		fatal = ext3_journal_get_write_access(handle, bh2);
+		if (fatal) goto error_return;
+
+		BUFFER_TRACE(sb->u.ext3_sb.s_sbh, "get write access");
+		fatal = ext3_journal_get_write_access(handle, sb->u.ext3_sb.s_sbh);
+		if (fatal) goto error_return;
+
+		if (gdp) {
+			gdp->bg_free_inodes_count = cpu_to_le16(
+				le16_to_cpu(gdp->bg_free_inodes_count) + 1);
+			if (is_directory)
+				gdp->bg_used_dirs_count = cpu_to_le16(
+				  le16_to_cpu(gdp->bg_used_dirs_count) - 1);
+		}
+		BUFFER_TRACE(bh2, "call ext3_journal_dirty_metadata");
+		err = ext3_journal_dirty_metadata(handle, bh2);
+		if (!fatal) fatal = err;
+		es->s_free_inodes_count =
+			cpu_to_le32(le32_to_cpu(es->s_free_inodes_count) + 1);
+		BUFFER_TRACE(sb->u.ext3_sb.s_sbh,
+					"call ext3_journal_dirty_metadata");
+		err = ext3_journal_dirty_metadata(handle, sb->u.ext3_sb.s_sbh);
+		if (!fatal) fatal = err;
+	}
+	BUFFER_TRACE(bh, "call ext3_journal_dirty_metadata");
+	err = ext3_journal_dirty_metadata(handle, bh);
+	if (!fatal)
+		fatal = err;
+	sb->s_dirt = 1;
+error_return:
+	ext3_std_error(sb, fatal);
+	unlock_super(sb);
+}
+
+/*
+ * There are two policies for allocating an inode.  If the new inode is
+ * a directory, then a forward search is made for a block group with both
+ * free space and a low directory-to-inode ratio; if that fails, then of
+ * the groups with above-average free space, that group with the fewest
+ * directories already is chosen.
+ *
+ * For other inodes, search forward from the parent directory's block
+ * group to find a free inode.
+ */
+struct inode * ext3_new_inode (handle_t *handle,
+				const struct inode * dir, int mode)
+{
+	struct super_block * sb;
+	struct buffer_head * bh;
+	struct buffer_head * bh2;
+	int i, j, avefreei;
+	struct inode * inode;
+	int bitmap_nr;
+	struct ext3_group_desc * gdp;
+	struct ext3_group_desc * tmp;
+	struct ext3_super_block * es;
+	int err = 0;
+
+	/* Cannot create files in a deleted directory */
+	if (!dir || !dir->i_nlink)
+		return ERR_PTR(-EPERM);
+
+	sb = dir->i_sb;
+	inode = new_inode(sb);
+	if (!inode)
+		return ERR_PTR(-ENOMEM);
+	init_rwsem(&inode->u.ext3_i.truncate_sem);
+
+	lock_super (sb);
+	es = sb->u.ext3_sb.s_es;
+repeat:
+	gdp = NULL;
+	i = 0;
+
+	if (S_ISDIR(mode)) {
+		avefreei = le32_to_cpu(es->s_free_inodes_count) /
+			sb->u.ext3_sb.s_groups_count;
+		if (!gdp) {
+			for (j = 0; j < sb->u.ext3_sb.s_groups_count; j++) {
+				struct buffer_head *temp_buffer;
+				tmp = ext3_get_group_desc (sb, j, &temp_buffer);
+				if (tmp &&
+				    le16_to_cpu(tmp->bg_free_inodes_count) &&
+				    le16_to_cpu(tmp->bg_free_inodes_count) >=
+							avefreei) {
+					if (!gdp || (le16_to_cpu(tmp->bg_free_blocks_count) >
+						le16_to_cpu(gdp->bg_free_blocks_count))) {
+						i = j;
+						gdp = tmp;
+						bh2 = temp_buffer;
+					}
+				}
+			}
+		}
+	} else {
+		/*
+		 * Try to place the inode in its parent directory
+		 */
+		i = dir->u.ext3_i.i_block_group;
+		tmp = ext3_get_group_desc (sb, i, &bh2);
+		if (tmp && le16_to_cpu(tmp->bg_free_inodes_count))
+			gdp = tmp;
+		else
+		{
+			/*
+			 * Use a quadratic hash to find a group with a
+			 * free inode
+			 */
+			for (j = 1; j < sb->u.ext3_sb.s_groups_count; j <<= 1) {
+				i += j;
+				if (i >= sb->u.ext3_sb.s_groups_count)
+					i -= sb->u.ext3_sb.s_groups_count;
+				tmp = ext3_get_group_desc (sb, i, &bh2);
+				if (tmp &&
+				    le16_to_cpu(tmp->bg_free_inodes_count)) {
+					gdp = tmp;
+					break;
+				}
+			}
+		}
+		if (!gdp) {
+			/*
+			 * That failed: try linear search for a free inode
+			 */
+			i = dir->u.ext3_i.i_block_group + 1;
+			for (j = 2; j < sb->u.ext3_sb.s_groups_count; j++) {
+				if (++i >= sb->u.ext3_sb.s_groups_count)
+					i = 0;
+				tmp = ext3_get_group_desc (sb, i, &bh2);
+				if (tmp &&
+				    le16_to_cpu(tmp->bg_free_inodes_count)) {
+					gdp = tmp;
+					break;
+				}
+			}
+		}
+	}
+
+	err = -ENOSPC;
+	if (!gdp)
+		goto fail;
+
+	err = -EIO;
+	bitmap_nr = load_inode_bitmap (sb, i);
+	if (bitmap_nr < 0)
+		goto fail;
+
+	bh = sb->u.ext3_sb.s_inode_bitmap[bitmap_nr];
+
+	if ((j = ext3_find_first_zero_bit ((unsigned long *) bh->b_data,
+				      EXT3_INODES_PER_GROUP(sb))) <
+	    EXT3_INODES_PER_GROUP(sb)) {
+		BUFFER_TRACE(bh, "get_write_access");
+		err = ext3_journal_get_write_access(handle, bh);
+		if (err) goto fail;
+		
+		if (ext3_set_bit (j, bh->b_data)) {
+			ext3_error (sb, "ext3_new_inode",
+				      "bit already set for inode %d", j);
+			goto repeat;
+		}
+		BUFFER_TRACE(bh, "call ext3_journal_dirty_metadata");
+		err = ext3_journal_dirty_metadata(handle, bh);
+		if (err) goto fail;
+	} else {
+		if (le16_to_cpu(gdp->bg_free_inodes_count) != 0) {
+			ext3_error (sb, "ext3_new_inode",
+				    "Free inodes count corrupted in group %d",
+				    i);
+			/* Is it really ENOSPC? */
+			err = -ENOSPC;
+			if (sb->s_flags & MS_RDONLY)
+				goto fail;
+
+			BUFFER_TRACE(bh2, "get_write_access");
+			err = ext3_journal_get_write_access(handle, bh2);
+			if (err) goto fail;
+			gdp->bg_free_inodes_count = 0;
+			BUFFER_TRACE(bh2, "call ext3_journal_dirty_metadata");
+			err = ext3_journal_dirty_metadata(handle, bh2);
+			if (err) goto fail;
+		}
+		goto repeat;
+	}
+	j += i * EXT3_INODES_PER_GROUP(sb) + 1;
+	if (j < EXT3_FIRST_INO(sb) || j > le32_to_cpu(es->s_inodes_count)) {
+		ext3_error (sb, "ext3_new_inode",
+			    "reserved inode or inode > inodes count - "
+			    "block_group = %d,inode=%d", i, j);
+		err = -EIO;
+		goto fail;
+	}
+
+	BUFFER_TRACE(bh2, "get_write_access");
+	err = ext3_journal_get_write_access(handle, bh2);
+	if (err) goto fail;
+	gdp->bg_free_inodes_count =
+		cpu_to_le16(le16_to_cpu(gdp->bg_free_inodes_count) - 1);
+	if (S_ISDIR(mode))
+		gdp->bg_used_dirs_count =
+			cpu_to_le16(le16_to_cpu(gdp->bg_used_dirs_count) + 1);
+	BUFFER_TRACE(bh2, "call ext3_journal_dirty_metadata");
+	err = ext3_journal_dirty_metadata(handle, bh2);
+	if (err) goto fail;
+	
+	BUFFER_TRACE(sb->u.ext3_sb.s_sbh, "get_write_access");
+	err = ext3_journal_get_write_access(handle, sb->u.ext3_sb.s_sbh);
+	if (err) goto fail;
+	es->s_free_inodes_count =
+		cpu_to_le32(le32_to_cpu(es->s_free_inodes_count) - 1);
+	BUFFER_TRACE(sb->u.ext3_sb.s_sbh, "call ext3_journal_dirty_metadata");
+	err = ext3_journal_dirty_metadata(handle, sb->u.ext3_sb.s_sbh);
+	sb->s_dirt = 1;
+	if (err) goto fail;
+
+	inode->i_uid = current->fsuid;
+	if (test_opt (sb, GRPID))
+		inode->i_gid = dir->i_gid;
+	else if (dir->i_mode & S_ISGID) {
+		inode->i_gid = dir->i_gid;
+		if (S_ISDIR(mode))
+			mode |= S_ISGID;
+	} else
+		inode->i_gid = current->fsgid;
+	inode->i_mode = mode;
+
+	inode->i_ino = j;
+	/* This is the optimal IO size (for stat), not the fs block size */
+	inode->i_blksize = PAGE_SIZE;
+	inode->i_blocks = 0;
+	inode->i_mtime = inode->i_atime = inode->i_ctime = CURRENT_TIME;
+	inode->u.ext3_i.i_flags = dir->u.ext3_i.i_flags & ~EXT3_INDEX_FL;
+	if (S_ISLNK(mode))
+		inode->u.ext3_i.i_flags &= ~(EXT3_IMMUTABLE_FL|EXT3_APPEND_FL);
+#ifdef EXT3_FRAGMENTS
+	inode->u.ext3_i.i_faddr = 0;
+	inode->u.ext3_i.i_frag_no = 0;
+	inode->u.ext3_i.i_frag_size = 0;
+#endif
+	inode->u.ext3_i.i_file_acl = 0;
+	inode->u.ext3_i.i_dir_acl = 0;
+	inode->u.ext3_i.i_dtime = 0;
+	INIT_LIST_HEAD(&inode->u.ext3_i.i_orphan);
+#ifdef EXT3_PREALLOCATE
+	inode->u.ext3_i.i_prealloc_count = 0;
+#endif
+	inode->u.ext3_i.i_block_group = i;
+	
+	if (inode->u.ext3_i.i_flags & EXT3_SYNC_FL)
+		inode->i_flags |= S_SYNC;
+	if (IS_SYNC(inode))
+		handle->h_sync = 1;
+	insert_inode_hash(inode);
+	inode->i_generation = sb->u.ext3_sb.s_next_generation++;
+
+	inode->u.ext3_i.i_state = EXT3_STATE_NEW;
+	err = ext3_mark_inode_dirty(handle, inode);
+	if (err) goto fail;
+	
+	unlock_super (sb);
+	if(DQUOT_ALLOC_INODE(inode)) {
+		DQUOT_DROP(inode);
+		inode->i_flags |= S_NOQUOTA;
+		inode->i_nlink = 0;
+		iput(inode);
+		return ERR_PTR(-EDQUOT);
+	}
+	ext3_debug ("allocating inode %lu\n", inode->i_ino);
+	return inode;
+
+fail:
+	unlock_super(sb);
+	iput(inode);
+	ext3_std_error(sb, err);
+	return ERR_PTR(err);
+}
+
+/* Verify that we are loading a valid orphan from disk */
+struct inode *ext3_orphan_get (struct super_block * sb, ino_t ino)
+{
+	ino_t max_ino = le32_to_cpu(EXT3_SB(sb)->s_es->s_inodes_count);
+	unsigned long block_group;
+	int bit;
+	int bitmap_nr;
+	struct buffer_head *bh;
+	struct inode *inode = NULL;
+	
+	/* Error cases - e2fsck has already cleaned up for us */
+	if (ino > max_ino) {
+		ext3_warning(sb, __FUNCTION__,
+			     "bad orphan ino %ld!  e2fsck was run?\n", ino);
+		return NULL;
+	}
+
+	block_group = (ino - 1) / EXT3_INODES_PER_GROUP(sb);
+	bit = (ino - 1) % EXT3_INODES_PER_GROUP(sb);
+	if ((bitmap_nr = load_inode_bitmap(sb, block_group)) < 0 ||
+	    !(bh = EXT3_SB(sb)->s_inode_bitmap[bitmap_nr])) {
+		ext3_warning(sb, __FUNCTION__,
+			     "inode bitmap error for orphan %ld\n", ino);
+		return NULL;
+	}
+
+	/* Having the inode bit set should be a 100% indicator that this
+	 * is a valid orphan (no e2fsck run on fs).  Orphans also include
+	 * inodes that were being truncated, so we can't check i_nlink==0.
+	 */
+	if (!ext3_test_bit(bit, bh->b_data) || !(inode = iget(sb, ino)) ||
+	    is_bad_inode(inode) || NEXT_ORPHAN(inode) > max_ino) {
+		ext3_warning(sb, __FUNCTION__,
+			     "bad orphan inode %ld!  e2fsck was run?\n", ino);
+		printk(KERN_NOTICE "ext3_test_bit(bit=%d, block=%ld) = %d\n",
+		       bit, bh->b_blocknr, ext3_test_bit(bit, bh->b_data));
+		printk(KERN_NOTICE "inode=%p\n", inode);
+		if (inode) {
+			printk(KERN_NOTICE "is_bad_inode(inode)=%d\n",
+			       is_bad_inode(inode));
+			printk(KERN_NOTICE "NEXT_ORPHAN(inode)=%d\n",
+			       NEXT_ORPHAN(inode));
+			printk(KERN_NOTICE "max_ino=%ld\n", max_ino);
+		}
+		/* Avoid freeing blocks if we got a bad deleted inode */
+		if (inode && inode->i_nlink == 0)
+			inode->i_blocks = 0;
+		iput(inode);
+		return NULL;
+	}
+
+	return inode;
+}
+
+unsigned long ext3_count_free_inodes (struct super_block * sb)
+{
+#ifdef EXT3FS_DEBUG
+	struct ext3_super_block * es;
+	unsigned long desc_count, bitmap_count, x;
+	int bitmap_nr;
+	struct ext3_group_desc * gdp;
+	int i;
+
+	lock_super (sb);
+	es = sb->u.ext3_sb.s_es;
+	desc_count = 0;
+	bitmap_count = 0;
+	gdp = NULL;
+	for (i = 0; i < sb->u.ext3_sb.s_groups_count; i++) {
+		gdp = ext3_get_group_desc (sb, i, NULL);
+		if (!gdp)
+			continue;
+		desc_count += le16_to_cpu(gdp->bg_free_inodes_count);
+		bitmap_nr = load_inode_bitmap (sb, i);
+		if (bitmap_nr < 0)
+			continue;
+
+		x = ext3_count_free (sb->u.ext3_sb.s_inode_bitmap[bitmap_nr],
+				     EXT3_INODES_PER_GROUP(sb) / 8);
+		printk ("group %d: stored = %d, counted = %lu\n",
+			i, le16_to_cpu(gdp->bg_free_inodes_count), x);
+		bitmap_count += x;
+	}
+	printk("ext3_count_free_inodes: stored = %lu, computed = %lu, %lu\n",
+		le32_to_cpu(es->s_free_inodes_count), desc_count, bitmap_count);
+	unlock_super (sb);
+	return desc_count;
+#else
+	return le32_to_cpu(sb->u.ext3_sb.s_es->s_free_inodes_count);
+#endif
+}
+
+#ifdef CONFIG_EXT3_CHECK
+/* Called at mount-time, super-block is locked */
+void ext3_check_inodes_bitmap (struct super_block * sb)
+{
+	struct ext3_super_block * es;
+	unsigned long desc_count, bitmap_count, x;
+	int bitmap_nr;
+	struct ext3_group_desc * gdp;
+	int i;
+
+	es = sb->u.ext3_sb.s_es;
+	desc_count = 0;
+	bitmap_count = 0;
+	gdp = NULL;
+	for (i = 0; i < sb->u.ext3_sb.s_groups_count; i++) {
+		gdp = ext3_get_group_desc (sb, i, NULL);
+		if (!gdp)
+			continue;
+		desc_count += le16_to_cpu(gdp->bg_free_inodes_count);
+		bitmap_nr = load_inode_bitmap (sb, i);
+		if (bitmap_nr < 0)
+			continue;
+
+		x = ext3_count_free (sb->u.ext3_sb.s_inode_bitmap[bitmap_nr],
+				     EXT3_INODES_PER_GROUP(sb) / 8);
+		if (le16_to_cpu(gdp->bg_free_inodes_count) != x)
+			ext3_error (sb, "ext3_check_inodes_bitmap",
+				    "Wrong free inodes count in group %d, "
+				    "stored = %d, counted = %lu", i,
+				    le16_to_cpu(gdp->bg_free_inodes_count), x);
+		bitmap_count += x;
+	}
+	if (le32_to_cpu(es->s_free_inodes_count) != bitmap_count)
+		ext3_error (sb, "ext3_check_inodes_bitmap",
+			    "Wrong free inodes count in super block, "
+			    "stored = %lu, counted = %lu",
+			    (unsigned long)le32_to_cpu(es->s_free_inodes_count),
+			    bitmap_count);
+}
+#endif
diff -rup --new-file linux.mcp2/fs/ext3/inode.c linux_tmp/fs/ext3/inode.c
--- linux.mcp2/fs/ext3/inode.c	1969-12-31 16:00:00.000000000 -0800
+++ linux_tmp/fs/ext3/inode.c	2002-08-02 17:39:45.000000000 -0700
@@ -0,0 +1,2699 @@
+/*
+ *  linux/fs/ext3/inode.c
+ *
+ * Copyright (C) 1992, 1993, 1994, 1995
+ * Remy Card (card@masi.ibp.fr)
+ * Laboratoire MASI - Institut Blaise Pascal
+ * Universite Pierre et Marie Curie (Paris VI)
+ *
+ *  from
+ *
+ *  linux/fs/minix/inode.c
+ *
+ *  Copyright (C) 1991, 1992  Linus Torvalds
+ *
+ *  Goal-directed block allocation by Stephen Tweedie
+ * 	(sct@redhat.com), 1993, 1998
+ *  Big-endian to little-endian byte-swapping/bitmaps by
+ *        David S. Miller (davem@caip.rutgers.edu), 1995
+ *  64-bit file support on 64-bit platforms by Jakub Jelinek
+ * 	(jj@sunsite.ms.mff.cuni.cz)
+ *
+ *  Assorted race fixes, rewrite of ext3_get_block() by Al Viro, 2000
+ */
+
+#include <linux/fs.h>
+#include <linux/sched.h>
+#include <linux/ext3_jbd.h>
+#include <linux/jbd.h>
+#include <linux/locks.h>
+#include <linux/smp_lock.h>
+#include <linux/highuid.h>
+#include <linux/quotaops.h>
+#include <linux/module.h>
+
+/*
+ * SEARCH_FROM_ZERO forces each block allocation to search from the start
+ * of the filesystem.  This is to force rapid reallocation of recently-freed
+ * blocks.  The file fragmentation is horrendous.
+ */
+#undef SEARCH_FROM_ZERO
+
+/* The ext3 forget function must perform a revoke if we are freeing data
+ * which has been journaled.  Metadata (eg. indirect blocks) must be
+ * revoked in all cases. 
+ *
+ * "bh" may be NULL: a metadata block may have been freed from memory
+ * but there may still be a record of it in the journal, and that record
+ * still needs to be revoked.
+ */
+
+static int ext3_forget(handle_t *handle, int is_metadata,
+		       struct inode *inode, struct buffer_head *bh,
+		       int blocknr)
+{
+	int err;
+
+	BUFFER_TRACE(bh, "enter");
+
+	jbd_debug(4, "forgetting bh %p: is_metadata = %d, mode %o, "
+		  "data mode %lx\n",
+		  bh, is_metadata, inode->i_mode,
+		  test_opt(inode->i_sb, DATA_FLAGS));
+	
+	/* Never use the revoke function if we are doing full data
+	 * journaling: there is no need to, and a V1 superblock won't
+	 * support it.  Otherwise, only skip the revoke on un-journaled
+	 * data blocks. */
+
+	if (test_opt(inode->i_sb, DATA_FLAGS) == EXT3_MOUNT_JOURNAL_DATA ||
+	    (!is_metadata && !ext3_should_journal_data(inode))) {
+		if (bh) {
+			BUFFER_TRACE(bh, "call journal_forget");
+			ext3_journal_forget(handle, bh);
+		}
+		return 0;
+	}
+
+	/*
+	 * data!=journal && (is_metadata || should_journal_data(inode))
+	 */
+	BUFFER_TRACE(bh, "call ext3_journal_revoke");
+	err = ext3_journal_revoke(handle, blocknr, bh);
+	if (err)
+		ext3_abort(inode->i_sb, __FUNCTION__,
+			   "error %d when attempting revoke", err);
+	BUFFER_TRACE(bh, "exit");
+	return err;
+}
+
+/* 
+ * Truncate transactions can be complex and absolutely huge.  So we need to
+ * be able to restart the transaction at a conventient checkpoint to make
+ * sure we don't overflow the journal.
+ *
+ * start_transaction gets us a new handle for a truncate transaction,
+ * and extend_transaction tries to extend the existing one a bit.  If
+ * extend fails, we need to propagate the failure up and restart the
+ * transaction in the top-level truncate loop. --sct 
+ */
+
+static handle_t *start_transaction(struct inode *inode) 
+{
+	long needed;
+	handle_t *result;
+	
+	needed = inode->i_blocks;
+	if (needed > EXT3_MAX_TRANS_DATA) 
+		needed = EXT3_MAX_TRANS_DATA;
+	
+	result = ext3_journal_start(inode, EXT3_DATA_TRANS_BLOCKS + needed);
+	if (!IS_ERR(result))
+		return result;
+	
+	ext3_std_error(inode->i_sb, PTR_ERR(result));
+	return result;
+}
+
+/*
+ * Try to extend this transaction for the purposes of truncation.
+ *
+ * Returns 0 if we managed to create more room.  If we can't create more
+ * room, and the transaction must be restarted we return 1.
+ */
+static int try_to_extend_transaction(handle_t *handle, struct inode *inode)
+{
+	long needed;
+	
+	if (handle->h_buffer_credits > EXT3_RESERVE_TRANS_BLOCKS)
+		return 0;
+	needed = inode->i_blocks;
+	if (needed > EXT3_MAX_TRANS_DATA) 
+		needed = EXT3_MAX_TRANS_DATA;
+	if (!ext3_journal_extend(handle, EXT3_RESERVE_TRANS_BLOCKS + needed))
+		return 0;
+	return 1;
+}
+
+/*
+ * Restart the transaction associated with *handle.  This does a commit,
+ * so before we call here everything must be consistently dirtied against
+ * this transaction.
+ */
+static int ext3_journal_test_restart(handle_t *handle, struct inode *inode)
+{
+	long needed = inode->i_blocks;
+	if (needed > EXT3_MAX_TRANS_DATA) 
+		needed = EXT3_MAX_TRANS_DATA;
+	jbd_debug(2, "restarting handle %p\n", handle);
+	return ext3_journal_restart(handle, EXT3_DATA_TRANS_BLOCKS + needed);
+}
+
+/*
+ * Called at each iput()
+ */
+void ext3_put_inode (struct inode * inode)
+{
+	ext3_discard_prealloc (inode);
+}
+
+/*
+ * Called at the last iput() if i_nlink is zero.
+ */
+void ext3_delete_inode (struct inode * inode)
+{
+	handle_t *handle;
+	
+	if (is_bad_inode(inode) ||
+	    inode->i_ino == EXT3_ACL_IDX_INO ||
+	    inode->i_ino == EXT3_ACL_DATA_INO)
+		goto no_delete;
+
+	lock_kernel();
+	handle = start_transaction(inode);
+	if (IS_ERR(handle)) {
+		/* If we're going to skip the normal cleanup, we still
+		 * need to make sure that the in-core orphan linked list
+		 * is properly cleaned up. */
+		ext3_orphan_del(NULL, inode);
+
+		ext3_std_error(inode->i_sb, PTR_ERR(handle));
+		unlock_kernel();
+		goto no_delete;
+	}
+	
+	if (IS_SYNC(inode))
+		handle->h_sync = 1;
+	inode->i_size = 0;
+	if (inode->i_blocks)
+		ext3_truncate(inode);
+	/*
+	 * Kill off the orphan record which ext3_truncate created.
+	 * AKPM: I think this can be inside the above `if'.
+	 * Note that ext3_orphan_del() has to be able to cope with the
+	 * deletion of a non-existent orphan - this is because we don't
+	 * know if ext3_truncate() actually created an orphan record.
+	 * (Well, we could do this if we need to, but heck - it works)
+	 */
+	ext3_orphan_del(handle, inode);
+	inode->u.ext3_i.i_dtime	= CURRENT_TIME;
+
+	/* 
+	 * One subtle ordering requirement: if anything has gone wrong
+	 * (transaction abort, IO errors, whatever), then we can still
+	 * do these next steps (the fs will already have been marked as
+	 * having errors), but we can't free the inode if the mark_dirty
+	 * fails.  
+	 */
+	if (ext3_mark_inode_dirty(handle, inode))
+		/* If that failed, just do the required in-core inode clear. */
+		clear_inode(inode);
+	else
+		ext3_free_inode(handle, inode);
+	ext3_journal_stop(handle, inode);
+	unlock_kernel();
+	return;
+no_delete:
+	clear_inode(inode);	/* We must guarantee clearing of inode... */
+}
+
+void ext3_discard_prealloc (struct inode * inode)
+{
+#ifdef EXT3_PREALLOCATE
+	lock_kernel();
+	/* Writer: ->i_prealloc* */
+	if (inode->u.ext3_i.i_prealloc_count) {
+		unsigned short total = inode->u.ext3_i.i_prealloc_count;
+		unsigned long block = inode->u.ext3_i.i_prealloc_block;
+		inode->u.ext3_i.i_prealloc_count = 0;
+		inode->u.ext3_i.i_prealloc_block = 0;
+		/* Writer: end */
+		ext3_free_blocks (inode, block, total);
+	}
+	unlock_kernel();
+#endif
+}
+
+static int ext3_alloc_block (handle_t *handle,
+			struct inode * inode, unsigned long goal, int *err)
+{
+#ifdef EXT3FS_DEBUG
+	static unsigned long alloc_hits = 0, alloc_attempts = 0;
+#endif
+	unsigned long result;
+
+#ifdef EXT3_PREALLOCATE
+	/* Writer: ->i_prealloc* */
+	if (inode->u.ext3_i.i_prealloc_count &&
+	    (goal == inode->u.ext3_i.i_prealloc_block ||
+	     goal + 1 == inode->u.ext3_i.i_prealloc_block))
+	{
+		result = inode->u.ext3_i.i_prealloc_block++;
+		inode->u.ext3_i.i_prealloc_count--;
+		/* Writer: end */
+		ext3_debug ("preallocation hit (%lu/%lu).\n",
+			    ++alloc_hits, ++alloc_attempts);
+	} else {
+		ext3_discard_prealloc (inode);
+		ext3_debug ("preallocation miss (%lu/%lu).\n",
+			    alloc_hits, ++alloc_attempts);
+		if (S_ISREG(inode->i_mode))
+			result = ext3_new_block (inode, goal, 
+				 &inode->u.ext3_i.i_prealloc_count,
+				 &inode->u.ext3_i.i_prealloc_block, err);
+		else
+			result = ext3_new_block (inode, goal, 0, 0, err);
+		/*
+		 * AKPM: this is somewhat sticky.  I'm not surprised it was
+		 * disabled in 2.2's ext3.  Need to integrate b_committed_data
+		 * guarding with preallocation, if indeed preallocation is
+		 * effective.
+		 */
+	}
+#else
+	result = ext3_new_block (handle, inode, goal, 0, 0, err);
+#endif
+	return result;
+}
+
+
+typedef struct {
+	u32	*p;
+	u32	key;
+	struct buffer_head *bh;
+} Indirect;
+
+static inline void add_chain(Indirect *p, struct buffer_head *bh, u32 *v)
+{
+	p->key = *(p->p = v);
+	p->bh = bh;
+}
+
+static inline int verify_chain(Indirect *from, Indirect *to)
+{
+	while (from <= to && from->key == *from->p)
+		from++;
+	return (from > to);
+}
+
+/**
+ *	ext3_block_to_path - parse the block number into array of offsets
+ *	@inode: inode in question (we are only interested in its superblock)
+ *	@i_block: block number to be parsed
+ *	@offsets: array to store the offsets in
+ *
+ *	To store the locations of file's data ext3 uses a data structure common
+ *	for UNIX filesystems - tree of pointers anchored in the inode, with
+ *	data blocks at leaves and indirect blocks in intermediate nodes.
+ *	This function translates the block number into path in that tree -
+ *	return value is the path length and @offsets[n] is the offset of
+ *	pointer to (n+1)th node in the nth one. If @block is out of range
+ *	(negative or too large) warning is printed and zero returned.
+ *
+ *	Note: function doesn't find node addresses, so no IO is needed. All
+ *	we need to know is the capacity of indirect blocks (taken from the
+ *	inode->i_sb).
+ */
+
+/*
+ * Portability note: the last comparison (check that we fit into triple
+ * indirect block) is spelled differently, because otherwise on an
+ * architecture with 32-bit longs and 8Kb pages we might get into trouble
+ * if our filesystem had 8Kb blocks. We might use long long, but that would
+ * kill us on x86. Oh, well, at least the sign propagation does not matter -
+ * i_block would have to be negative in the very beginning, so we would not
+ * get there at all.
+ */
+
+static int ext3_block_to_path(struct inode *inode, long i_block, int offsets[4])
+{
+	int ptrs = EXT3_ADDR_PER_BLOCK(inode->i_sb);
+	int ptrs_bits = EXT3_ADDR_PER_BLOCK_BITS(inode->i_sb);
+	const long direct_blocks = EXT3_NDIR_BLOCKS,
+		indirect_blocks = ptrs,
+		double_blocks = (1 << (ptrs_bits * 2));
+	int n = 0;
+
+	if (i_block < 0) {
+		ext3_warning (inode->i_sb, "ext3_block_to_path", "block < 0");
+	} else if (i_block < direct_blocks) {
+		offsets[n++] = i_block;
+	} else if ( (i_block -= direct_blocks) < indirect_blocks) {
+		offsets[n++] = EXT3_IND_BLOCK;
+		offsets[n++] = i_block;
+	} else if ((i_block -= indirect_blocks) < double_blocks) {
+		offsets[n++] = EXT3_DIND_BLOCK;
+		offsets[n++] = i_block >> ptrs_bits;
+		offsets[n++] = i_block & (ptrs - 1);
+	} else if (((i_block -= double_blocks) >> (ptrs_bits * 2)) < ptrs) {
+		offsets[n++] = EXT3_TIND_BLOCK;
+		offsets[n++] = i_block >> (ptrs_bits * 2);
+		offsets[n++] = (i_block >> ptrs_bits) & (ptrs - 1);
+		offsets[n++] = i_block & (ptrs - 1);
+	} else {
+		ext3_warning (inode->i_sb, "ext3_block_to_path", "block > big");
+	}
+	return n;
+}
+
+/**
+ *	ext3_get_branch - read the chain of indirect blocks leading to data
+ *	@inode: inode in question
+ *	@depth: depth of the chain (1 - direct pointer, etc.)
+ *	@offsets: offsets of pointers in inode/indirect blocks
+ *	@chain: place to store the result
+ *	@err: here we store the error value
+ *
+ *	Function fills the array of triples <key, p, bh> and returns %NULL
+ *	if everything went OK or the pointer to the last filled triple
+ *	(incomplete one) otherwise. Upon the return chain[i].key contains
+ *	the number of (i+1)-th block in the chain (as it is stored in memory,
+ *	i.e. little-endian 32-bit), chain[i].p contains the address of that
+ *	number (it points into struct inode for i==0 and into the bh->b_data
+ *	for i>0) and chain[i].bh points to the buffer_head of i-th indirect
+ *	block for i>0 and NULL for i==0. In other words, it holds the block
+ *	numbers of the chain, addresses they were taken from (and where we can
+ *	verify that chain did not change) and buffer_heads hosting these
+ *	numbers.
+ *
+ *	Function stops when it stumbles upon zero pointer (absent block)
+ *		(pointer to last triple returned, *@err == 0)
+ *	or when it gets an IO error reading an indirect block
+ *		(ditto, *@err == -EIO)
+ *	or when it notices that chain had been changed while it was reading
+ *		(ditto, *@err == -EAGAIN)
+ *	or when it reads all @depth-1 indirect blocks successfully and finds
+ *	the whole chain, all way to the data (returns %NULL, *err == 0).
+ */
+static Indirect *ext3_get_branch(struct inode *inode, int depth, int *offsets,
+				 Indirect chain[4], int *err)
+{
+	struct super_block *sb = inode->i_sb;
+	Indirect *p = chain;
+	struct buffer_head *bh;
+
+	*err = 0;
+	/* i_data is not going away, no lock needed */
+	add_chain (chain, NULL, inode->u.ext3_i.i_data + *offsets);
+	if (!p->key)
+		goto no_block;
+	while (--depth) {
+		bh = sb_bread(sb, le32_to_cpu(p->key));
+		if (!bh)
+			goto failure;
+		/* Reader: pointers */
+		if (!verify_chain(chain, p))
+			goto changed;
+		add_chain(++p, bh, (u32*)bh->b_data + *++offsets);
+		/* Reader: end */
+		if (!p->key)
+			goto no_block;
+	}
+	return NULL;
+
+changed:
+	*err = -EAGAIN;
+	goto no_block;
+failure:
+	*err = -EIO;
+no_block:
+	return p;
+}
+
+/**
+ *	ext3_find_near - find a place for allocation with sufficient locality
+ *	@inode: owner
+ *	@ind: descriptor of indirect block.
+ *
+ *	This function returns the prefered place for block allocation.
+ *	It is used when heuristic for sequential allocation fails.
+ *	Rules are:
+ *	  + if there is a block to the left of our position - allocate near it.
+ *	  + if pointer will live in indirect block - allocate near that block.
+ *	  + if pointer will live in inode - allocate in the same
+ *	    cylinder group. 
+ *	Caller must make sure that @ind is valid and will stay that way.
+ */
+
+static inline unsigned long ext3_find_near(struct inode *inode, Indirect *ind)
+{
+	u32 *start = ind->bh ? (u32*) ind->bh->b_data : inode->u.ext3_i.i_data;
+	u32 *p;
+
+	/* Try to find previous block */
+	for (p = ind->p - 1; p >= start; p--)
+		if (*p)
+			return le32_to_cpu(*p);
+
+	/* No such thing, so let's try location of indirect block */
+	if (ind->bh)
+		return ind->bh->b_blocknr;
+
+	/*
+	 * It is going to be refered from inode itself? OK, just put it into
+	 * the same cylinder group then.
+	 */
+	return (inode->u.ext3_i.i_block_group * 
+		EXT3_BLOCKS_PER_GROUP(inode->i_sb)) +
+	       le32_to_cpu(inode->i_sb->u.ext3_sb.s_es->s_first_data_block);
+}
+
+/**
+ *	ext3_find_goal - find a prefered place for allocation.
+ *	@inode: owner
+ *	@block:  block we want
+ *	@chain:  chain of indirect blocks
+ *	@partial: pointer to the last triple within a chain
+ *	@goal:	place to store the result.
+ *
+ *	Normally this function find the prefered place for block allocation,
+ *	stores it in *@goal and returns zero. If the branch had been changed
+ *	under us we return -EAGAIN.
+ */
+
+static int ext3_find_goal(struct inode *inode, long block, Indirect chain[4],
+			  Indirect *partial, unsigned long *goal)
+{
+	/* Writer: ->i_next_alloc* */
+	if (block == inode->u.ext3_i.i_next_alloc_block + 1) {
+		inode->u.ext3_i.i_next_alloc_block++;
+		inode->u.ext3_i.i_next_alloc_goal++;
+	}
+#ifdef SEARCH_FROM_ZERO
+	inode->u.ext3_i.i_next_alloc_block = 0;
+	inode->u.ext3_i.i_next_alloc_goal = 0;
+#endif
+	/* Writer: end */
+	/* Reader: pointers, ->i_next_alloc* */
+	if (verify_chain(chain, partial)) {
+		/*
+		 * try the heuristic for sequential allocation,
+		 * failing that at least try to get decent locality.
+		 */
+		if (block == inode->u.ext3_i.i_next_alloc_block)
+			*goal = inode->u.ext3_i.i_next_alloc_goal;
+		if (!*goal)
+			*goal = ext3_find_near(inode, partial);
+#ifdef SEARCH_FROM_ZERO
+		*goal = 0;
+#endif
+		return 0;
+	}
+	/* Reader: end */
+	return -EAGAIN;
+}
+
+/**
+ *	ext3_alloc_branch - allocate and set up a chain of blocks.
+ *	@inode: owner
+ *	@num: depth of the chain (number of blocks to allocate)
+ *	@offsets: offsets (in the blocks) to store the pointers to next.
+ *	@branch: place to store the chain in.
+ *
+ *	This function allocates @num blocks, zeroes out all but the last one,
+ *	links them into chain and (if we are synchronous) writes them to disk.
+ *	In other words, it prepares a branch that can be spliced onto the
+ *	inode. It stores the information about that chain in the branch[], in
+ *	the same format as ext3_get_branch() would do. We are calling it after
+ *	we had read the existing part of chain and partial points to the last
+ *	triple of that (one with zero ->key). Upon the exit we have the same
+ *	picture as after the successful ext3_get_block(), excpet that in one
+ *	place chain is disconnected - *branch->p is still zero (we did not
+ *	set the last link), but branch->key contains the number that should
+ *	be placed into *branch->p to fill that gap.
+ *
+ *	If allocation fails we free all blocks we've allocated (and forget
+ *	their buffer_heads) and return the error value the from failed
+ *	ext3_alloc_block() (normally -ENOSPC). Otherwise we set the chain
+ *	as described above and return 0.
+ */
+
+static int ext3_alloc_branch(handle_t *handle, struct inode *inode,
+			     int num,
+			     unsigned long goal,
+			     int *offsets,
+			     Indirect *branch)
+{
+	int blocksize = inode->i_sb->s_blocksize;
+	int n = 0, keys = 0;
+	int err = 0;
+	int i;
+	int parent = ext3_alloc_block(handle, inode, goal, &err);
+
+	branch[0].key = cpu_to_le32(parent);
+	if (parent) {
+		for (n = 1; n < num; n++) {
+			struct buffer_head *bh;
+			/* Allocate the next block */
+			int nr = ext3_alloc_block(handle, inode, parent, &err);
+			if (!nr)
+				break;
+			branch[n].key = cpu_to_le32(nr);
+			keys = n+1;
+			
+			/*
+			 * Get buffer_head for parent block, zero it out
+			 * and set the pointer to new one, then send
+			 * parent to disk.  
+			 */
+			bh = sb_getblk(inode->i_sb, parent);
+			branch[n].bh = bh;
+			lock_buffer(bh);
+			BUFFER_TRACE(bh, "call get_create_access");
+			err = ext3_journal_get_create_access(handle, bh);
+			if (err) {
+				unlock_buffer(bh);
+				brelse(bh);
+				break;
+			}
+
+			memset(bh->b_data, 0, blocksize);
+			branch[n].p = (u32*) bh->b_data + offsets[n];
+			*branch[n].p = branch[n].key;
+			BUFFER_TRACE(bh, "marking uptodate");
+			mark_buffer_uptodate(bh, 1);
+			unlock_buffer(bh);
+
+			BUFFER_TRACE(bh, "call ext3_journal_dirty_metadata");
+			err = ext3_journal_dirty_metadata(handle, bh);
+			if (err)
+				break;
+			
+			parent = nr;
+		}
+	}
+	if (n == num)
+		return 0;
+
+	/* Allocation failed, free what we already allocated */
+	for (i = 1; i < keys; i++) {
+		BUFFER_TRACE(branch[i].bh, "call journal_forget");
+		ext3_journal_forget(handle, branch[i].bh);
+	}
+	for (i = 0; i < keys; i++)
+		ext3_free_blocks(handle, inode, le32_to_cpu(branch[i].key), 1);
+	return err;
+}
+
+/**
+ *	ext3_splice_branch - splice the allocated branch onto inode.
+ *	@inode: owner
+ *	@block: (logical) number of block we are adding
+ *	@chain: chain of indirect blocks (with a missing link - see
+ *		ext3_alloc_branch)
+ *	@where: location of missing link
+ *	@num:   number of blocks we are adding
+ *
+ *	This function verifies that chain (up to the missing link) had not
+ *	changed, fills the missing link and does all housekeeping needed in
+ *	inode (->i_blocks, etc.). In case of success we end up with the full
+ *	chain to new block and return 0. Otherwise (== chain had been changed)
+ *	we free the new blocks (forgetting their buffer_heads, indeed) and
+ *	return -EAGAIN.
+ */
+
+static int ext3_splice_branch(handle_t *handle, struct inode *inode, long block,
+			      Indirect chain[4], Indirect *where, int num)
+{
+	int i;
+	int err = 0;
+
+	/*
+	 * If we're splicing into a [td]indirect block (as opposed to the
+	 * inode) then we need to get write access to the [td]indirect block
+	 * before the splice.
+	 */
+	if (where->bh) {
+		BUFFER_TRACE(where->bh, "get_write_access");
+		err = ext3_journal_get_write_access(handle, where->bh);
+		if (err)
+			goto err_out;
+	}
+	/* Verify that place we are splicing to is still there and vacant */
+
+	/* Writer: pointers, ->i_next_alloc* */
+	if (!verify_chain(chain, where-1) || *where->p)
+		/* Writer: end */
+		goto changed;
+
+	/* That's it */
+
+	*where->p = where->key;
+	inode->u.ext3_i.i_next_alloc_block = block;
+	inode->u.ext3_i.i_next_alloc_goal = le32_to_cpu(where[num-1].key);
+#ifdef SEARCH_FROM_ZERO
+	inode->u.ext3_i.i_next_alloc_block = 0;
+	inode->u.ext3_i.i_next_alloc_goal = 0;
+#endif
+	/* Writer: end */
+
+	/* We are done with atomic stuff, now do the rest of housekeeping */
+
+	inode->i_ctime = CURRENT_TIME;
+	ext3_mark_inode_dirty(handle, inode);
+
+	/* had we spliced it onto indirect block? */
+	if (where->bh) {
+		/*
+		 * akpm: If we spliced it onto an indirect block, we haven't
+		 * altered the inode.  Note however that if it is being spliced
+		 * onto an indirect block at the very end of the file (the
+		 * file is growing) then we *will* alter the inode to reflect
+		 * the new i_size.  But that is not done here - it is done in
+		 * generic_commit_write->__mark_inode_dirty->ext3_dirty_inode.
+		 */
+		jbd_debug(5, "splicing indirect only\n");
+		BUFFER_TRACE(where->bh, "call ext3_journal_dirty_metadata");
+		err = ext3_journal_dirty_metadata(handle, where->bh);
+		if (err) 
+			goto err_out;
+	} else {
+		/*
+		 * OK, we spliced it into the inode itself on a direct block.
+		 * Inode was dirtied above.
+		 */
+		jbd_debug(5, "splicing direct\n");
+	}
+	return err;
+
+changed:
+	/*
+	 * AKPM: if where[i].bh isn't part of the current updating
+	 * transaction then we explode nastily.  Test this code path.
+	 */
+	jbd_debug(1, "the chain changed: try again\n");
+	err = -EAGAIN;
+	
+err_out:
+	for (i = 1; i < num; i++) {
+		BUFFER_TRACE(where[i].bh, "call journal_forget");
+		ext3_journal_forget(handle, where[i].bh);
+	}
+	/* For the normal collision cleanup case, we free up the blocks.
+	 * On genuine filesystem errors we don't even think about doing
+	 * that. */
+	if (err == -EAGAIN)
+		for (i = 0; i < num; i++)
+			ext3_free_blocks(handle, inode, 
+					 le32_to_cpu(where[i].key), 1);
+	return err;
+}
+
+/*
+ * Allocation strategy is simple: if we have to allocate something, we will
+ * have to go the whole way to leaf. So let's do it before attaching anything
+ * to tree, set linkage between the newborn blocks, write them if sync is
+ * required, recheck the path, free and repeat if check fails, otherwise
+ * set the last missing link (that will protect us from any truncate-generated
+ * removals - all blocks on the path are immune now) and possibly force the
+ * write on the parent block.
+ * That has a nice additional property: no special recovery from the failed
+ * allocations is needed - we simply release blocks and do not touch anything
+ * reachable from inode.
+ *
+ * akpm: `handle' can be NULL if create == 0.
+ *
+ * The BKL may not be held on entry here.  Be sure to take it early.
+ */
+
+static int ext3_get_block_handle(handle_t *handle, struct inode *inode, 
+				 long iblock,
+				 struct buffer_head *bh_result, int create)
+{
+	int err = -EIO;
+	int offsets[4];
+	Indirect chain[4];
+	Indirect *partial;
+	unsigned long goal;
+	int left;
+	int depth = ext3_block_to_path(inode, iblock, offsets);
+	loff_t new_size;
+
+	J_ASSERT(handle != NULL || create == 0);
+
+	if (depth == 0)
+		goto out;
+
+	lock_kernel();
+reread:
+	partial = ext3_get_branch(inode, depth, offsets, chain, &err);
+
+	/* Simplest case - block found, no allocation needed */
+	if (!partial) {
+		bh_result->b_state &= ~(1UL << BH_New);
+got_it:
+		bh_result->b_dev = inode->i_dev;
+		bh_result->b_blocknr = le32_to_cpu(chain[depth-1].key);
+		bh_result->b_state |= (1UL << BH_Mapped);
+		/* Clean up and exit */
+		partial = chain+depth-1; /* the whole chain */
+		goto cleanup;
+	}
+
+	/* Next simple case - plain lookup or failed read of indirect block */
+	if (!create || err == -EIO) {
+cleanup:
+		while (partial > chain) {
+			BUFFER_TRACE(partial->bh, "call brelse");
+			brelse(partial->bh);
+			partial--;
+		}
+		BUFFER_TRACE(bh_result, "returned");
+		unlock_kernel();
+out:
+		return err;
+	}
+
+	/*
+	 * Indirect block might be removed by truncate while we were
+	 * reading it. Handling of that case (forget what we've got and
+	 * reread) is taken out of the main path.
+	 */
+	if (err == -EAGAIN)
+		goto changed;
+
+	if (ext3_find_goal(inode, iblock, chain, partial, &goal) < 0)
+		goto changed;
+
+	left = (chain + depth) - partial;
+
+	/*
+	 * Block out ext3_truncate while we alter the tree
+	 */
+	down_read(&inode->u.ext3_i.truncate_sem);
+	err = ext3_alloc_branch(handle, inode, left, goal,
+					offsets+(partial-chain), partial);
+
+	/* The ext3_splice_branch call will free and forget any buffers
+	 * on the new chain if there is a failure, but that risks using
+	 * up transaction credits, especially for bitmaps where the
+	 * credits cannot be returned.  Can we handle this somehow?  We
+	 * may need to return -EAGAIN upwards in the worst case.  --sct */
+	if (!err)
+		err = ext3_splice_branch(handle, inode, iblock, chain,
+					 partial, left);
+	up_read(&inode->u.ext3_i.truncate_sem);
+	if (err == -EAGAIN)
+		goto changed;
+	if (err)
+		goto cleanup;
+
+	new_size = inode->i_size;
+	/*
+	 * This is not racy against ext3_truncate's modification of i_disksize
+	 * because VM/VFS ensures that the file cannot be extended while
+	 * truncate is in progress.  It is racy between multiple parallel
+	 * instances of get_block, but we have the BKL.
+	 */
+	if (new_size > inode->u.ext3_i.i_disksize)
+		inode->u.ext3_i.i_disksize = new_size;
+
+	bh_result->b_state |= (1UL << BH_New);
+	goto got_it;
+
+changed:
+	while (partial > chain) {
+		jbd_debug(1, "buffer chain changed, retrying\n");
+		BUFFER_TRACE(partial->bh, "brelsing");
+		brelse(partial->bh);
+		partial--;
+	}
+	goto reread;
+}
+
+/*
+ * The BKL is not held on entry here.
+ */
+static int ext3_get_block(struct inode *inode, long iblock,
+			struct buffer_head *bh_result, int create)
+{
+	handle_t *handle = 0;
+	int ret;
+
+	if (create) {
+		handle = ext3_journal_current_handle();
+		J_ASSERT(handle != 0);
+	}
+	ret = ext3_get_block_handle(handle, inode, iblock, bh_result, create);
+	return ret;
+}
+
+/*
+ * `handle' can be NULL if create is zero
+ */
+struct buffer_head *ext3_getblk(handle_t *handle, struct inode * inode,
+				long block, int create, int * errp)
+{
+	struct buffer_head dummy;
+	int fatal = 0, err;
+	
+	J_ASSERT(handle != NULL || create == 0);
+
+	dummy.b_state = 0;
+	dummy.b_blocknr = -1000;
+	buffer_trace_init(&dummy.b_history);
+	*errp = ext3_get_block_handle(handle, inode, block, &dummy, create);
+	if (!*errp && buffer_mapped(&dummy)) {
+		struct buffer_head *bh;
+		bh = sb_getblk(inode->i_sb, dummy.b_blocknr);
+		if (buffer_new(&dummy)) {
+			J_ASSERT(create != 0);
+			J_ASSERT(handle != 0);
+
+			/* Now that we do not always journal data, we
+			   should keep in mind whether this should
+			   always journal the new buffer as metadata.
+			   For now, regular file writes use
+			   ext3_get_block instead, so it's not a
+			   problem. */
+			lock_kernel();
+			lock_buffer(bh);
+			BUFFER_TRACE(bh, "call get_create_access");
+			fatal = ext3_journal_get_create_access(handle, bh);
+			if (!fatal) {
+				memset(bh->b_data, 0,
+				       inode->i_sb->s_blocksize);
+				mark_buffer_uptodate(bh, 1);
+			}
+			unlock_buffer(bh);
+			BUFFER_TRACE(bh, "call ext3_journal_dirty_metadata");
+			err = ext3_journal_dirty_metadata(handle, bh);
+			if (!fatal) fatal = err;
+			unlock_kernel();
+		} else {
+			BUFFER_TRACE(bh, "not a new buffer");
+		}
+		if (fatal) {
+			*errp = fatal;
+			brelse(bh);
+			bh = NULL;
+		}
+		return bh;
+	}
+	return NULL;
+}
+
+struct buffer_head *ext3_bread(handle_t *handle, struct inode * inode,
+			       int block, int create, int *err)
+{
+	struct buffer_head * bh;
+	int prev_blocks;
+
+	prev_blocks = inode->i_blocks;
+
+	bh = ext3_getblk (handle, inode, block, create, err);
+	if (!bh)
+		return bh;
+#ifdef EXT3_PREALLOCATE
+	/*
+	 * If the inode has grown, and this is a directory, then use a few
+	 * more of the preallocated blocks to keep directory fragmentation
+	 * down.  The preallocated blocks are guaranteed to be contiguous.
+	 */
+	if (create &&
+	    S_ISDIR(inode->i_mode) &&
+	    inode->i_blocks > prev_blocks &&
+	    EXT3_HAS_COMPAT_FEATURE(inode->i_sb,
+				    EXT3_FEATURE_COMPAT_DIR_PREALLOC)) {
+		int i;
+		struct buffer_head *tmp_bh;
+
+		for (i = 1;
+		     inode->u.ext3_i.i_prealloc_count &&
+		     i < EXT3_SB(inode->i_sb)->s_es->s_prealloc_dir_blocks;
+		     i++) {
+			/*
+			 * ext3_getblk will zero out the contents of the
+			 * directory for us
+			 */
+			tmp_bh = ext3_getblk(handle, inode,
+						block+i, create, err);
+			if (!tmp_bh) {
+				brelse (bh);
+				return 0;
+			}
+			brelse (tmp_bh);
+		}
+	}
+#endif
+	if (buffer_uptodate(bh))
+		return bh;
+	ll_rw_block (READ, 1, &bh);
+	wait_on_buffer (bh);
+	if (buffer_uptodate(bh))
+		return bh;
+	brelse (bh);
+	*err = -EIO;
+	return NULL;
+}
+
+static int walk_page_buffers(	handle_t *handle,
+				struct buffer_head *head,
+				unsigned from,
+				unsigned to,
+				int *partial,
+				int (*fn)(	handle_t *handle,
+						struct buffer_head *bh))
+{
+	struct buffer_head *bh;
+	unsigned block_start, block_end;
+	unsigned blocksize = head->b_size;
+	int err, ret = 0;
+
+	for (	bh = head, block_start = 0;
+		ret == 0 && (bh != head || !block_start);
+	    	block_start = block_end, bh = bh->b_this_page)
+	{
+		block_end = block_start + blocksize;
+		if (block_end <= from || block_start >= to) {
+			if (partial && !buffer_uptodate(bh))
+				*partial = 1;
+			continue;
+		}
+		err = (*fn)(handle, bh);
+		if (!ret)
+			ret = err;
+	}
+	return ret;
+}
+
+/*
+ * To preserve ordering, it is essential that the hole instantiation and
+ * the data write be encapsulated in a single transaction.  We cannot
+ * close off a transaction and start a new one between the ext3_get_block()
+ * and the commit_write().  So doing the journal_start at the start of
+ * prepare_write() is the right place.
+ *
+ * Also, this function can nest inside ext3_writepage() ->
+ * block_write_full_page(). In that case, we *know* that ext3_writepage()
+ * has generated enough buffer credits to do the whole page.  So we won't
+ * block on the journal in that case, which is good, because the caller may
+ * be PF_MEMALLOC.
+ *
+ * By accident, ext3 can be reentered when a transaction is open via
+ * quota file writes.  If we were to commit the transaction while thus
+ * reentered, there can be a deadlock - we would be holding a quota
+ * lock, and the commit would never complete if another thread had a
+ * transaction open and was blocking on the quota lock - a ranking
+ * violation.
+ *
+ * So what we do is to rely on the fact that journal_stop/journal_start
+ * will _not_ run commit under these circumstances because handle->h_ref
+ * is elevated.  We'll still have enough credits for the tiny quotafile
+ * write.  
+ */
+
+static int do_journal_get_write_access(handle_t *handle, 
+				       struct buffer_head *bh)
+{
+	return ext3_journal_get_write_access(handle, bh);
+}
+
+static int ext3_prepare_write(struct file *file, struct page *page,
+			      unsigned from, unsigned to)
+{
+	struct inode *inode = page->mapping->host;
+	int ret, needed_blocks = ext3_writepage_trans_blocks(inode);
+	handle_t *handle;
+
+	lock_kernel();
+	handle = ext3_journal_start(inode, needed_blocks);
+	if (IS_ERR(handle)) {
+		ret = PTR_ERR(handle);
+		goto out;
+	}
+	unlock_kernel();
+	ret = block_prepare_write(page, from, to, ext3_get_block);
+	lock_kernel();
+	if (ret != 0)
+		goto prepare_write_failed;
+
+	if (ext3_should_journal_data(inode)) {
+		ret = walk_page_buffers(handle, page->buffers,
+				from, to, NULL, do_journal_get_write_access);
+		if (ret) {
+			/*
+			 * We're going to fail this prepare_write(),
+			 * so commit_write() will not be called.
+			 * We need to undo block_prepare_write()'s kmap().
+			 * AKPM: Do we need to clear PageUptodate?  I don't
+			 * think so.
+			 */
+			kunmap(page);
+		}
+	}
+prepare_write_failed:
+	if (ret)
+		ext3_journal_stop(handle, inode);
+out:
+	unlock_kernel();
+	return ret;
+}
+
+static int journal_dirty_sync_data(handle_t *handle, struct buffer_head *bh)
+{
+	return ext3_journal_dirty_data(handle, bh, 0);
+}
+
+/*
+ * For ext3_writepage().  We also brelse() the buffer to account for
+ * the bget() which ext3_writepage() performs.
+ */
+static int journal_dirty_async_data(handle_t *handle, struct buffer_head *bh)
+{
+	int ret = ext3_journal_dirty_data(handle, bh, 1);
+	__brelse(bh);
+	return ret;
+}
+
+/* For commit_write() in data=journal mode */
+static int commit_write_fn(handle_t *handle, struct buffer_head *bh)
+{
+	set_bit(BH_Uptodate, &bh->b_state);
+	return ext3_journal_dirty_metadata(handle, bh);
+}
+
+/*
+ * We need to pick up the new inode size which generic_commit_write gave us
+ * `file' can be NULL - eg, when called from block_symlink().
+ *
+ * ext3 inode->i_dirty_buffers policy:  If we're journalling data we
+ * definitely don't want them to appear on the inode at all - instead
+ * we need to manage them at the JBD layer and we need to intercept
+ * the relevant sync operations and translate them into journal operations.
+ *
+ * If we're not journalling data then we can just leave the buffers
+ * on ->i_dirty_buffers.  If someone writes them out for us then thanks.
+ * Otherwise we'll do it in commit, if we're using ordered data.
+ */
+
+static int ext3_commit_write(struct file *file, struct page *page,
+			     unsigned from, unsigned to)
+{
+	handle_t *handle = ext3_journal_current_handle();
+	struct inode *inode = page->mapping->host;
+	int ret = 0, ret2;
+
+	lock_kernel();
+	if (ext3_should_journal_data(inode)) {
+		/*
+		 * Here we duplicate the generic_commit_write() functionality
+		 */
+		int partial = 0;
+		loff_t pos = ((loff_t)page->index << PAGE_CACHE_SHIFT) + to;
+
+		ret = walk_page_buffers(handle, page->buffers,
+			from, to, &partial, commit_write_fn);
+		if (!partial)
+			SetPageUptodate(page);
+		kunmap(page);
+		if (pos > inode->i_size)
+			inode->i_size = pos;
+		EXT3_I(inode)->i_state |= EXT3_STATE_JDATA;
+	} else {
+		if (ext3_should_order_data(inode)) {
+			ret = walk_page_buffers(handle, page->buffers,
+				from, to, NULL, journal_dirty_sync_data);
+		}
+		/* Be careful here if generic_commit_write becomes a
+		 * required invocation after block_prepare_write. */
+		if (ret == 0) {
+			ret = generic_commit_write(file, page, from, to);
+		} else {
+			/*
+			 * block_prepare_write() was called, but we're not
+			 * going to call generic_commit_write().  So we
+			 * need to perform generic_commit_write()'s kunmap
+			 * by hand.
+			 */
+			kunmap(page);
+		}
+	}
+	if (inode->i_size > inode->u.ext3_i.i_disksize) {
+		inode->u.ext3_i.i_disksize = inode->i_size;
+		ret2 = ext3_mark_inode_dirty(handle, inode);
+		if (!ret) 
+			ret = ret2;
+	}
+	ret2 = ext3_journal_stop(handle, inode);
+	unlock_kernel();
+	if (!ret)
+		ret = ret2;
+	return ret;
+}
+
+/* 
+ * bmap() is special.  It gets used by applications such as lilo and by
+ * the swapper to find the on-disk block of a specific piece of data.
+ *
+ * Naturally, this is dangerous if the block concerned is still in the
+ * journal.  If somebody makes a swapfile on an ext3 data-journaling
+ * filesystem and enables swap, then they may get a nasty shock when the
+ * data getting swapped to that swapfile suddenly gets overwritten by
+ * the original zero's written out previously to the journal and
+ * awaiting writeback in the kernel's buffer cache. 
+ *
+ * So, if we see any bmap calls here on a modified, data-journaled file,
+ * take extra steps to flush any blocks which might be in the cache. 
+ */
+static int ext3_bmap(struct address_space *mapping, long block)
+{
+	struct inode *inode = mapping->host;
+	journal_t *journal;
+	int err;
+	
+	if (EXT3_I(inode)->i_state & EXT3_STATE_JDATA) {
+		/* 
+		 * This is a REALLY heavyweight approach, but the use of
+		 * bmap on dirty files is expected to be extremely rare:
+		 * only if we run lilo or swapon on a freshly made file
+		 * do we expect this to happen. 
+		 *
+		 * (bmap requires CAP_SYS_RAWIO so this does not
+		 * represent an unprivileged user DOS attack --- we'd be
+		 * in trouble if mortal users could trigger this path at
+		 * will.) 
+		 *
+		 * NB. EXT3_STATE_JDATA is not set on files other than
+		 * regular files.  If somebody wants to bmap a directory
+		 * or symlink and gets confused because the buffer
+		 * hasn't yet been flushed to disk, they deserve
+		 * everything they get.
+		 */
+		
+		EXT3_I(inode)->i_state &= ~EXT3_STATE_JDATA;
+		journal = EXT3_JOURNAL(inode);
+		journal_lock_updates(journal);
+		err = journal_flush(journal);
+		journal_unlock_updates(journal);
+		
+		if (err)
+			return 0;
+	}
+	
+	return generic_block_bmap(mapping,block,ext3_get_block);
+}
+
+static int bget_one(handle_t *handle, struct buffer_head *bh)
+{
+	atomic_inc(&bh->b_count);
+	return 0;
+}
+
+/*
+ * Note that we always start a transaction even if we're not journalling
+ * data.  This is to preserve ordering: any hole instantiation within
+ * __block_write_full_page -> ext3_get_block() should be journalled
+ * along with the data so we don't crash and then get metadata which
+ * refers to old data.
+ *
+ * In all journalling modes block_write_full_page() will start the I/O.
+ *
+ * Problem:
+ *
+ *	ext3_writepage() -> kmalloc() -> __alloc_pages() -> page_launder() ->
+ *		ext3_writepage()
+ *
+ * Similar for:
+ *
+ *	ext3_file_write() -> generic_file_write() -> __alloc_pages() -> ...
+ *
+ * Same applies to ext3_get_block().  We will deadlock on various things like
+ * lock_journal and i_truncate_sem.
+ *
+ * Setting PF_MEMALLOC here doesn't work - too many internal memory
+ * allocations fail.
+ *
+ * 16May01: If we're reentered then journal_current_handle() will be
+ *	    non-zero. We simply *return*.
+ *
+ * 1 July 2001: @@@ FIXME:
+ *   In journalled data mode, a data buffer may be metadata against the
+ *   current transaction.  But the same file is part of a shared mapping
+ *   and someone does a writepage() on it.
+ *
+ *   We will move the buffer onto the async_data list, but *after* it has
+ *   been dirtied. So there's a small window where we have dirty data on
+ *   BJ_Metadata.
+ *
+ *   Note that this only applies to the last partial page in the file.  The
+ *   bit which block_write_full_page() uses prepare/commit for.  (That's
+ *   broken code anyway: it's wrong for msync()).
+ *
+ *   It's a rare case: affects the final partial page, for journalled data
+ *   where the file is subject to bith write() and writepage() in the same
+ *   transction.  To fix it we'll need a custom block_write_full_page().
+ *   We'll probably need that anyway for journalling writepage() output.
+ *
+ * We don't honour synchronous mounts for writepage().  That would be
+ * disastrous.  Any write() or metadata operation will sync the fs for
+ * us.
+ */
+static int ext3_writepage(struct page *page)
+{
+	struct inode *inode = page->mapping->host;
+	struct buffer_head *page_buffers;
+	handle_t *handle = NULL;
+	int ret = 0, err;
+	int needed;
+	int order_data;
+
+	J_ASSERT(PageLocked(page));
+	
+	/*
+	 * We give up here if we're reentered, because it might be
+	 * for a different filesystem.  One *could* look for a
+	 * nested transaction opportunity.
+	 */
+	lock_kernel();
+	if (ext3_journal_current_handle())
+		goto out_fail;
+
+	needed = ext3_writepage_trans_blocks(inode);
+	if (current->flags & PF_MEMALLOC)
+		handle = ext3_journal_try_start(inode, needed);
+	else
+		handle = ext3_journal_start(inode, needed);
+				
+	if (IS_ERR(handle)) {
+		ret = PTR_ERR(handle);
+		goto out_fail;
+	}
+
+	order_data = ext3_should_order_data(inode) ||
+			ext3_should_journal_data(inode);
+
+	unlock_kernel();
+
+	page_buffers = NULL;	/* Purely to prevent compiler warning */
+
+	/* bget() all the buffers */
+	if (order_data) {
+		if (!page->buffers)
+			create_empty_buffers(page,
+				inode->i_dev, inode->i_sb->s_blocksize);
+		page_buffers = page->buffers;
+		walk_page_buffers(handle, page_buffers, 0,
+				PAGE_CACHE_SIZE, NULL, bget_one);
+	}
+
+	ret = block_write_full_page(page, ext3_get_block);
+
+	/*
+	 * The page can become unlocked at any point now, and
+	 * truncate can then come in and change things.  So we
+	 * can't touch *page from now on.  But *page_buffers is
+	 * safe due to elevated refcount.
+	 */
+
+	handle = ext3_journal_current_handle();
+	lock_kernel();
+
+	/* And attach them to the current transaction */
+	if (order_data) {
+		err = walk_page_buffers(handle, page_buffers,
+			0, PAGE_CACHE_SIZE, NULL, journal_dirty_async_data);
+		if (!ret)
+			ret = err;
+	}
+
+	err = ext3_journal_stop(handle, inode);
+	if (!ret)
+		ret = err;
+	unlock_kernel();
+	return ret;
+
+out_fail:
+	
+	unlock_kernel();
+	SetPageDirty(page);
+	UnlockPage(page);
+	return ret;
+}
+
+static int ext3_readpage(struct file *file, struct page *page)
+{
+	return block_read_full_page(page,ext3_get_block);
+}
+
+
+static int ext3_flushpage(struct page *page, unsigned long offset)
+{
+	journal_t *journal = EXT3_JOURNAL(page->mapping->host);
+	return journal_flushpage(journal, page, offset);
+}
+
+static int ext3_releasepage(struct page *page, int wait)
+{
+	journal_t *journal = EXT3_JOURNAL(page->mapping->host);
+	return journal_try_to_free_buffers(journal, page, wait);
+}
+
+
+struct address_space_operations ext3_aops = {
+	readpage:	ext3_readpage,		/* BKL not held.  Don't need */
+	writepage:	ext3_writepage,		/* BKL not held.  We take it */
+	sync_page:	block_sync_page,
+	prepare_write:	ext3_prepare_write,	/* BKL not held.  We take it */
+	commit_write:	ext3_commit_write,	/* BKL not held.  We take it */
+	bmap:		ext3_bmap,		/* BKL held */
+	flushpage:	ext3_flushpage,		/* BKL not held.  Don't need */
+	releasepage:	ext3_releasepage,	/* BKL not held.  Don't need */
+};
+
+/*
+ * ext3_block_truncate_page() zeroes out a mapping from file offset `from'
+ * up to the end of the block which corresponds to `from'.
+ * This required during truncate. We need to physically zero the tail end
+ * of that block so it doesn't yield old data if the file is later grown.
+ */
+static int ext3_block_truncate_page(handle_t *handle,
+		struct address_space *mapping, loff_t from)
+{
+	unsigned long index = from >> PAGE_CACHE_SHIFT;
+	unsigned offset = from & (PAGE_CACHE_SIZE-1);
+	unsigned blocksize, iblock, length, pos;
+	struct inode *inode = mapping->host;
+	struct page *page;
+	struct buffer_head *bh;
+	int err;
+
+	blocksize = inode->i_sb->s_blocksize;
+	length = offset & (blocksize - 1);
+
+	/* Block boundary? Nothing to do */
+	if (!length)
+		return 0;
+
+	length = blocksize - length;
+	iblock = index << (PAGE_CACHE_SHIFT - inode->i_sb->s_blocksize_bits);
+
+	page = grab_cache_page(mapping, index);
+	err = -ENOMEM;
+	if (!page)
+		goto out;
+
+	if (!page->buffers)
+		create_empty_buffers(page, inode->i_dev, blocksize);
+
+	/* Find the buffer that contains "offset" */
+	bh = page->buffers;
+	pos = blocksize;
+	while (offset >= pos) {
+		bh = bh->b_this_page;
+		iblock++;
+		pos += blocksize;
+	}
+
+	err = 0;
+	if (!buffer_mapped(bh)) {
+		/* Hole? Nothing to do */
+		if (buffer_uptodate(bh))
+			goto unlock;
+		ext3_get_block(inode, iblock, bh, 0);
+		/* Still unmapped? Nothing to do */
+		if (!buffer_mapped(bh))
+			goto unlock;
+	}
+
+	/* Ok, it's mapped. Make sure it's up-to-date */
+	if (Page_Uptodate(page))
+		set_bit(BH_Uptodate, &bh->b_state);
+
+	if (!buffer_uptodate(bh)) {
+		err = -EIO;
+		ll_rw_block(READ, 1, &bh);
+		wait_on_buffer(bh);
+		/* Uhhuh. Read error. Complain and punt. */
+		if (!buffer_uptodate(bh))
+			goto unlock;
+	}
+
+	if (ext3_should_journal_data(inode)) {
+		BUFFER_TRACE(bh, "get write access");
+		err = ext3_journal_get_write_access(handle, bh);
+		if (err)
+			goto unlock;
+	}
+	
+	memset(kmap(page) + offset, 0, length);
+	flush_dcache_page(page);
+	kunmap(page);
+
+	BUFFER_TRACE(bh, "zeroed end of block");
+
+	err = 0;
+	if (ext3_should_journal_data(inode)) {
+		err = ext3_journal_dirty_metadata(handle, bh);
+	} else {
+		if (ext3_should_order_data(inode))
+			err = ext3_journal_dirty_data(handle, bh, 0);
+		__mark_buffer_dirty(bh);
+	}
+
+unlock:
+	UnlockPage(page);
+	page_cache_release(page);
+out:
+	return err;
+}
+
+/*
+ * Probably it should be a library function... search for first non-zero word
+ * or memcmp with zero_page, whatever is better for particular architecture.
+ * Linus?
+ */
+static inline int all_zeroes(u32 *p, u32 *q)
+{
+	while (p < q)
+		if (*p++)
+			return 0;
+	return 1;
+}
+
+/**
+ *	ext3_find_shared - find the indirect blocks for partial truncation.
+ *	@inode:	  inode in question
+ *	@depth:	  depth of the affected branch
+ *	@offsets: offsets of pointers in that branch (see ext3_block_to_path)
+ *	@chain:	  place to store the pointers to partial indirect blocks
+ *	@top:	  place to the (detached) top of branch
+ *
+ *	This is a helper function used by ext3_truncate().
+ *
+ *	When we do truncate() we may have to clean the ends of several
+ *	indirect blocks but leave the blocks themselves alive. Block is
+ *	partially truncated if some data below the new i_size is refered
+ *	from it (and it is on the path to the first completely truncated
+ *	data block, indeed).  We have to free the top of that path along
+ *	with everything to the right of the path. Since no allocation
+ *	past the truncation point is possible until ext3_truncate()
+ *	finishes, we may safely do the latter, but top of branch may
+ *	require special attention - pageout below the truncation point
+ *	might try to populate it.
+ *
+ *	We atomically detach the top of branch from the tree, store the
+ *	block number of its root in *@top, pointers to buffer_heads of
+ *	partially truncated blocks - in @chain[].bh and pointers to
+ *	their last elements that should not be removed - in
+ *	@chain[].p. Return value is the pointer to last filled element
+ *	of @chain.
+ *
+ *	The work left to caller to do the actual freeing of subtrees:
+ *		a) free the subtree starting from *@top
+ *		b) free the subtrees whose roots are stored in
+ *			(@chain[i].p+1 .. end of @chain[i].bh->b_data)
+ *		c) free the subtrees growing from the inode past the @chain[0].
+ *			(no partially truncated stuff there).  */
+
+static Indirect *ext3_find_shared(struct inode *inode,
+				int depth,
+				int offsets[4],
+				Indirect chain[4],
+				u32 *top)
+{
+	Indirect *partial, *p;
+	int k, err;
+
+	*top = 0;
+	/* Make k index the deepest non-null offest + 1 */
+	for (k = depth; k > 1 && !offsets[k-1]; k--)
+		;
+	partial = ext3_get_branch(inode, k, offsets, chain, &err);
+	/* Writer: pointers */
+	if (!partial)
+		partial = chain + k-1;
+	/*
+	 * If the branch acquired continuation since we've looked at it -
+	 * fine, it should all survive and (new) top doesn't belong to us.
+	 */
+	if (!partial->key && *partial->p)
+		/* Writer: end */
+		goto no_top;
+	for (p=partial; p>chain && all_zeroes((u32*)p->bh->b_data,p->p); p--)
+		;
+	/*
+	 * OK, we've found the last block that must survive. The rest of our
+	 * branch should be detached before unlocking. However, if that rest
+	 * of branch is all ours and does not grow immediately from the inode
+	 * it's easier to cheat and just decrement partial->p.
+	 */
+	if (p == chain + k - 1 && p > chain) {
+		p->p--;
+	} else {
+		*top = *p->p;
+		/* Nope, don't do this in ext3.  Must leave the tree intact */
+#if 0
+		*p->p = 0;
+#endif
+	}
+	/* Writer: end */
+
+	while(partial > p)
+	{
+		brelse(partial->bh);
+		partial--;
+	}
+no_top:
+	return partial;
+}
+
+/*
+ * Zero a number of block pointers in either an inode or an indirect block.
+ * If we restart the transaction we must again get write access to the
+ * indirect block for further modification.
+ *
+ * We release `count' blocks on disk, but (last - first) may be greater
+ * than `count' because there can be holes in there.
+ */
+static void
+ext3_clear_blocks(handle_t *handle, struct inode *inode, struct buffer_head *bh,
+		unsigned long block_to_free, unsigned long count,
+		u32 *first, u32 *last)
+{
+	u32 *p;
+	if (try_to_extend_transaction(handle, inode)) {
+		if (bh) {
+			BUFFER_TRACE(bh, "call ext3_journal_dirty_metadata");
+			ext3_journal_dirty_metadata(handle, bh);
+		}
+		ext3_mark_inode_dirty(handle, inode);
+		ext3_journal_test_restart(handle, inode);
+		BUFFER_TRACE(bh, "get_write_access");
+		ext3_journal_get_write_access(handle, bh);
+	}
+
+	/*
+	 * Any buffers which are on the journal will be in memory. We find
+	 * them on the hash table so journal_revoke() will run journal_forget()
+	 * on them.  We've already detached each block from the file, so
+	 * bforget() in journal_forget() should be safe.
+	 *
+	 * AKPM: turn on bforget in journal_forget()!!!
+	 */
+	for (p = first; p < last; p++) {
+		u32 nr = le32_to_cpu(*p);
+		if (nr) {
+			struct buffer_head *bh;
+
+			*p = 0;
+			bh = sb_get_hash_table(inode->i_sb, nr);
+			ext3_forget(handle, 0, inode, bh, nr);
+		}
+	}
+
+	ext3_free_blocks(handle, inode, block_to_free, count);
+}
+
+/**
+ * ext3_free_data - free a list of data blocks
+ * @handle:	handle for this transaction
+ * @inode:	inode we are dealing with
+ * @this_bh:	indirect buffer_head which contains *@first and *@last
+ * @first:	array of block numbers
+ * @last:	points immediately past the end of array
+ *
+ * We are freeing all blocks refered from that array (numbers are stored as
+ * little-endian 32-bit) and updating @inode->i_blocks appropriately.
+ *
+ * We accumulate contiguous runs of blocks to free.  Conveniently, if these
+ * blocks are contiguous then releasing them at one time will only affect one
+ * or two bitmap blocks (+ group descriptor(s) and superblock) and we won't
+ * actually use a lot of journal space.
+ *
+ * @this_bh will be %NULL if @first and @last point into the inode's direct
+ * block pointers.
+ */
+static void ext3_free_data(handle_t *handle, struct inode *inode,
+			   struct buffer_head *this_bh, u32 *first, u32 *last)
+{
+	unsigned long block_to_free = 0;    /* Starting block # of a run */
+	unsigned long count = 0;	    /* Number of blocks in the run */ 
+	u32 *block_to_free_p = NULL;	    /* Pointer into inode/ind
+					       corresponding to
+					       block_to_free */
+	unsigned long nr;		    /* Current block # */
+	u32 *p;				    /* Pointer into inode/ind
+					       for current block */
+	int err;
+
+	if (this_bh) {				/* For indirect block */
+		BUFFER_TRACE(this_bh, "get_write_access");
+		err = ext3_journal_get_write_access(handle, this_bh);
+		/* Important: if we can't update the indirect pointers
+		 * to the blocks, we can't free them. */
+		if (err)
+			return;
+	}
+
+	for (p = first; p < last; p++) {
+		nr = le32_to_cpu(*p);
+		if (nr) {
+			/* accumulate blocks to free if they're contiguous */
+			if (count == 0) {
+				block_to_free = nr;
+				block_to_free_p = p;
+				count = 1;
+			} else if (nr == block_to_free + count) {
+				count++;
+			} else {
+				ext3_clear_blocks(handle, inode, this_bh, 
+						  block_to_free,
+						  count, block_to_free_p, p);
+				block_to_free = nr;
+				block_to_free_p = p;
+				count = 1;
+			}
+		}
+	}
+
+	if (count > 0)
+		ext3_clear_blocks(handle, inode, this_bh, block_to_free,
+				  count, block_to_free_p, p);
+
+	if (this_bh) {
+		BUFFER_TRACE(this_bh, "call ext3_journal_dirty_metadata");
+		ext3_journal_dirty_metadata(handle, this_bh);
+	}
+}
+
+/**
+ *	ext3_free_branches - free an array of branches
+ *	@handle: JBD handle for this transaction
+ *	@inode:	inode we are dealing with
+ *	@parent_bh: the buffer_head which contains *@first and *@last
+ *	@first:	array of block numbers
+ *	@last:	pointer immediately past the end of array
+ *	@depth:	depth of the branches to free
+ *
+ *	We are freeing all blocks refered from these branches (numbers are
+ *	stored as little-endian 32-bit) and updating @inode->i_blocks
+ *	appropriately.
+ */
+static void ext3_free_branches(handle_t *handle, struct inode *inode,
+			       struct buffer_head *parent_bh,
+			       u32 *first, u32 *last, int depth)
+{
+	unsigned long nr;
+	u32 *p;
+
+	if (is_handle_aborted(handle))
+		return;
+	
+	if (depth--) {
+		struct buffer_head *bh;
+		int addr_per_block = EXT3_ADDR_PER_BLOCK(inode->i_sb);
+		p = last;
+		while (--p >= first) {
+			nr = le32_to_cpu(*p);
+			if (!nr)
+				continue;		/* A hole */
+
+			/* Go read the buffer for the next level down */
+			bh = sb_bread(inode->i_sb, nr);
+
+			/*
+			 * A read failure? Report error and clear slot
+			 * (should be rare).
+			 */
+			if (!bh) {
+				ext3_error(inode->i_sb, "ext3_free_branches",
+					   "Read failure, inode=%ld, block=%ld",
+					   inode->i_ino, nr);
+				continue;
+			}
+
+			/* This zaps the entire block.  Bottom up. */
+			BUFFER_TRACE(bh, "free child branches");
+			ext3_free_branches(handle, inode, bh, (u32*)bh->b_data,
+					   (u32*)bh->b_data + addr_per_block,
+					   depth);
+
+			/*
+			 * We've probably journalled the indirect block several
+			 * times during the truncate.  But it's no longer
+			 * needed and we now drop it from the transaction via
+			 * journal_revoke().
+			 *
+			 * That's easy if it's exclusively part of this
+			 * transaction.  But if it's part of the committing
+			 * transaction then journal_forget() will simply
+			 * brelse() it.  That means that if the underlying
+			 * block is reallocated in ext3_get_block(),
+			 * unmap_underlying_metadata() will find this block
+			 * and will try to get rid of it.  damn, damn.
+			 *
+			 * If this block has already been committed to the
+			 * journal, a revoke record will be written.  And
+			 * revoke records must be emitted *before* clearing
+			 * this block's bit in the bitmaps.
+			 */
+			ext3_forget(handle, 1, inode, bh, bh->b_blocknr);
+
+			/*
+			 * Everything below this this pointer has been
+			 * released.  Now let this top-of-subtree go.
+			 *
+			 * We want the freeing of this indirect block to be
+			 * atomic in the journal with the updating of the
+			 * bitmap block which owns it.  So make some room in
+			 * the journal.
+			 *
+			 * We zero the parent pointer *after* freeing its
+			 * pointee in the bitmaps, so if extend_transaction()
+			 * for some reason fails to put the bitmap changes and
+			 * the release into the same transaction, recovery
+			 * will merely complain about releasing a free block,
+			 * rather than leaking blocks.
+			 */
+			if (is_handle_aborted(handle))
+				return;
+			if (try_to_extend_transaction(handle, inode)) {
+				ext3_mark_inode_dirty(handle, inode);
+				ext3_journal_test_restart(handle, inode);
+			}
+
+			ext3_free_blocks(handle, inode, nr, 1);
+
+			if (parent_bh) {
+				/*
+				 * The block which we have just freed is
+				 * pointed to by an indirect block: journal it
+				 */
+				BUFFER_TRACE(parent_bh, "get_write_access");
+				if (!ext3_journal_get_write_access(handle,
+								   parent_bh)){
+					*p = 0;
+					BUFFER_TRACE(parent_bh,
+					"call ext3_journal_dirty_metadata");
+					ext3_journal_dirty_metadata(handle, 
+								    parent_bh);
+				}
+			}
+		}
+	} else {
+		/* We have reached the bottom of the tree. */
+		BUFFER_TRACE(parent_bh, "free data blocks");
+		ext3_free_data(handle, inode, parent_bh, first, last);
+	}
+}
+
+/*
+ * ext3_truncate()
+ *
+ * We block out ext3_get_block() block instantiations across the entire
+ * transaction, and VFS/VM ensures that ext3_truncate() cannot run
+ * simultaneously on behalf of the same inode.
+ *
+ * As we work through the truncate and commmit bits of it to the journal there
+ * is one core, guiding principle: the file's tree must always be consistent on
+ * disk.  We must be able to restart the truncate after a crash.
+ *
+ * The file's tree may be transiently inconsistent in memory (although it
+ * probably isn't), but whenever we close off and commit a journal transaction,
+ * the contents of (the filesystem + the journal) must be consistent and
+ * restartable.  It's pretty simple, really: bottom up, right to left (although
+ * left-to-right works OK too).
+ *
+ * Note that at recovery time, journal replay occurs *before* the restart of
+ * truncate against the orphan inode list.
+ *
+ * The committed inode has the new, desired i_size (which is the same as
+ * i_disksize in this case).  After a crash, ext3_orphan_cleanup() will see
+ * that this inode's truncate did not complete and it will again call
+ * ext3_truncate() to have another go.  So there will be instantiated blocks
+ * to the right of the truncation point in a crashed ext3 filesystem.  But
+ * that's fine - as long as they are linked from the inode, the post-crash
+ * ext3_truncate() run will find them and release them.
+ */
+
+void ext3_truncate(struct inode * inode)
+{
+	handle_t *handle;
+	u32 *i_data = inode->u.ext3_i.i_data;
+	int addr_per_block = EXT3_ADDR_PER_BLOCK(inode->i_sb);
+	int offsets[4];
+	Indirect chain[4];
+	Indirect *partial;
+	int nr = 0;
+	int n;
+	long last_block;
+	unsigned blocksize;
+
+	if (!(S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode) ||
+	    S_ISLNK(inode->i_mode)))
+		return;
+	if (IS_APPEND(inode) || IS_IMMUTABLE(inode))
+		return;
+
+	ext3_discard_prealloc(inode);
+
+	handle = start_transaction(inode);
+	if (IS_ERR(handle))
+		return;		/* AKPM: return what? */
+
+	blocksize = inode->i_sb->s_blocksize;
+	last_block = (inode->i_size + blocksize-1)
+					>> EXT3_BLOCK_SIZE_BITS(inode->i_sb);
+
+	ext3_block_truncate_page(handle, inode->i_mapping, inode->i_size);
+		
+
+	n = ext3_block_to_path(inode, last_block, offsets);
+	if (n == 0)
+		goto out_stop;	/* error */
+
+	/*
+	 * OK.  This truncate is going to happen.  We add the inode to the
+	 * orphan list, so that if this truncate spans multiple transactions,
+	 * and we crash, we will resume the truncate when the filesystem
+	 * recovers.  It also marks the inode dirty, to catch the new size.
+	 *
+	 * Implication: the file must always be in a sane, consistent
+	 * truncatable state while each transaction commits.
+	 */
+	if (ext3_orphan_add(handle, inode))
+		goto out_stop;
+
+	/*
+	 * The orphan list entry will now protect us from any crash which
+	 * occurs before the truncate completes, so it is now safe to propagate
+	 * the new, shorter inode size (held for now in i_size) into the
+	 * on-disk inode. We do this via i_disksize, which is the value which
+	 * ext3 *really* writes onto the disk inode.
+	 */
+	inode->u.ext3_i.i_disksize = inode->i_size;
+
+	/*
+	 * From here we block out all ext3_get_block() callers who want to
+	 * modify the block allocation tree.
+	 */
+	down_write(&inode->u.ext3_i.truncate_sem);
+
+	if (n == 1) {		/* direct blocks */
+		ext3_free_data(handle, inode, NULL, i_data+offsets[0],
+			       i_data + EXT3_NDIR_BLOCKS);
+		goto do_indirects;
+	}
+
+	partial = ext3_find_shared(inode, n, offsets, chain, &nr);
+	/* Kill the top of shared branch (not detached) */
+	if (nr) {
+		if (partial == chain) {
+			/* Shared branch grows from the inode */
+			ext3_free_branches(handle, inode, NULL,
+					   &nr, &nr+1, (chain+n-1) - partial);
+			*partial->p = 0;
+			/*
+			 * We mark the inode dirty prior to restart,
+			 * and prior to stop.  No need for it here.
+			 */
+		} else {
+			/* Shared branch grows from an indirect block */
+			BUFFER_TRACE(partial->bh, "get_write_access");
+			ext3_free_branches(handle, inode, partial->bh,
+					partial->p,
+					partial->p+1, (chain+n-1) - partial);
+		}
+	}
+	/* Clear the ends of indirect blocks on the shared branch */
+	while (partial > chain) {
+		ext3_free_branches(handle, inode, partial->bh, partial->p + 1,
+				   (u32*)partial->bh->b_data + addr_per_block,
+				   (chain+n-1) - partial);
+		BUFFER_TRACE(partial->bh, "call brelse");
+		brelse (partial->bh);
+		partial--;
+	}
+do_indirects:
+	/* Kill the remaining (whole) subtrees */
+	switch (offsets[0]) {
+		default:
+			nr = i_data[EXT3_IND_BLOCK];
+			if (nr) {
+				ext3_free_branches(handle, inode, NULL,
+						   &nr, &nr+1, 1);
+				i_data[EXT3_IND_BLOCK] = 0;
+			}
+		case EXT3_IND_BLOCK:
+			nr = i_data[EXT3_DIND_BLOCK];
+			if (nr) {
+				ext3_free_branches(handle, inode, NULL,
+						   &nr, &nr+1, 2);
+				i_data[EXT3_DIND_BLOCK] = 0;
+			}
+		case EXT3_DIND_BLOCK:
+			nr = i_data[EXT3_TIND_BLOCK];
+			if (nr) {
+				ext3_free_branches(handle, inode, NULL,
+						   &nr, &nr+1, 3);
+				i_data[EXT3_TIND_BLOCK] = 0;
+			}
+		case EXT3_TIND_BLOCK:
+			;
+	}
+	up_write(&inode->u.ext3_i.truncate_sem);
+	inode->i_mtime = inode->i_ctime = CURRENT_TIME;
+	ext3_mark_inode_dirty(handle, inode);
+
+	/* In a multi-transaction truncate, we only make the final
+	 * transaction synchronous */
+	if (IS_SYNC(inode))
+		handle->h_sync = 1;
+out_stop:
+	/*
+	 * If this was a simple ftruncate(), and the file will remain alive
+	 * then we need to clear up the orphan record which we created above.
+	 * However, if this was a real unlink then we were called by
+	 * ext3_delete_inode(), and we allow that function to clean up the
+	 * orphan info for us.
+	 */
+	if (inode->i_nlink)
+		ext3_orphan_del(handle, inode);
+
+	ext3_journal_stop(handle, inode);
+}
+
+/* 
+ * ext3_get_inode_loc returns with an extra refcount against the
+ * inode's underlying buffer_head on success. 
+ */
+
+int ext3_get_inode_loc (struct inode *inode, struct ext3_iloc *iloc)
+{
+	struct buffer_head *bh = 0;
+	unsigned long block;
+	unsigned long block_group;
+	unsigned long group_desc;
+	unsigned long desc;
+	unsigned long offset;
+	struct ext3_group_desc * gdp;
+		
+	if ((inode->i_ino != EXT3_ROOT_INO &&
+		inode->i_ino != EXT3_ACL_IDX_INO &&
+		inode->i_ino != EXT3_ACL_DATA_INO &&
+		inode->i_ino != EXT3_JOURNAL_INO &&
+		inode->i_ino < EXT3_FIRST_INO(inode->i_sb)) ||
+		inode->i_ino > le32_to_cpu(
+			inode->i_sb->u.ext3_sb.s_es->s_inodes_count)) {
+		ext3_error (inode->i_sb, "ext3_get_inode_loc",
+			    "bad inode number: %lu", inode->i_ino);
+		goto bad_inode;
+	}
+	block_group = (inode->i_ino - 1) / EXT3_INODES_PER_GROUP(inode->i_sb);
+	if (block_group >= inode->i_sb->u.ext3_sb.s_groups_count) {
+		ext3_error (inode->i_sb, "ext3_get_inode_loc",
+			    "group >= groups count");
+		goto bad_inode;
+	}
+	group_desc = block_group >> EXT3_DESC_PER_BLOCK_BITS(inode->i_sb);
+	desc = block_group & (EXT3_DESC_PER_BLOCK(inode->i_sb) - 1);
+	bh = inode->i_sb->u.ext3_sb.s_group_desc[group_desc];
+	if (!bh) {
+		ext3_error (inode->i_sb, "ext3_get_inode_loc",
+			    "Descriptor not loaded");
+		goto bad_inode;
+	}
+
+	gdp = (struct ext3_group_desc *) bh->b_data;
+	/*
+	 * Figure out the offset within the block group inode table
+	 */
+	offset = ((inode->i_ino - 1) % EXT3_INODES_PER_GROUP(inode->i_sb)) *
+		EXT3_INODE_SIZE(inode->i_sb);
+	block = le32_to_cpu(gdp[desc].bg_inode_table) +
+		(offset >> EXT3_BLOCK_SIZE_BITS(inode->i_sb));
+	if (!(bh = sb_bread(inode->i_sb, block))) {
+		ext3_error (inode->i_sb, "ext3_get_inode_loc",
+			    "unable to read inode block - "
+			    "inode=%lu, block=%lu", inode->i_ino, block);
+		goto bad_inode;
+	}
+	offset &= (EXT3_BLOCK_SIZE(inode->i_sb) - 1);
+
+	iloc->bh = bh;
+	iloc->raw_inode = (struct ext3_inode *) (bh->b_data + offset);
+	iloc->block_group = block_group;
+	
+	return 0;
+	
+ bad_inode:
+	return -EIO;
+}
+
+void ext3_read_inode(struct inode * inode)
+{
+	struct ext3_iloc iloc;
+	struct ext3_inode *raw_inode;
+	struct buffer_head *bh;
+	int block;
+	
+	if(ext3_get_inode_loc(inode, &iloc))
+		goto bad_inode;
+	bh = iloc.bh;
+	raw_inode = iloc.raw_inode;
+	init_rwsem(&inode->u.ext3_i.truncate_sem);
+	inode->i_mode = le16_to_cpu(raw_inode->i_mode);
+	inode->i_uid = (uid_t)le16_to_cpu(raw_inode->i_uid_low);
+	inode->i_gid = (gid_t)le16_to_cpu(raw_inode->i_gid_low);
+	if(!(test_opt (inode->i_sb, NO_UID32))) {
+		inode->i_uid |= le16_to_cpu(raw_inode->i_uid_high) << 16;
+		inode->i_gid |= le16_to_cpu(raw_inode->i_gid_high) << 16;
+	}
+	inode->i_nlink = le16_to_cpu(raw_inode->i_links_count);
+	inode->i_size = le32_to_cpu(raw_inode->i_size);
+	inode->i_atime = le32_to_cpu(raw_inode->i_atime);
+	inode->i_ctime = le32_to_cpu(raw_inode->i_ctime);
+	inode->i_mtime = le32_to_cpu(raw_inode->i_mtime);
+	inode->u.ext3_i.i_dtime = le32_to_cpu(raw_inode->i_dtime);
+	/* We now have enough fields to check if the inode was active or not.
+	 * This is needed because nfsd might try to access dead inodes
+	 * the test is that same one that e2fsck uses
+	 * NeilBrown 1999oct15
+	 */
+	if (inode->i_nlink == 0) {
+		if (inode->i_mode == 0 ||
+		    !(inode->i_sb->u.ext3_sb.s_mount_state & EXT3_ORPHAN_FS)) {
+			/* this inode is deleted */
+			brelse (bh);
+			goto bad_inode;
+		}
+		/* The only unlinked inodes we let through here have
+		 * valid i_mode and are being read by the orphan
+		 * recovery code: that's fine, we're about to complete
+		 * the process of deleting those. */
+	}
+	inode->i_blksize = PAGE_SIZE;	/* This is the optimal IO size
+					 * (for stat), not the fs block
+					 * size */  
+	inode->i_blocks = le32_to_cpu(raw_inode->i_blocks);
+	inode->i_version = ++event;
+	inode->u.ext3_i.i_flags = le32_to_cpu(raw_inode->i_flags);
+#ifdef EXT3_FRAGMENTS
+	inode->u.ext3_i.i_faddr = le32_to_cpu(raw_inode->i_faddr);
+	inode->u.ext3_i.i_frag_no = raw_inode->i_frag;
+	inode->u.ext3_i.i_frag_size = raw_inode->i_fsize;
+#endif
+	inode->u.ext3_i.i_file_acl = le32_to_cpu(raw_inode->i_file_acl);
+	if (!S_ISREG(inode->i_mode)) {
+		inode->u.ext3_i.i_dir_acl = le32_to_cpu(raw_inode->i_dir_acl);
+	} else {
+		inode->i_size |=
+			((__u64)le32_to_cpu(raw_inode->i_size_high)) << 32;
+	}
+	inode->u.ext3_i.i_disksize = inode->i_size;
+	inode->i_generation = le32_to_cpu(raw_inode->i_generation);
+#ifdef EXT3_PREALLOCATE
+	inode->u.ext3_i.i_prealloc_count = 0;
+#endif
+	inode->u.ext3_i.i_block_group = iloc.block_group;
+
+	/*
+	 * NOTE! The in-memory inode i_data array is in little-endian order
+	 * even on big-endian machines: we do NOT byteswap the block numbers!
+	 */
+	for (block = 0; block < EXT3_N_BLOCKS; block++)
+		inode->u.ext3_i.i_data[block] = iloc.raw_inode->i_block[block];
+	INIT_LIST_HEAD(&inode->u.ext3_i.i_orphan);
+
+	brelse (iloc.bh);
+
+	if (inode->i_ino == EXT3_ACL_IDX_INO ||
+	    inode->i_ino == EXT3_ACL_DATA_INO)
+		/* Nothing to do */ ;
+	else if (S_ISREG(inode->i_mode)) {
+		inode->i_op = &ext3_file_inode_operations;
+		inode->i_fop = &ext3_file_operations;
+		inode->i_mapping->a_ops = &ext3_aops;
+	} else if (S_ISDIR(inode->i_mode)) {
+		inode->i_op = &ext3_dir_inode_operations;
+		inode->i_fop = &ext3_dir_operations;
+	} else if (S_ISLNK(inode->i_mode)) {
+		if (!inode->i_blocks)
+			inode->i_op = &ext3_fast_symlink_inode_operations;
+		else {
+			inode->i_op = &page_symlink_inode_operations;
+			inode->i_mapping->a_ops = &ext3_aops;
+		}
+	} else 
+		init_special_inode(inode, inode->i_mode,
+				   le32_to_cpu(iloc.raw_inode->i_block[0]));
+	/* inode->i_attr_flags = 0;				unused */
+	if (inode->u.ext3_i.i_flags & EXT3_SYNC_FL) {
+		/* inode->i_attr_flags |= ATTR_FLAG_SYNCRONOUS; unused */
+		inode->i_flags |= S_SYNC;
+	}
+	if (inode->u.ext3_i.i_flags & EXT3_APPEND_FL) {
+		/* inode->i_attr_flags |= ATTR_FLAG_APPEND;	unused */
+		inode->i_flags |= S_APPEND;
+	}
+	if (inode->u.ext3_i.i_flags & EXT3_IMMUTABLE_FL) {
+		/* inode->i_attr_flags |= ATTR_FLAG_IMMUTABLE;	unused */
+		inode->i_flags |= S_IMMUTABLE;
+	}
+	if (inode->u.ext3_i.i_flags & EXT3_NOATIME_FL) {
+		/* inode->i_attr_flags |= ATTR_FLAG_NOATIME;	unused */
+		inode->i_flags |= S_NOATIME;
+	}
+	return;
+	
+bad_inode:
+	make_bad_inode(inode);
+	return;
+}
+
+/*
+ * Post the struct inode info into an on-disk inode location in the
+ * buffer-cache.  This gobbles the caller's reference to the
+ * buffer_head in the inode location struct.  
+ */
+
+static int ext3_do_update_inode(handle_t *handle, 
+				struct inode *inode, 
+				struct ext3_iloc *iloc)
+{
+	struct ext3_inode *raw_inode = iloc->raw_inode;
+	struct buffer_head *bh = iloc->bh;
+	int err = 0, rc, block;
+
+	if (handle) {
+		BUFFER_TRACE(bh, "get_write_access");
+		err = ext3_journal_get_write_access(handle, bh);
+		if (err)
+			goto out_brelse;
+	}
+	raw_inode->i_mode = cpu_to_le16(inode->i_mode);
+	if(!(test_opt(inode->i_sb, NO_UID32))) {
+		raw_inode->i_uid_low = cpu_to_le16(low_16_bits(inode->i_uid));
+		raw_inode->i_gid_low = cpu_to_le16(low_16_bits(inode->i_gid));
+/*
+ * Fix up interoperability with old kernels. Otherwise, old inodes get
+ * re-used with the upper 16 bits of the uid/gid intact
+ */
+		if(!inode->u.ext3_i.i_dtime) {
+			raw_inode->i_uid_high =
+				cpu_to_le16(high_16_bits(inode->i_uid));
+			raw_inode->i_gid_high =
+				cpu_to_le16(high_16_bits(inode->i_gid));
+		} else {
+			raw_inode->i_uid_high = 0;
+			raw_inode->i_gid_high = 0;
+		}
+	} else {
+		raw_inode->i_uid_low =
+			cpu_to_le16(fs_high2lowuid(inode->i_uid));
+		raw_inode->i_gid_low =
+			cpu_to_le16(fs_high2lowgid(inode->i_gid));
+		raw_inode->i_uid_high = 0;
+		raw_inode->i_gid_high = 0;
+	}
+	raw_inode->i_links_count = cpu_to_le16(inode->i_nlink);
+	raw_inode->i_size = cpu_to_le32(inode->u.ext3_i.i_disksize);
+	raw_inode->i_atime = cpu_to_le32(inode->i_atime);
+	raw_inode->i_ctime = cpu_to_le32(inode->i_ctime);
+	raw_inode->i_mtime = cpu_to_le32(inode->i_mtime);
+	raw_inode->i_blocks = cpu_to_le32(inode->i_blocks);
+	raw_inode->i_dtime = cpu_to_le32(inode->u.ext3_i.i_dtime);
+	raw_inode->i_flags = cpu_to_le32(inode->u.ext3_i.i_flags);
+#ifdef EXT3_FRAGMENTS
+	raw_inode->i_faddr = cpu_to_le32(inode->u.ext3_i.i_faddr);
+	raw_inode->i_frag = inode->u.ext3_i.i_frag_no;
+	raw_inode->i_fsize = inode->u.ext3_i.i_frag_size;
+#else
+	/* If we are not tracking these fields in the in-memory inode,
+	 * then preserve them on disk, but still initialise them to zero
+	 * for new inodes. */
+	if (EXT3_I(inode)->i_state & EXT3_STATE_NEW) {
+		raw_inode->i_faddr = 0;
+		raw_inode->i_frag = 0;
+		raw_inode->i_fsize = 0;
+	}
+#endif
+	raw_inode->i_file_acl = cpu_to_le32(inode->u.ext3_i.i_file_acl);
+	if (!S_ISREG(inode->i_mode)) {
+		raw_inode->i_dir_acl = cpu_to_le32(inode->u.ext3_i.i_dir_acl);
+	} else {
+		raw_inode->i_size_high =
+			cpu_to_le32(inode->u.ext3_i.i_disksize >> 32);
+		if (inode->u.ext3_i.i_disksize > 0x7fffffffULL) {
+			struct super_block *sb = inode->i_sb;
+			if (!EXT3_HAS_RO_COMPAT_FEATURE(sb,
+					EXT3_FEATURE_RO_COMPAT_LARGE_FILE) ||
+			    EXT3_SB(sb)->s_es->s_rev_level ==
+					cpu_to_le32(EXT3_GOOD_OLD_REV)) {
+			       /* If this is the first large file
+				* created, add a flag to the superblock.
+				*/
+				err = ext3_journal_get_write_access(handle,
+						sb->u.ext3_sb.s_sbh);
+				if (err)
+					goto out_brelse;
+				ext3_update_dynamic_rev(sb);
+				EXT3_SET_RO_COMPAT_FEATURE(sb,
+					EXT3_FEATURE_RO_COMPAT_LARGE_FILE);
+				sb->s_dirt = 1;
+				handle->h_sync = 1;
+				err = ext3_journal_dirty_metadata(handle,
+						sb->u.ext3_sb.s_sbh);
+			}
+		}
+	}
+	raw_inode->i_generation = le32_to_cpu(inode->i_generation);
+	if (S_ISCHR(inode->i_mode) || S_ISBLK(inode->i_mode))
+		raw_inode->i_block[0] =
+			cpu_to_le32(kdev_t_to_nr(inode->i_rdev));
+	else for (block = 0; block < EXT3_N_BLOCKS; block++)
+		raw_inode->i_block[block] = inode->u.ext3_i.i_data[block];
+
+	BUFFER_TRACE(bh, "call ext3_journal_dirty_metadata");
+	rc = ext3_journal_dirty_metadata(handle, bh);
+	if (!err)
+		err = rc;
+	EXT3_I(inode)->i_state &= ~EXT3_STATE_NEW;
+
+out_brelse:
+	brelse (bh);
+	ext3_std_error(inode->i_sb, err);
+	return err;
+}
+
+/*
+ * ext3_write_inode()
+ *
+ * We are called from a few places:
+ *
+ * - Within generic_file_write() for O_SYNC files.
+ *   Here, there will be no transaction running. We wait for any running
+ *   trasnaction to commit.
+ *
+ * - Within sys_sync(), kupdate and such.
+ *   We wait on commit, if tol to.
+ *
+ * - Within prune_icache() (PF_MEMALLOC == true)
+ *   Here we simply return.  We can't afford to block kswapd on the
+ *   journal commit.
+ *
+ * In all cases it is actually safe for us to return without doing anything,
+ * because the inode has been copied into a raw inode buffer in
+ * ext3_mark_inode_dirty().  This is a correctness thing for O_SYNC and for
+ * knfsd.
+ *
+ * Note that we are absolutely dependent upon all inode dirtiers doing the
+ * right thing: they *must* call mark_inode_dirty() after dirtying info in
+ * which we are interested.
+ *
+ * It would be a bug for them to not do this.  The code:
+ *
+ *	mark_inode_dirty(inode)
+ *	stuff();
+ *	inode->i_size = expr;
+ *
+ * is in error because a kswapd-driven write_inode() could occur while
+ * `stuff()' is running, and the new i_size will be lost.  Plus the inode
+ * will no longer be on the superblock's dirty inode list.
+ */
+void ext3_write_inode(struct inode *inode, int wait)
+{
+	if (current->flags & PF_MEMALLOC)
+		return;
+
+	if (ext3_journal_current_handle()) {
+		jbd_debug(0, "called recursively, non-PF_MEMALLOC!\n");
+		return;
+	}
+
+	if (!wait)
+		return;
+
+	ext3_force_commit(inode->i_sb);	
+}
+
+/*
+ * ext3_setattr()
+ *
+ * Called from notify_change.
+ *
+ * We want to trap VFS attempts to truncate the file as soon as
+ * possible.  In particular, we want to make sure that when the VFS
+ * shrinks i_size, we put the inode on the orphan list and modify
+ * i_disksize immediately, so that during the subsequent flushing of
+ * dirty pages and freeing of disk blocks, we can guarantee that any
+ * commit will leave the blocks being flushed in an unused state on
+ * disk.  (On recovery, the inode will get truncated and the blocks will
+ * be freed, so we have a strong guarantee that no future commit will
+ * leave these blocks visible to the user.)  
+ *
+ * This is only needed for regular files.  rmdir() has its own path, and
+ * we can never truncate a direcory except on final unlink (at which
+ * point i_nlink is zero so recovery is easy.)
+ *
+ * Called with the BKL.  
+ */
+
+int ext3_setattr(struct dentry *dentry, struct iattr *attr)
+{
+	struct inode *inode = dentry->d_inode;
+	int error, rc = 0;
+	const unsigned int ia_valid = attr->ia_valid;
+
+	error = inode_change_ok(inode, attr);
+	if (error)
+		return error;
+
+	if ((ia_valid & ATTR_UID && attr->ia_uid != inode->i_uid) ||
+		(ia_valid & ATTR_GID && attr->ia_gid != inode->i_gid)) {
+		error = DQUOT_TRANSFER(inode, attr) ? -EDQUOT : 0;
+		if (error)
+			return error;
+	}
+
+	if (attr->ia_valid & ATTR_SIZE && attr->ia_size < inode->i_size) {
+		handle_t *handle;
+
+		handle = ext3_journal_start(inode, 3);
+		if (IS_ERR(handle)) {
+			error = PTR_ERR(handle);
+			goto err_out;
+		}
+		
+		error = ext3_orphan_add(handle, inode);
+		inode->u.ext3_i.i_disksize = attr->ia_size;
+		rc = ext3_mark_inode_dirty(handle, inode);
+		if (!error)
+			error = rc;
+		ext3_journal_stop(handle, inode);
+	}
+	
+	rc = inode_setattr(inode, attr);
+
+	/* If inode_setattr's call to ext3_truncate failed to get a
+	 * transaction handle at all, we need to clean up the in-core
+	 * orphan list manually. */
+	if (inode->i_nlink)
+		ext3_orphan_del(NULL, inode);
+
+err_out:
+	ext3_std_error(inode->i_sb, error);
+	if (!error)
+		error = rc;
+	return error;
+}
+
+
+/*
+ * akpm: how many blocks doth make a writepage()?
+ *
+ * With N blocks per page, it may be:
+ * N data blocks
+ * 2 indirect block
+ * 2 dindirect
+ * 1 tindirect
+ * N+5 bitmap blocks (from the above)
+ * N+5 group descriptor summary blocks
+ * 1 inode block
+ * 1 superblock.
+ * 2 * EXT3_SINGLEDATA_TRANS_BLOCKS for the quote files
+ *
+ * 3 * (N + 5) + 2 + 2 * EXT3_SINGLEDATA_TRANS_BLOCKS
+ *
+ * With ordered or writeback data it's the same, less the N data blocks.
+ *
+ * If the inode's direct blocks can hold an integral number of pages then a
+ * page cannot straddle two indirect blocks, and we can only touch one indirect
+ * and dindirect block, and the "5" above becomes "3".
+ *
+ * This still overestimates under most circumstances.  If we were to pass the
+ * start and end offsets in here as well we could do block_to_path() on each
+ * block and work out the exact number of indirects which are touched.  Pah.
+ */
+
+int ext3_writepage_trans_blocks(struct inode *inode)
+{
+	int bpp = ext3_journal_blocks_per_page(inode);
+	int indirects = (EXT3_NDIR_BLOCKS % bpp) ? 5 : 3;
+	int ret;
+	
+	if (ext3_should_journal_data(inode))
+		ret = 3 * (bpp + indirects) + 2;
+	else
+		ret = 2 * (bpp + indirects) + 2;
+
+#ifdef CONFIG_QUOTA
+	ret += 2 * EXT3_SINGLEDATA_TRANS_BLOCKS;
+#endif
+
+	return ret;
+}
+
+int
+ext3_mark_iloc_dirty(handle_t *handle, 
+		     struct inode *inode,
+		     struct ext3_iloc *iloc)
+{
+	int err = 0;
+
+	if (handle) {
+		/* the do_update_inode consumes one bh->b_count */
+		atomic_inc(&iloc->bh->b_count);
+		err = ext3_do_update_inode(handle, inode, iloc);
+		/* ext3_do_update_inode() does journal_dirty_metadata */
+		brelse(iloc->bh);
+	} else {
+		printk(KERN_EMERG __FUNCTION__ ": called with no handle!\n");
+	}
+	return err;
+}
+
+/* 
+ * On success, We end up with an outstanding reference count against
+ * iloc->bh.  This _must_ be cleaned up later. 
+ */
+
+int
+ext3_reserve_inode_write(handle_t *handle, struct inode *inode, 
+			 struct ext3_iloc *iloc)
+{
+	int err = 0;
+	if (handle) {
+		err = ext3_get_inode_loc(inode, iloc);
+		if (!err) {
+			BUFFER_TRACE(iloc->bh, "get_write_access");
+			err = ext3_journal_get_write_access(handle, iloc->bh);
+			if (err) {
+				brelse(iloc->bh);
+				iloc->bh = NULL;
+			}
+		}
+	}
+	ext3_std_error(inode->i_sb, err);
+	return err;
+}
+
+/*
+ * akpm: What we do here is to mark the in-core inode as clean
+ * with respect to inode dirtiness (it may still be data-dirty).
+ * This means that the in-core inode may be reaped by prune_icache
+ * without having to perform any I/O.  This is a very good thing,
+ * because *any* task may call prune_icache - even ones which
+ * have a transaction open against a different journal.
+ *
+ * Is this cheating?  Not really.  Sure, we haven't written the
+ * inode out, but prune_icache isn't a user-visible syncing function.
+ * Whenever the user wants stuff synced (sys_sync, sys_msync, sys_fsync)
+ * we start and wait on commits.
+ *
+ * Is this efficient/effective?  Well, we're being nice to the system
+ * by cleaning up our inodes proactively so they can be reaped
+ * without I/O.  But we are potentially leaving up to five seconds'
+ * worth of inodes floating about which prune_icache wants us to
+ * write out.  One way to fix that would be to get prune_icache()
+ * to do a write_super() to free up some memory.  It has the desired
+ * effect.
+ */
+int ext3_mark_inode_dirty(handle_t *handle, struct inode *inode)
+{
+	struct ext3_iloc iloc;
+	int err;
+
+	err = ext3_reserve_inode_write(handle, inode, &iloc);
+	if (!err)
+		err = ext3_mark_iloc_dirty(handle, inode, &iloc);
+	return err;
+}
+
+/*
+ * akpm: ext3_dirty_inode() is called from __mark_inode_dirty()
+ *
+ * We're really interested in the case where a file is being extended.
+ * i_size has been changed by generic_commit_write() and we thus need
+ * to include the updated inode in the current transaction.
+ *
+ * Also, DQUOT_ALLOC_SPACE() will always dirty the inode when blocks
+ * are allocated to the file.
+ *
+ * If the inode is marked synchronous, we don't honour that here - doing
+ * so would cause a commit on atime updates, which we don't bother doing.
+ * We handle synchronous inodes at the highest possible level.
+ */
+void ext3_dirty_inode(struct inode *inode)
+{
+	handle_t *current_handle = ext3_journal_current_handle();
+	handle_t *handle;
+
+	lock_kernel();
+	handle = ext3_journal_start(inode, 1);
+	if (IS_ERR(handle))
+		goto out;
+	if (current_handle &&
+		current_handle->h_transaction != handle->h_transaction) {
+		/* This task has a transaction open against a different fs */
+		printk(KERN_EMERG __FUNCTION__": transactions do not match!\n");
+	} else {
+		jbd_debug(5, "marking dirty.  outer handle=%p\n",
+				current_handle);
+		ext3_mark_inode_dirty(handle, inode);
+	}
+	ext3_journal_stop(handle, inode);
+out:
+	unlock_kernel();
+}
+
+#ifdef AKPM
+/* 
+ * Bind an inode's backing buffer_head into this transaction, to prevent
+ * it from being flushed to disk early.  Unlike
+ * ext3_reserve_inode_write, this leaves behind no bh reference and
+ * returns no iloc structure, so the caller needs to repeat the iloc
+ * lookup to mark the inode dirty later.
+ */
+static inline int
+ext3_pin_inode(handle_t *handle, struct inode *inode)
+{
+	struct ext3_iloc iloc;
+	
+	int err = 0;
+	if (handle) {
+		err = ext3_get_inode_loc(inode, &iloc);
+		if (!err) {
+			BUFFER_TRACE(iloc.bh, "get_write_access");
+			err = journal_get_write_access(handle, iloc.bh);
+			if (!err)
+				err = ext3_journal_dirty_metadata(handle, 
+								  iloc.bh);
+			brelse(iloc.bh);
+		}
+	}
+	ext3_std_error(inode->i_sb, err);
+	return err;
+}
+#endif
+
+int ext3_change_inode_journal_flag(struct inode *inode, int val)
+{
+	journal_t *journal;
+	handle_t *handle;
+	int err;
+
+	/*
+	 * We have to be very careful here: changing a data block's
+	 * journaling status dynamically is dangerous.  If we write a
+	 * data block to the journal, change the status and then delete
+	 * that block, we risk forgetting to revoke the old log record
+	 * from the journal and so a subsequent replay can corrupt data.
+	 * So, first we make sure that the journal is empty and that
+	 * nobody is changing anything.
+	 */
+
+	journal = EXT3_JOURNAL(inode);
+	if (is_journal_aborted(journal) || IS_RDONLY(inode))
+		return -EROFS;
+	
+	journal_lock_updates(journal);
+	journal_flush(journal);
+
+	/*
+	 * OK, there are no updates running now, and all cached data is
+	 * synced to disk.  We are now in a completely consistent state
+	 * which doesn't have anything in the journal, and we know that
+	 * no filesystem updates are running, so it is safe to modify
+	 * the inode's in-core data-journaling state flag now.
+	 */
+
+	if (val)
+		inode->u.ext3_i.i_flags |= EXT3_JOURNAL_DATA_FL;
+	else
+		inode->u.ext3_i.i_flags &= ~EXT3_JOURNAL_DATA_FL;
+
+	journal_unlock_updates(journal);
+
+	/* Finally we can mark the inode as dirty. */
+
+	handle = ext3_journal_start(inode, 1);
+	if (IS_ERR(handle))
+		return PTR_ERR(handle);
+
+	err = ext3_mark_inode_dirty(handle, inode);
+	handle->h_sync = 1;
+	ext3_journal_stop(handle, inode);
+	ext3_std_error(inode->i_sb, err);
+	
+	return err;
+}
+
+
+/*
+ * ext3_aops_journal_start().
+ *
+ * <This function died, but the comment lives on>
+ *
+ * We need to take the inode semaphore *outside* the
+ * journal_start/journal_stop.  Otherwise, a different task could do a
+ * wait_for_commit() while holding ->i_sem, which deadlocks.  The rule
+ * is: transaction open/closes are considered to be a locking operation
+ * and they nest *inside* ->i_sem.
+ * ----------------------------------------------------------------------------
+ * Possible problem:
+ *	ext3_file_write()
+ *	-> generic_file_write()
+ *	   -> __alloc_pages()
+ *	      -> page_launder()
+ *		 -> ext3_writepage()
+ *
+ * And the writepage can be on a different fs while we have a
+ * transaction open against this one!  Bad.
+ *
+ * I tried making the task PF_MEMALLOC here, but that simply results in
+ * 0-order allocation failures passed back to generic_file_write().
+ * Instead, we rely on the reentrancy protection in ext3_writepage().
+ * ----------------------------------------------------------------------------
+ * When we do the journal_start() here we don't really need to reserve
+ * any blocks - we won't need any until we hit ext3_prepare_write(),
+ * which does all the needed journal extending.  However!  There is a
+ * problem with quotas:
+ *
+ * Thread 1:
+ * sys_sync
+ * ->sync_dquots
+ *   ->commit_dquot
+ *     ->lock_dquot
+ *     ->write_dquot
+ *       ->ext3_file_write
+ *         ->journal_start
+ *         ->ext3_prepare_write
+ *           ->journal_extend
+ *           ->journal_start
+ * Thread 2:
+ * ext3_create		(for example)
+ * ->ext3_new_inode
+ *   ->dquot_initialize
+ *     ->lock_dquot
+ *
+ * Deadlock.  Thread 1's journal_start blocks because thread 2 has a
+ * transaction open.  Thread 2's transaction will never close because
+ * thread 2 is stuck waiting for the dquot lock.
+ *
+ * So.  We must ensure that thread 1 *never* needs to extend the journal
+ * for quota writes.  We do that by reserving enough journal blocks
+ * here, in ext3_aops_journal_start() to ensure that the forthcoming "see if we
+ * need to extend" test in ext3_prepare_write() succeeds.  
+ */
diff -rup --new-file linux.mcp2/fs/ext3/ioctl.c linux_tmp/fs/ext3/ioctl.c
--- linux.mcp2/fs/ext3/ioctl.c	1969-12-31 16:00:00.000000000 -0800
+++ linux_tmp/fs/ext3/ioctl.c	2001-11-09 14:25:04.000000000 -0800
@@ -0,0 +1,170 @@
+/*
+ * linux/fs/ext3/ioctl.c
+ *
+ * Copyright (C) 1993, 1994, 1995
+ * Remy Card (card@masi.ibp.fr)
+ * Laboratoire MASI - Institut Blaise Pascal
+ * Universite Pierre et Marie Curie (Paris VI)
+ */
+
+#include <linux/fs.h>
+#include <linux/jbd.h>
+#include <linux/ext3_fs.h>
+#include <linux/ext3_jbd.h>
+#include <linux/sched.h>
+#include <asm/uaccess.h>
+
+
+int ext3_ioctl (struct inode * inode, struct file * filp, unsigned int cmd,
+		unsigned long arg)
+{
+	unsigned int flags;
+
+	ext3_debug ("cmd = %u, arg = %lu\n", cmd, arg);
+
+	switch (cmd) {
+	case EXT3_IOC_GETFLAGS:
+		flags = inode->u.ext3_i.i_flags & EXT3_FL_USER_VISIBLE;
+		return put_user(flags, (int *) arg);
+	case EXT3_IOC_SETFLAGS: {
+		handle_t *handle = NULL;
+		int err;
+		struct ext3_iloc iloc;
+		unsigned int oldflags;
+		unsigned int jflag;
+
+		if (IS_RDONLY(inode))
+			return -EROFS;
+
+		if ((current->fsuid != inode->i_uid) && !capable(CAP_FOWNER))
+			return -EPERM;
+
+		if (get_user(flags, (int *) arg))
+			return -EFAULT;
+
+		oldflags = inode->u.ext3_i.i_flags;
+
+		/* The JOURNAL_DATA flag is modifiable only by root */
+		jflag = flags & EXT3_JOURNAL_DATA_FL;
+
+		/*
+		 * The IMMUTABLE and APPEND_ONLY flags can only be changed by
+		 * the relevant capability.
+		 *
+		 * This test looks nicer. Thanks to Pauline Middelink
+		 */
+		if ((flags ^ oldflags) & (EXT3_APPEND_FL | EXT3_IMMUTABLE_FL)) {
+			if (!capable(CAP_LINUX_IMMUTABLE))
+				return -EPERM;
+		}
+		
+		/*
+		 * The JOURNAL_DATA flag can only be changed by
+		 * the relevant capability.
+		 */
+		if ((jflag ^ oldflags) & (EXT3_JOURNAL_DATA_FL)) {
+			if (!capable(CAP_SYS_RESOURCE))
+				return -EPERM;
+		}
+
+
+		handle = ext3_journal_start(inode, 1);
+		if (IS_ERR(handle))
+			return PTR_ERR(handle);
+		if (IS_SYNC(inode))
+			handle->h_sync = 1;
+		err = ext3_reserve_inode_write(handle, inode, &iloc);
+		if (err)
+			goto flags_err;
+		
+		flags = flags & EXT3_FL_USER_MODIFIABLE;
+		flags |= oldflags & ~EXT3_FL_USER_MODIFIABLE;
+		inode->u.ext3_i.i_flags = flags;
+
+		if (flags & EXT3_SYNC_FL)
+			inode->i_flags |= S_SYNC;
+		else
+			inode->i_flags &= ~S_SYNC;
+		if (flags & EXT3_APPEND_FL)
+			inode->i_flags |= S_APPEND;
+		else
+			inode->i_flags &= ~S_APPEND;
+		if (flags & EXT3_IMMUTABLE_FL)
+			inode->i_flags |= S_IMMUTABLE;
+		else
+			inode->i_flags &= ~S_IMMUTABLE;
+		if (flags & EXT3_NOATIME_FL)
+			inode->i_flags |= S_NOATIME;
+		else
+			inode->i_flags &= ~S_NOATIME;
+		inode->i_ctime = CURRENT_TIME;
+
+		err = ext3_mark_iloc_dirty(handle, inode, &iloc);
+flags_err:
+		ext3_journal_stop(handle, inode);
+		if (err)
+			return err;
+		
+		if ((jflag ^ oldflags) & (EXT3_JOURNAL_DATA_FL))
+			err = ext3_change_inode_journal_flag(inode, jflag);
+		return err;
+	}
+	case EXT3_IOC_GETVERSION:
+	case EXT3_IOC_GETVERSION_OLD:
+		return put_user(inode->i_generation, (int *) arg);
+	case EXT3_IOC_SETVERSION:
+	case EXT3_IOC_SETVERSION_OLD: {
+		handle_t *handle;
+		struct ext3_iloc iloc;
+		__u32 generation;
+		int err;
+
+		if ((current->fsuid != inode->i_uid) && !capable(CAP_FOWNER))
+			return -EPERM;
+		if (IS_RDONLY(inode))
+			return -EROFS;
+		if (get_user(generation, (int *) arg))
+			return -EFAULT;
+
+		handle = ext3_journal_start(inode, 1);
+		if (IS_ERR(handle))
+			return PTR_ERR(handle);
+		err = ext3_reserve_inode_write(handle, inode, &iloc);
+		if (err)
+			return err;
+
+		inode->i_ctime = CURRENT_TIME;
+		inode->i_generation = generation;
+
+		err = ext3_mark_iloc_dirty(handle, inode, &iloc);
+		ext3_journal_stop(handle, inode);
+		return err;
+	}
+#ifdef CONFIG_JBD_DEBUG
+	case EXT3_IOC_WAIT_FOR_READONLY:
+		/*
+		 * This is racy - by the time we're woken up and running,
+		 * the superblock could be released.  And the module could
+		 * have been unloaded.  So sue me.
+		 *
+		 * Returns 1 if it slept, else zero.
+		 */
+		{
+			struct super_block *sb = inode->i_sb;
+			DECLARE_WAITQUEUE(wait, current);
+			int ret = 0;
+
+			set_current_state(TASK_INTERRUPTIBLE);
+			add_wait_queue(&sb->u.ext3_sb.ro_wait_queue, &wait);
+			if (timer_pending(&sb->u.ext3_sb.turn_ro_timer)) {
+				schedule();
+				ret = 1;
+			}
+			remove_wait_queue(&sb->u.ext3_sb.ro_wait_queue, &wait);
+			return ret;
+		}
+#endif
+	default:
+		return -ENOTTY;
+	}
+}
diff -rup --new-file linux.mcp2/fs/ext3/namei.c linux_tmp/fs/ext3/namei.c
--- linux.mcp2/fs/ext3/namei.c	1969-12-31 16:00:00.000000000 -0800
+++ linux_tmp/fs/ext3/namei.c	2001-11-09 14:25:04.000000000 -0800
@@ -0,0 +1,1125 @@
+/*
+ *  linux/fs/ext3/namei.c
+ *
+ * Copyright (C) 1992, 1993, 1994, 1995
+ * Remy Card (card@masi.ibp.fr)
+ * Laboratoire MASI - Institut Blaise Pascal
+ * Universite Pierre et Marie Curie (Paris VI)
+ *
+ *  from
+ *
+ *  linux/fs/minix/namei.c
+ *
+ *  Copyright (C) 1991, 1992  Linus Torvalds
+ *
+ *  Big-endian to little-endian byte-swapping/bitmaps by
+ *        David S. Miller (davem@caip.rutgers.edu), 1995
+ *  Directory entry file type support and forward compatibility hooks
+ *  	for B-tree directories by Theodore Ts'o (tytso@mit.edu), 1998
+ */
+
+#include <linux/fs.h>
+#include <linux/jbd.h>
+#include <linux/sched.h>
+#include <linux/ext3_fs.h>
+#include <linux/ext3_jbd.h>
+#include <linux/fcntl.h>
+#include <linux/stat.h>
+#include <linux/string.h>
+#include <linux/locks.h>
+#include <linux/quotaops.h>
+
+
+/*
+ * define how far ahead to read directories while searching them.
+ */
+#define NAMEI_RA_CHUNKS  2
+#define NAMEI_RA_BLOCKS  4
+#define NAMEI_RA_SIZE        (NAMEI_RA_CHUNKS * NAMEI_RA_BLOCKS)
+#define NAMEI_RA_INDEX(c,b)  (((c) * NAMEI_RA_BLOCKS) + (b))
+
+/*
+ * NOTE! unlike strncmp, ext3_match returns 1 for success, 0 for failure.
+ *
+ * `len <= EXT3_NAME_LEN' is guaranteed by caller.
+ * `de != NULL' is guaranteed by caller.
+ */
+static inline int ext3_match (int len, const char * const name,
+			      struct ext3_dir_entry_2 * de)
+{
+	if (len != de->name_len)
+		return 0;
+	if (!de->inode)
+		return 0;
+	return !memcmp(name, de->name, len);
+}
+
+/*
+ * Returns 0 if not found, -1 on failure, and 1 on success
+ */
+static int inline search_dirblock(struct buffer_head * bh,
+				  struct inode *dir,
+				  struct dentry *dentry,
+				  unsigned long offset,
+				  struct ext3_dir_entry_2 ** res_dir)
+{
+	struct ext3_dir_entry_2 * de;
+	char * dlimit;
+	int de_len;
+	const char *name = dentry->d_name.name;
+	int namelen = dentry->d_name.len;
+
+	de = (struct ext3_dir_entry_2 *) bh->b_data;
+	dlimit = bh->b_data + dir->i_sb->s_blocksize;
+	while ((char *) de < dlimit) {
+		/* this code is executed quadratically often */
+		/* do minimal checking `by hand' */
+
+		if ((char *) de + namelen <= dlimit &&
+		    ext3_match (namelen, name, de)) {
+			/* found a match - just to be sure, do a full check */
+			if (!ext3_check_dir_entry("ext3_find_entry",
+						  dir, de, bh, offset))
+				return -1;
+			*res_dir = de;
+			return 1;
+		}
+		/* prevent looping on a bad block */
+		de_len = le16_to_cpu(de->rec_len);
+		if (de_len <= 0)
+			return -1;
+		offset += de_len;
+		de = (struct ext3_dir_entry_2 *) ((char *) de + de_len);
+	}
+	return 0;
+}
+
+/*
+ *	ext3_find_entry()
+ *
+ * finds an entry in the specified directory with the wanted name. It
+ * returns the cache buffer in which the entry was found, and the entry
+ * itself (as a parameter - res_dir). It does NOT read the inode of the
+ * entry - you'll have to do that yourself if you want to.
+ *
+ * The returned buffer_head has ->b_count elevated.  The caller is expected
+ * to brelse() it when appropriate.
+ */
+static struct buffer_head * ext3_find_entry (struct dentry *dentry,
+					struct ext3_dir_entry_2 ** res_dir)
+{
+	struct super_block * sb;
+	struct buffer_head * bh_use[NAMEI_RA_SIZE];
+	struct buffer_head * bh, *ret = NULL;
+	unsigned long start, block, b;
+	int ra_max = 0;		/* Number of bh's in the readahead
+				   buffer, bh_use[] */
+	int ra_ptr = 0;		/* Current index into readahead
+				   buffer */
+	int num = 0;
+	int nblocks, i, err;
+	struct inode *dir = dentry->d_parent->d_inode;
+
+	*res_dir = NULL;
+	sb = dir->i_sb;
+
+	nblocks = dir->i_size >> EXT3_BLOCK_SIZE_BITS(sb);
+	start = dir->u.ext3_i.i_dir_start_lookup;
+	if (start >= nblocks)
+		start = 0;
+	block = start;
+restart:
+	do {
+		/*
+		 * We deal with the read-ahead logic here.
+		 */
+		if (ra_ptr >= ra_max) {
+			/* Refill the readahead buffer */
+			ra_ptr = 0;
+			b = block;
+			for (ra_max = 0; ra_max < NAMEI_RA_SIZE; ra_max++) {
+				/*
+				 * Terminate if we reach the end of the
+				 * directory and must wrap, or if our
+				 * search has finished at this block.
+				 */
+				if (b >= nblocks || (num && block == start)) {
+					bh_use[ra_max] = NULL;
+					break;
+				}
+				num++;
+				bh = ext3_getblk(NULL, dir, b++, 0, &err);
+				bh_use[ra_max] = bh;
+				if (bh)
+					ll_rw_block(READ, 1, &bh);
+			}
+		}
+		if ((bh = bh_use[ra_ptr++]) == NULL)
+			goto next;
+		wait_on_buffer(bh);
+		if (!buffer_uptodate(bh)) {
+			/* read error, skip block & hope for the best */
+			brelse(bh);
+			goto next;
+		}
+		i = search_dirblock(bh, dir, dentry,
+			    block << EXT3_BLOCK_SIZE_BITS(sb), res_dir);
+		if (i == 1) {
+			dir->u.ext3_i.i_dir_start_lookup = block;
+			ret = bh;
+			goto cleanup_and_exit;
+		} else {
+			brelse(bh);
+			if (i < 0)
+				goto cleanup_and_exit;
+		}
+	next:
+		if (++block >= nblocks)
+			block = 0;
+	} while (block != start);
+
+	/*
+	 * If the directory has grown while we were searching, then
+	 * search the last part of the directory before giving up.
+	 */
+	block = nblocks;
+	nblocks = dir->i_size >> EXT3_BLOCK_SIZE_BITS(sb);
+	if (block < nblocks) {
+		start = 0;
+		goto restart;
+	}
+		
+cleanup_and_exit:
+	/* Clean up the read-ahead blocks */
+	for (; ra_ptr < ra_max; ra_ptr++)
+		brelse (bh_use[ra_ptr]);
+	return ret;
+}
+
+static struct dentry *ext3_lookup(struct inode * dir, struct dentry *dentry)
+{
+	struct inode * inode;
+	struct ext3_dir_entry_2 * de;
+	struct buffer_head * bh;
+
+	if (dentry->d_name.len > EXT3_NAME_LEN)
+		return ERR_PTR(-ENAMETOOLONG);
+
+	bh = ext3_find_entry(dentry, &de);
+	inode = NULL;
+	if (bh) {
+		unsigned long ino = le32_to_cpu(de->inode);
+		brelse (bh);
+		inode = iget(dir->i_sb, ino);
+
+		if (!inode)
+			return ERR_PTR(-EACCES);
+	}
+	d_add(dentry, inode);
+	return NULL;
+}
+
+#define S_SHIFT 12
+static unsigned char ext3_type_by_mode[S_IFMT >> S_SHIFT] = {
+	[S_IFREG >> S_SHIFT]	EXT3_FT_REG_FILE,
+	[S_IFDIR >> S_SHIFT]	EXT3_FT_DIR,
+	[S_IFCHR >> S_SHIFT]	EXT3_FT_CHRDEV,
+	[S_IFBLK >> S_SHIFT]	EXT3_FT_BLKDEV,
+	[S_IFIFO >> S_SHIFT]	EXT3_FT_FIFO,
+	[S_IFSOCK >> S_SHIFT]	EXT3_FT_SOCK,
+	[S_IFLNK >> S_SHIFT]	EXT3_FT_SYMLINK,
+};
+
+static inline void ext3_set_de_type(struct super_block *sb,
+				struct ext3_dir_entry_2 *de,
+				umode_t mode) {
+	if (EXT3_HAS_INCOMPAT_FEATURE(sb, EXT3_FEATURE_INCOMPAT_FILETYPE))
+		de->file_type = ext3_type_by_mode[(mode & S_IFMT)>>S_SHIFT];
+}
+
+/*
+ *	ext3_add_entry()
+ *
+ * adds a file entry to the specified directory, using the same
+ * semantics as ext3_find_entry(). It returns NULL if it failed.
+ *
+ * NOTE!! The inode part of 'de' is left at 0 - which means you
+ * may not sleep between calling this and putting something into
+ * the entry, as someone else might have used it while you slept.
+ */
+
+/*
+ * AKPM: the journalling code here looks wrong on the error paths
+ */
+static int ext3_add_entry (handle_t *handle, struct dentry *dentry,
+	struct inode *inode)
+{
+	struct inode *dir = dentry->d_parent->d_inode;
+	const char *name = dentry->d_name.name;
+	int namelen = dentry->d_name.len;
+	unsigned long offset;
+	unsigned short rec_len;
+	struct buffer_head * bh;
+	struct ext3_dir_entry_2 * de, * de1;
+	struct super_block * sb;
+	int	retval;
+
+	sb = dir->i_sb;
+
+	if (!namelen)
+		return -EINVAL;
+	bh = ext3_bread (handle, dir, 0, 0, &retval);
+	if (!bh)
+		return retval;
+	rec_len = EXT3_DIR_REC_LEN(namelen);
+	offset = 0;
+	de = (struct ext3_dir_entry_2 *) bh->b_data;
+	while (1) {
+		if ((char *)de >= sb->s_blocksize + bh->b_data) {
+			brelse (bh);
+			bh = NULL;
+			bh = ext3_bread (handle, dir,
+				offset >> EXT3_BLOCK_SIZE_BITS(sb), 1, &retval);
+			if (!bh)
+				return retval;
+			if (dir->i_size <= offset) {
+				if (dir->i_size == 0) {
+					brelse(bh);
+					return -ENOENT;
+				}
+
+				ext3_debug ("creating next block\n");
+
+				BUFFER_TRACE(bh, "get_write_access");
+				ext3_journal_get_write_access(handle, bh);
+				de = (struct ext3_dir_entry_2 *) bh->b_data;
+				de->inode = 0;
+				de->rec_len = le16_to_cpu(sb->s_blocksize);
+				dir->u.ext3_i.i_disksize =
+					dir->i_size = offset + sb->s_blocksize;
+				dir->u.ext3_i.i_flags &= ~EXT3_INDEX_FL;
+				ext3_mark_inode_dirty(handle, dir);
+			} else {
+
+				ext3_debug ("skipping to next block\n");
+
+				de = (struct ext3_dir_entry_2 *) bh->b_data;
+			}
+		}
+		if (!ext3_check_dir_entry ("ext3_add_entry", dir, de, bh,
+					   offset)) {
+			brelse (bh);
+			return -ENOENT;
+		}
+		if (ext3_match (namelen, name, de)) {
+				brelse (bh);
+				return -EEXIST;
+		}
+		if ((le32_to_cpu(de->inode) == 0 &&
+				le16_to_cpu(de->rec_len) >= rec_len) ||
+		    (le16_to_cpu(de->rec_len) >=
+				EXT3_DIR_REC_LEN(de->name_len) + rec_len)) {
+			BUFFER_TRACE(bh, "get_write_access");
+			ext3_journal_get_write_access(handle, bh);
+			/* By now the buffer is marked for journaling */
+			offset += le16_to_cpu(de->rec_len);
+			if (le32_to_cpu(de->inode)) {
+				de1 = (struct ext3_dir_entry_2 *) ((char *) de +
+					EXT3_DIR_REC_LEN(de->name_len));
+				de1->rec_len =
+					cpu_to_le16(le16_to_cpu(de->rec_len) -
+					EXT3_DIR_REC_LEN(de->name_len));
+				de->rec_len = cpu_to_le16(
+						EXT3_DIR_REC_LEN(de->name_len));
+				de = de1;
+			}
+			de->file_type = EXT3_FT_UNKNOWN;
+			if (inode) {
+				de->inode = cpu_to_le32(inode->i_ino);
+				ext3_set_de_type(dir->i_sb, de, inode->i_mode);
+			} else
+				de->inode = 0;
+			de->name_len = namelen;
+			memcpy (de->name, name, namelen);
+			/*
+			 * XXX shouldn't update any times until successful
+			 * completion of syscall, but too many callers depend
+			 * on this.
+			 *
+			 * XXX similarly, too many callers depend on
+			 * ext3_new_inode() setting the times, but error
+			 * recovery deletes the inode, so the worst that can
+			 * happen is that the times are slightly out of date
+			 * and/or different from the directory change time.
+			 */
+			dir->i_mtime = dir->i_ctime = CURRENT_TIME;
+			dir->u.ext3_i.i_flags &= ~EXT3_INDEX_FL;
+			ext3_mark_inode_dirty(handle, dir);
+			dir->i_version = ++event;
+			BUFFER_TRACE(bh, "call ext3_journal_dirty_metadata");
+			ext3_journal_dirty_metadata(handle, bh);
+			brelse(bh);
+			return 0;
+		}
+		offset += le16_to_cpu(de->rec_len);
+		de = (struct ext3_dir_entry_2 *)
+			((char *) de + le16_to_cpu(de->rec_len));
+	}
+	brelse (bh);
+	return -ENOSPC;
+}
+
+/*
+ * ext3_delete_entry deletes a directory entry by merging it with the
+ * previous entry
+ */
+static int ext3_delete_entry (handle_t *handle, 
+			      struct inode * dir,
+			      struct ext3_dir_entry_2 * de_del,
+			      struct buffer_head * bh)
+{
+	struct ext3_dir_entry_2 * de, * pde;
+	int i;
+
+	i = 0;
+	pde = NULL;
+	de = (struct ext3_dir_entry_2 *) bh->b_data;
+	while (i < bh->b_size) {
+		if (!ext3_check_dir_entry("ext3_delete_entry", dir, de, bh, i))
+			return -EIO;
+		if (de == de_del)  {
+			BUFFER_TRACE(bh, "get_write_access");
+			ext3_journal_get_write_access(handle, bh);
+			if (pde)
+				pde->rec_len =
+					cpu_to_le16(le16_to_cpu(pde->rec_len) +
+						    le16_to_cpu(de->rec_len));
+			else
+				de->inode = 0;
+			dir->i_version = ++event;
+			BUFFER_TRACE(bh, "call ext3_journal_dirty_metadata");
+			ext3_journal_dirty_metadata(handle, bh);
+			return 0;
+		}
+		i += le16_to_cpu(de->rec_len);
+		pde = de;
+		de = (struct ext3_dir_entry_2 *)
+			((char *) de + le16_to_cpu(de->rec_len));
+	}
+	return -ENOENT;
+}
+
+/*
+ * ext3_mark_inode_dirty is somewhat expensive, so unlike ext2 we
+ * do not perform it in these functions.  We perform it at the call site,
+ * if it is needed.
+ */
+static inline void ext3_inc_count(handle_t *handle, struct inode *inode)
+{
+	inode->i_nlink++;
+}
+
+static inline void ext3_dec_count(handle_t *handle, struct inode *inode)
+{
+	inode->i_nlink--;
+}
+
+static int ext3_add_nondir(handle_t *handle,
+		struct dentry *dentry, struct inode *inode)
+{
+	int err = ext3_add_entry(handle, dentry, inode);
+	if (!err) {
+		d_instantiate(dentry, inode);
+		return 0;
+	}
+	ext3_dec_count(handle, inode);
+	iput(inode);
+	return err;
+}
+
+/*
+ * By the time this is called, we already have created
+ * the directory cache entry for the new file, but it
+ * is so far negative - it has no inode.
+ *
+ * If the create succeeds, we fill in the inode information
+ * with d_instantiate(). 
+ */
+static int ext3_create (struct inode * dir, struct dentry * dentry, int mode)
+{
+	handle_t *handle; 
+	struct inode * inode;
+	int err;
+
+	handle = ext3_journal_start(dir, EXT3_DATA_TRANS_BLOCKS + 3);
+	if (IS_ERR(handle))
+		return PTR_ERR(handle);
+
+	if (IS_SYNC(dir))
+		handle->h_sync = 1;
+
+	inode = ext3_new_inode (handle, dir, mode);
+	err = PTR_ERR(inode);
+	if (!IS_ERR(inode)) {
+		inode->i_op = &ext3_file_inode_operations;
+		inode->i_fop = &ext3_file_operations;
+		inode->i_mapping->a_ops = &ext3_aops;
+		ext3_mark_inode_dirty(handle, inode);
+		err = ext3_add_nondir(handle, dentry, inode);
+	}
+	ext3_journal_stop(handle, dir);
+	return err;
+}
+
+static int ext3_mknod (struct inode * dir, struct dentry *dentry,
+			int mode, int rdev)
+{
+	handle_t *handle;
+	struct inode *inode;
+	int err;
+
+	handle = ext3_journal_start(dir, EXT3_DATA_TRANS_BLOCKS + 3);
+	if (IS_ERR(handle))
+		return PTR_ERR(handle);
+
+	if (IS_SYNC(dir))
+		handle->h_sync = 1;
+
+	inode = ext3_new_inode (handle, dir, mode);
+	err = PTR_ERR(inode);
+	if (!IS_ERR(inode)) {
+		init_special_inode(inode, mode, rdev);
+		ext3_mark_inode_dirty(handle, inode);
+		err = ext3_add_nondir(handle, dentry, inode);
+	}
+	ext3_journal_stop(handle, dir);
+	return err;
+}
+
+static int ext3_mkdir(struct inode * dir, struct dentry * dentry, int mode)
+{
+	handle_t *handle;
+	struct inode * inode;
+	struct buffer_head * dir_block;
+	struct ext3_dir_entry_2 * de;
+	int err;
+
+	if (dir->i_nlink >= EXT3_LINK_MAX)
+		return -EMLINK;
+
+	handle = ext3_journal_start(dir, EXT3_DATA_TRANS_BLOCKS + 3);
+	if (IS_ERR(handle))
+		return PTR_ERR(handle);
+
+	if (IS_SYNC(dir))
+		handle->h_sync = 1;
+
+	inode = ext3_new_inode (handle, dir, S_IFDIR);
+	err = PTR_ERR(inode);
+	if (IS_ERR(inode))
+		goto out_stop;
+
+	inode->i_op = &ext3_dir_inode_operations;
+	inode->i_fop = &ext3_dir_operations;
+	inode->i_size = inode->u.ext3_i.i_disksize = inode->i_sb->s_blocksize;
+	inode->i_blocks = 0;	
+	dir_block = ext3_bread (handle, inode, 0, 1, &err);
+	if (!dir_block) {
+		inode->i_nlink--; /* is this nlink == 0? */
+		ext3_mark_inode_dirty(handle, inode);
+		iput (inode);
+		goto out_stop;
+	}
+	BUFFER_TRACE(dir_block, "get_write_access");
+	ext3_journal_get_write_access(handle, dir_block);
+	de = (struct ext3_dir_entry_2 *) dir_block->b_data;
+	de->inode = cpu_to_le32(inode->i_ino);
+	de->name_len = 1;
+	de->rec_len = cpu_to_le16(EXT3_DIR_REC_LEN(de->name_len));
+	strcpy (de->name, ".");
+	ext3_set_de_type(dir->i_sb, de, S_IFDIR);
+	de = (struct ext3_dir_entry_2 *)
+			((char *) de + le16_to_cpu(de->rec_len));
+	de->inode = cpu_to_le32(dir->i_ino);
+	de->rec_len = cpu_to_le16(inode->i_sb->s_blocksize-EXT3_DIR_REC_LEN(1));
+	de->name_len = 2;
+	strcpy (de->name, "..");
+	ext3_set_de_type(dir->i_sb, de, S_IFDIR);
+	inode->i_nlink = 2;
+	BUFFER_TRACE(dir_block, "call ext3_journal_dirty_metadata");
+	ext3_journal_dirty_metadata(handle, dir_block);
+	brelse (dir_block);
+	inode->i_mode = S_IFDIR | mode;
+	if (dir->i_mode & S_ISGID)
+		inode->i_mode |= S_ISGID;
+	ext3_mark_inode_dirty(handle, inode);
+	err = ext3_add_entry (handle, dentry, inode);
+	if (err)
+		goto out_no_entry;
+	dir->i_nlink++;
+	dir->u.ext3_i.i_flags &= ~EXT3_INDEX_FL;
+	ext3_mark_inode_dirty(handle, dir);
+	d_instantiate(dentry, inode);
+out_stop:
+	ext3_journal_stop(handle, dir);
+	return err;
+
+out_no_entry:
+	inode->i_nlink = 0;
+	ext3_mark_inode_dirty(handle, inode);
+	iput (inode);
+	goto out_stop;
+}
+
+/*
+ * routine to check that the specified directory is empty (for rmdir)
+ */
+static int empty_dir (struct inode * inode)
+{
+	unsigned long offset;
+	struct buffer_head * bh;
+	struct ext3_dir_entry_2 * de, * de1;
+	struct super_block * sb;
+	int err;
+
+	sb = inode->i_sb;
+	if (inode->i_size < EXT3_DIR_REC_LEN(1) + EXT3_DIR_REC_LEN(2) ||
+	    !(bh = ext3_bread (NULL, inode, 0, 0, &err))) {
+	    	ext3_warning (inode->i_sb, "empty_dir",
+			      "bad directory (dir #%lu) - no data block",
+			      inode->i_ino);
+		return 1;
+	}
+	de = (struct ext3_dir_entry_2 *) bh->b_data;
+	de1 = (struct ext3_dir_entry_2 *)
+			((char *) de + le16_to_cpu(de->rec_len));
+	if (le32_to_cpu(de->inode) != inode->i_ino ||
+			!le32_to_cpu(de1->inode) || 
+			strcmp (".", de->name) ||
+			strcmp ("..", de1->name)) {
+	    	ext3_warning (inode->i_sb, "empty_dir",
+			      "bad directory (dir #%lu) - no `.' or `..'",
+			      inode->i_ino);
+		brelse (bh);
+		return 1;
+	}
+	offset = le16_to_cpu(de->rec_len) + le16_to_cpu(de1->rec_len);
+	de = (struct ext3_dir_entry_2 *)
+			((char *) de1 + le16_to_cpu(de1->rec_len));
+	while (offset < inode->i_size ) {
+		if (!bh ||
+			(void *) de >= (void *) (bh->b_data+sb->s_blocksize)) {
+			brelse (bh);
+			bh = ext3_bread (NULL, inode,
+				offset >> EXT3_BLOCK_SIZE_BITS(sb), 0, &err);
+			if (!bh) {
+#if 0
+				ext3_error (sb, "empty_dir",
+				"directory #%lu contains a hole at offset %lu",
+					inode->i_ino, offset);
+#endif
+				offset += sb->s_blocksize;
+				continue;
+			}
+			de = (struct ext3_dir_entry_2 *) bh->b_data;
+		}
+		if (!ext3_check_dir_entry ("empty_dir", inode, de, bh,
+					   offset)) {
+			brelse (bh);
+			return 1;
+		}
+		if (le32_to_cpu(de->inode)) {
+			brelse (bh);
+			return 0;
+		}
+		offset += le16_to_cpu(de->rec_len);
+		de = (struct ext3_dir_entry_2 *)
+				((char *) de + le16_to_cpu(de->rec_len));
+	}
+	brelse (bh);
+	return 1;
+}
+
+/* ext3_orphan_add() links an unlinked or truncated inode into a list of
+ * such inodes, starting at the superblock, in case we crash before the
+ * file is closed/deleted, or in case the inode truncate spans multiple
+ * transactions and the last transaction is not recovered after a crash.
+ *
+ * At filesystem recovery time, we walk this list deleting unlinked
+ * inodes and truncating linked inodes in ext3_orphan_cleanup().
+ */
+int ext3_orphan_add(handle_t *handle, struct inode *inode)
+{
+	struct super_block *sb = inode->i_sb;
+	struct ext3_iloc iloc;
+	int err = 0, rc;
+	
+	lock_super(sb);
+	if (!list_empty(&inode->u.ext3_i.i_orphan))
+		goto out_unlock;
+
+	/* Orphan handling is only valid for files with data blocks
+	 * being truncated, or files being unlinked. */
+
+	/* @@@ FIXME: Observation from aviro:
+	 * I think I can trigger J_ASSERT in ext3_orphan_add().  We block 
+	 * here (on lock_super()), so race with ext3_link() which might bump
+	 * ->i_nlink. For, say it, character device. Not a regular file,
+	 * not a directory, not a symlink and ->i_nlink > 0.
+	 */
+	J_ASSERT ((S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode) ||
+		S_ISLNK(inode->i_mode)) || inode->i_nlink == 0);
+
+	BUFFER_TRACE(sb->u.ext3_sb.s_sbh, "get_write_access");
+	err = ext3_journal_get_write_access(handle, sb->u.ext3_sb.s_sbh);
+	if (err)
+		goto out_unlock;
+	
+	err = ext3_reserve_inode_write(handle, inode, &iloc);
+	if (err)
+		goto out_unlock;
+
+	/* Insert this inode at the head of the on-disk orphan list... */
+	NEXT_ORPHAN(inode) = le32_to_cpu(EXT3_SB(sb)->s_es->s_last_orphan);
+	EXT3_SB(sb)->s_es->s_last_orphan = cpu_to_le32(inode->i_ino);
+	err = ext3_journal_dirty_metadata(handle, sb->u.ext3_sb.s_sbh);
+	rc = ext3_mark_iloc_dirty(handle, inode, &iloc);
+	if (!err)
+		err = rc;
+
+	/* Only add to the head of the in-memory list if all the
+	 * previous operations succeeded.  If the orphan_add is going to
+	 * fail (possibly taking the journal offline), we can't risk
+	 * leaving the inode on the orphan list: stray orphan-list
+	 * entries can cause panics at unmount time.
+	 *
+	 * This is safe: on error we're going to ignore the orphan list
+	 * anyway on the next recovery. */
+	if (!err)
+		list_add(&inode->u.ext3_i.i_orphan, &EXT3_SB(sb)->s_orphan);
+
+	jbd_debug(4, "superblock will point to %ld\n", inode->i_ino);
+	jbd_debug(4, "orphan inode %ld will point to %d\n",
+			inode->i_ino, NEXT_ORPHAN(inode));
+out_unlock:
+	unlock_super(sb);
+	ext3_std_error(inode->i_sb, err);
+	return err;
+}
+
+/*
+ * ext3_orphan_del() removes an unlinked or truncated inode from the list
+ * of such inodes stored on disk, because it is finally being cleaned up.
+ */
+int ext3_orphan_del(handle_t *handle, struct inode *inode)
+{
+	struct list_head *prev;
+	struct ext3_sb_info *sbi;
+	ino_t ino_next; 
+	struct ext3_iloc iloc;
+	int err = 0;
+	
+	lock_super(inode->i_sb);
+	if (list_empty(&inode->u.ext3_i.i_orphan)) {
+		unlock_super(inode->i_sb);
+		return 0;
+	}
+
+	ino_next = NEXT_ORPHAN(inode);
+	prev = inode->u.ext3_i.i_orphan.prev;
+	sbi = EXT3_SB(inode->i_sb);
+
+	jbd_debug(4, "remove inode %ld from orphan list\n", inode->i_ino);
+
+	list_del(&inode->u.ext3_i.i_orphan);
+	INIT_LIST_HEAD(&inode->u.ext3_i.i_orphan);
+
+	/* If we're on an error path, we may not have a valid
+	 * transaction handle with which to update the orphan list on
+	 * disk, but we still need to remove the inode from the linked
+	 * list in memory. */
+	if (!handle)
+		goto out;
+	
+	err = ext3_reserve_inode_write(handle, inode, &iloc);
+	if (err)
+		goto out_err;
+
+	if (prev == &sbi->s_orphan) {
+		jbd_debug(4, "superblock will point to %ld\n", ino_next);
+		BUFFER_TRACE(sbi->s_sbh, "get_write_access");
+		err = ext3_journal_get_write_access(handle, sbi->s_sbh);
+		if (err)
+			goto out_brelse;
+		sbi->s_es->s_last_orphan = cpu_to_le32(ino_next);
+		err = ext3_journal_dirty_metadata(handle, sbi->s_sbh);
+	} else {
+		struct ext3_iloc iloc2;
+		struct inode *i_prev =
+			list_entry(prev, struct inode, u.ext3_i.i_orphan);
+		
+		jbd_debug(4, "orphan inode %ld will point to %ld\n",
+			  i_prev->i_ino, ino_next);
+		err = ext3_reserve_inode_write(handle, i_prev, &iloc2);
+		if (err)
+			goto out_brelse;
+		NEXT_ORPHAN(i_prev) = ino_next;
+		err = ext3_mark_iloc_dirty(handle, i_prev, &iloc2);
+	}
+	if (err)
+		goto out_brelse;
+	NEXT_ORPHAN(inode) = 0;
+	err = ext3_mark_iloc_dirty(handle, inode, &iloc);
+	if (err)
+		goto out_brelse;
+
+out_err: 	
+	ext3_std_error(inode->i_sb, err);
+out:
+	unlock_super(inode->i_sb);
+	return err;
+
+out_brelse:
+	brelse(iloc.bh);
+	goto out_err;
+}
+
+static int ext3_rmdir (struct inode * dir, struct dentry *dentry)
+{
+	int retval;
+	struct inode * inode;
+	struct buffer_head * bh;
+	struct ext3_dir_entry_2 * de;
+	handle_t *handle;
+
+	handle = ext3_journal_start(dir, EXT3_DELETE_TRANS_BLOCKS);
+	if (IS_ERR(handle))
+		return PTR_ERR(handle);
+
+	retval = -ENOENT;
+	bh = ext3_find_entry (dentry, &de);
+	if (!bh)
+		goto end_rmdir;
+
+	if (IS_SYNC(dir))
+		handle->h_sync = 1;
+
+	inode = dentry->d_inode;
+	DQUOT_INIT(inode);
+
+	retval = -EIO;
+	if (le32_to_cpu(de->inode) != inode->i_ino)
+		goto end_rmdir;
+
+	retval = -ENOTEMPTY;
+	if (!empty_dir (inode))
+		goto end_rmdir;
+
+	retval = ext3_delete_entry(handle, dir, de, bh);
+	if (retval)
+		goto end_rmdir;
+	if (inode->i_nlink != 2)
+		ext3_warning (inode->i_sb, "ext3_rmdir",
+			      "empty directory has nlink!=2 (%d)",
+			      inode->i_nlink);
+	inode->i_version = ++event;
+	inode->i_nlink = 0;
+	/* There's no need to set i_disksize: the fact that i_nlink is
+	 * zero will ensure that the right thing happens during any
+	 * recovery. */
+	inode->i_size = 0;
+	ext3_orphan_add(handle, inode);
+	ext3_mark_inode_dirty(handle, inode);
+	dir->i_nlink--;
+	inode->i_ctime = dir->i_ctime = dir->i_mtime = CURRENT_TIME;
+	dir->u.ext3_i.i_flags &= ~EXT3_INDEX_FL;
+	ext3_mark_inode_dirty(handle, dir);
+
+end_rmdir:
+	ext3_journal_stop(handle, dir);
+	brelse (bh);
+	return retval;
+}
+
+static int ext3_unlink(struct inode * dir, struct dentry *dentry)
+{
+	int retval;
+	struct inode * inode;
+	struct buffer_head * bh;
+	struct ext3_dir_entry_2 * de;
+	handle_t *handle;
+
+	handle = ext3_journal_start(dir, EXT3_DELETE_TRANS_BLOCKS);
+	if (IS_ERR(handle))
+		return PTR_ERR(handle);
+
+	if (IS_SYNC(dir))
+		handle->h_sync = 1;
+
+	retval = -ENOENT;
+	bh = ext3_find_entry (dentry, &de);
+	if (!bh)
+		goto end_unlink;
+
+	inode = dentry->d_inode;
+	DQUOT_INIT(inode);
+
+	retval = -EIO;
+	if (le32_to_cpu(de->inode) != inode->i_ino)
+		goto end_unlink;
+	
+	if (!inode->i_nlink) {
+		ext3_warning (inode->i_sb, "ext3_unlink",
+			      "Deleting nonexistent file (%lu), %d",
+			      inode->i_ino, inode->i_nlink);
+		inode->i_nlink = 1;
+	}
+	retval = ext3_delete_entry(handle, dir, de, bh);
+	if (retval)
+		goto end_unlink;
+	dir->i_ctime = dir->i_mtime = CURRENT_TIME;
+	dir->u.ext3_i.i_flags &= ~EXT3_INDEX_FL;
+	ext3_mark_inode_dirty(handle, dir);
+	inode->i_nlink--;
+	if (!inode->i_nlink)
+		ext3_orphan_add(handle, inode);
+	ext3_mark_inode_dirty(handle, inode);
+	inode->i_ctime = dir->i_ctime;
+	retval = 0;
+
+end_unlink:
+	ext3_journal_stop(handle, dir);
+	brelse (bh);
+	return retval;
+}
+
+static int ext3_symlink (struct inode * dir,
+		struct dentry *dentry, const char * symname)
+{
+	handle_t *handle;
+	struct inode * inode;
+	int l, err;
+
+	l = strlen(symname)+1;
+	if (l > dir->i_sb->s_blocksize)
+		return -ENAMETOOLONG;
+
+	handle = ext3_journal_start(dir, EXT3_DATA_TRANS_BLOCKS + 5);
+	if (IS_ERR(handle))
+		return PTR_ERR(handle);
+
+	if (IS_SYNC(dir))
+		handle->h_sync = 1;
+
+	inode = ext3_new_inode (handle, dir, S_IFLNK|S_IRWXUGO);
+	err = PTR_ERR(inode);
+	if (IS_ERR(inode))
+		goto out_stop;
+
+	if (l > sizeof (inode->u.ext3_i.i_data)) {
+		inode->i_op = &page_symlink_inode_operations;
+		inode->i_mapping->a_ops = &ext3_aops;
+		/*
+		 * block_symlink() calls back into ext3_prepare/commit_write.
+		 * We have a transaction open.  All is sweetness.  It also sets
+		 * i_size in generic_commit_write().
+		 */
+		err = block_symlink(inode, symname, l);
+		if (err)
+			goto out_no_entry;
+	} else {
+		inode->i_op = &ext3_fast_symlink_inode_operations;
+		memcpy((char*)&inode->u.ext3_i.i_data,symname,l);
+		inode->i_size = l-1;
+	}
+	inode->u.ext3_i.i_disksize = inode->i_size;
+	ext3_mark_inode_dirty(handle, inode);
+	err = ext3_add_nondir(handle, dentry, inode);
+out_stop:
+	ext3_journal_stop(handle, dir);
+	return err;
+
+out_no_entry:
+	ext3_dec_count(handle, inode);
+	ext3_mark_inode_dirty(handle, inode);
+	iput (inode);
+	goto out_stop;
+}
+
+static int ext3_link (struct dentry * old_dentry,
+		struct inode * dir, struct dentry *dentry)
+{
+	handle_t *handle;
+	struct inode *inode = old_dentry->d_inode;
+	int err;
+
+	if (S_ISDIR(inode->i_mode))
+		return -EPERM;
+
+	if (inode->i_nlink >= EXT3_LINK_MAX)
+		return -EMLINK;
+
+	handle = ext3_journal_start(dir, EXT3_DATA_TRANS_BLOCKS);
+	if (IS_ERR(handle))
+		return PTR_ERR(handle);
+
+	if (IS_SYNC(dir))
+		handle->h_sync = 1;
+
+	inode->i_ctime = CURRENT_TIME;
+	ext3_inc_count(handle, inode);
+	atomic_inc(&inode->i_count);
+
+	ext3_mark_inode_dirty(handle, inode);
+	err = ext3_add_nondir(handle, dentry, inode);
+	ext3_journal_stop(handle, dir);
+	return err;
+}
+
+#define PARENT_INO(buffer) \
+	((struct ext3_dir_entry_2 *) ((char *) buffer + \
+	le16_to_cpu(((struct ext3_dir_entry_2 *) buffer)->rec_len)))->inode
+
+/*
+ * Anybody can rename anything with this: the permission checks are left to the
+ * higher-level routines.
+ */
+static int ext3_rename (struct inode * old_dir, struct dentry *old_dentry,
+			   struct inode * new_dir,struct dentry *new_dentry)
+{
+	handle_t *handle;
+	struct inode * old_inode, * new_inode;
+	struct buffer_head * old_bh, * new_bh, * dir_bh;
+	struct ext3_dir_entry_2 * old_de, * new_de;
+	int retval;
+
+	old_bh = new_bh = dir_bh = NULL;
+
+	handle = ext3_journal_start(old_dir, 2 * EXT3_DATA_TRANS_BLOCKS + 2);
+	if (IS_ERR(handle))
+		return PTR_ERR(handle);
+
+	if (IS_SYNC(old_dir) || IS_SYNC(new_dir))
+		handle->h_sync = 1;
+
+	old_bh = ext3_find_entry (old_dentry, &old_de);
+	/*
+	 *  Check for inode number is _not_ due to possible IO errors.
+	 *  We might rmdir the source, keep it as pwd of some process
+	 *  and merrily kill the link to whatever was created under the
+	 *  same name. Goodbye sticky bit ;-<
+	 */
+	old_inode = old_dentry->d_inode;
+	retval = -ENOENT;
+	if (!old_bh || le32_to_cpu(old_de->inode) != old_inode->i_ino)
+		goto end_rename;
+
+	new_inode = new_dentry->d_inode;
+	new_bh = ext3_find_entry (new_dentry, &new_de);
+	if (new_bh) {
+		if (!new_inode) {
+			brelse (new_bh);
+			new_bh = NULL;
+		} else {
+			DQUOT_INIT(new_inode);
+		}
+	}
+	if (S_ISDIR(old_inode->i_mode)) {
+		if (new_inode) {
+			retval = -ENOTEMPTY;
+			if (!empty_dir (new_inode))
+				goto end_rename;
+		}
+		retval = -EIO;
+		dir_bh = ext3_bread (handle, old_inode, 0, 0, &retval);
+		if (!dir_bh)
+			goto end_rename;
+		if (le32_to_cpu(PARENT_INO(dir_bh->b_data)) != old_dir->i_ino)
+			goto end_rename;
+		retval = -EMLINK;
+		if (!new_inode && new_dir!=old_dir &&
+				new_dir->i_nlink >= EXT3_LINK_MAX)
+			goto end_rename;
+	}
+	if (!new_bh) {
+		retval = ext3_add_entry (handle, new_dentry, old_inode);
+		if (retval)
+			goto end_rename;
+	} else {
+		BUFFER_TRACE(new_bh, "get write access");
+		BUFFER_TRACE(new_bh, "get_write_access");
+		ext3_journal_get_write_access(handle, new_bh);
+		new_de->inode = le32_to_cpu(old_inode->i_ino);
+		if (EXT3_HAS_INCOMPAT_FEATURE(new_dir->i_sb,
+					      EXT3_FEATURE_INCOMPAT_FILETYPE))
+			new_de->file_type = old_de->file_type;
+		new_dir->i_version = ++event;
+		BUFFER_TRACE(new_bh, "call ext3_journal_dirty_metadata");
+		ext3_journal_dirty_metadata(handle, new_bh);
+		brelse(new_bh);
+		new_bh = NULL;
+	}
+
+	/*
+	 * Like most other Unix systems, set the ctime for inodes on a
+	 * rename.
+	 */
+	old_inode->i_ctime = CURRENT_TIME;
+	ext3_mark_inode_dirty(handle, old_inode);
+
+	/*
+	 * ok, that's it
+	 */
+	ext3_delete_entry(handle, old_dir, old_de, old_bh);
+
+	if (new_inode) {
+		new_inode->i_nlink--;
+		new_inode->i_ctime = CURRENT_TIME;
+	}
+	old_dir->i_ctime = old_dir->i_mtime = CURRENT_TIME;
+	old_dir->u.ext3_i.i_flags &= ~EXT3_INDEX_FL;
+	if (dir_bh) {
+		BUFFER_TRACE(dir_bh, "get_write_access");
+		ext3_journal_get_write_access(handle, dir_bh);
+		PARENT_INO(dir_bh->b_data) = le32_to_cpu(new_dir->i_ino);
+		BUFFER_TRACE(dir_bh, "call ext3_journal_dirty_metadata");
+		ext3_journal_dirty_metadata(handle, dir_bh);
+		old_dir->i_nlink--;
+		if (new_inode) {
+			new_inode->i_nlink--;
+		} else {
+			new_dir->i_nlink++;
+			new_dir->u.ext3_i.i_flags &= ~EXT3_INDEX_FL;
+			ext3_mark_inode_dirty(handle, new_dir);
+		}
+	}
+	ext3_mark_inode_dirty(handle, old_dir);
+	if (new_inode) {
+		ext3_mark_inode_dirty(handle, new_inode);
+		if (!new_inode->i_nlink)
+			ext3_orphan_add(handle, new_inode);
+	}
+	retval = 0;
+
+end_rename:
+	brelse (dir_bh);
+	brelse (old_bh);
+	brelse (new_bh);
+	ext3_journal_stop(handle, old_dir);
+	return retval;
+}
+
+/*
+ * directories can handle most operations...
+ */
+struct inode_operations ext3_dir_inode_operations = {
+	create:		ext3_create,		/* BKL held */
+	lookup:		ext3_lookup,		/* BKL held */
+	link:		ext3_link,		/* BKL held */
+	unlink:		ext3_unlink,		/* BKL held */
+	symlink:	ext3_symlink,		/* BKL held */
+	mkdir:		ext3_mkdir,		/* BKL held */
+	rmdir:		ext3_rmdir,		/* BKL held */
+	mknod:		ext3_mknod,		/* BKL held */
+	rename:		ext3_rename,		/* BKL held */
+};
diff -rup --new-file linux.mcp2/fs/ext3/super.c linux_tmp/fs/ext3/super.c
--- linux.mcp2/fs/ext3/super.c	1969-12-31 16:00:00.000000000 -0800
+++ linux_tmp/fs/ext3/super.c	2002-02-25 11:38:08.000000000 -0800
@@ -0,0 +1,1753 @@
+/*
+ *  linux/fs/ext3/super.c
+ *
+ * Copyright (C) 1992, 1993, 1994, 1995
+ * Remy Card (card@masi.ibp.fr)
+ * Laboratoire MASI - Institut Blaise Pascal
+ * Universite Pierre et Marie Curie (Paris VI)
+ *
+ *  from
+ *
+ *  linux/fs/minix/inode.c
+ *
+ *  Copyright (C) 1991, 1992  Linus Torvalds
+ *
+ *  Big-endian to little-endian byte-swapping/bitmaps by
+ *        David S. Miller (davem@caip.rutgers.edu), 1995
+ */
+
+#include <linux/config.h>
+#include <linux/module.h>
+#include <linux/string.h>
+#include <linux/fs.h>
+#include <linux/sched.h>
+#include <linux/jbd.h>
+#include <linux/ext3_fs.h>
+#include <linux/ext3_jbd.h>
+#include <linux/slab.h>
+#include <linux/init.h>
+#include <linux/locks.h>
+#include <linux/blkdev.h>
+#include <linux/smp_lock.h>
+#include <linux/random.h>
+#include <asm/uaccess.h>
+
+#ifdef CONFIG_JBD_DEBUG
+static int ext3_ro_after; /* Make fs read-only after this many jiffies */
+#endif
+
+static int ext3_load_journal(struct super_block *, struct ext3_super_block *);
+static int ext3_create_journal(struct super_block *, struct ext3_super_block *,
+			       int);
+static void ext3_commit_super (struct super_block * sb,
+			       struct ext3_super_block * es,
+			       int sync);
+static void ext3_mark_recovery_complete(struct super_block * sb,
+					struct ext3_super_block * es);
+static void ext3_clear_journal_err(struct super_block * sb,
+				   struct ext3_super_block * es);
+
+#ifdef CONFIG_JBD_DEBUG
+int journal_no_write[2];
+
+/*
+ * Debug code for turning filesystems "read-only" after a specified
+ * amount of time.  This is for crash/recovery testing.
+ */
+
+static void make_rdonly(kdev_t dev, int *no_write)
+{
+	if (dev) {
+		printk(KERN_WARNING "Turning device %s read-only\n", 
+		       bdevname(dev));
+		*no_write = 0xdead0000 + dev;
+	}
+}
+
+static void turn_fs_readonly(unsigned long arg)
+{
+	struct super_block *sb = (struct super_block *)arg;
+
+	make_rdonly(sb->s_dev, &journal_no_write[0]);
+	make_rdonly(EXT3_SB(sb)->s_journal->j_dev, &journal_no_write[1]);
+	wake_up(&EXT3_SB(sb)->ro_wait_queue);
+}
+
+static void setup_ro_after(struct super_block *sb)
+{
+	struct ext3_sb_info *sbi = EXT3_SB(sb);
+	init_timer(&sbi->turn_ro_timer);
+	if (ext3_ro_after) {
+		printk(KERN_DEBUG "fs will go read-only in %d jiffies\n",
+		       ext3_ro_after);
+		init_waitqueue_head(&sbi->ro_wait_queue);
+		journal_no_write[0] = 0;
+		journal_no_write[1] = 0;
+		sbi->turn_ro_timer.function = turn_fs_readonly;
+		sbi->turn_ro_timer.data = (unsigned long)sb;
+		sbi->turn_ro_timer.expires = jiffies + ext3_ro_after;
+		ext3_ro_after = 0;
+		add_timer(&sbi->turn_ro_timer);
+	}
+}
+
+static void clear_ro_after(struct super_block *sb)
+{
+	del_timer_sync(&EXT3_SB(sb)->turn_ro_timer);
+	journal_no_write[0] = 0;
+	journal_no_write[1] = 0;
+	ext3_ro_after = 0;
+}
+#else
+#define setup_ro_after(sb)	do {} while (0)
+#define clear_ro_after(sb)	do {} while (0)
+#endif
+
+
+static char error_buf[1024];
+
+/* Determine the appropriate response to ext3_error on a given filesystem */
+
+static int ext3_error_behaviour(struct super_block *sb)
+{
+	/* First check for mount-time options */
+	if (test_opt (sb, ERRORS_PANIC))
+		return EXT3_ERRORS_PANIC;
+	if (test_opt (sb, ERRORS_RO))
+		return EXT3_ERRORS_RO;
+	if (test_opt (sb, ERRORS_CONT))
+		return EXT3_ERRORS_CONTINUE;
+	
+	/* If no overrides were specified on the mount, then fall back
+	 * to the default behaviour set in the filesystem's superblock
+	 * on disk. */
+	switch (le16_to_cpu(sb->u.ext3_sb.s_es->s_errors)) {
+	case EXT3_ERRORS_PANIC:
+		return EXT3_ERRORS_PANIC;
+	case EXT3_ERRORS_RO:
+		return EXT3_ERRORS_RO;
+	default:
+		break;
+	}
+	return EXT3_ERRORS_CONTINUE;
+}
+
+/* Deal with the reporting of failure conditions on a filesystem such as
+ * inconsistencies detected or read IO failures.
+ *
+ * On ext2, we can store the error state of the filesystem in the
+ * superblock.  That is not possible on ext3, because we may have other
+ * write ordering constraints on the superblock which prevent us from
+ * writing it out straight away; and given that the journal is about to
+ * be aborted, we can't rely on the current, or future, transactions to
+ * write out the superblock safely.
+ *
+ * We'll just use the journal_abort() error code to record an error in
+ * the journal instead.  On recovery, the journal will compain about
+ * that error until we've noted it down and cleared it.
+ */
+
+static void ext3_handle_error(struct super_block *sb)
+{
+	struct ext3_super_block *es = EXT3_SB(sb)->s_es;
+
+	EXT3_SB(sb)->s_mount_state |= EXT3_ERROR_FS;
+	es->s_state |= cpu_to_le32(EXT3_ERROR_FS);
+
+	if (sb->s_flags & MS_RDONLY)
+		return;
+
+	if (ext3_error_behaviour(sb) != EXT3_ERRORS_CONTINUE) {
+		EXT3_SB(sb)->s_mount_opt |= EXT3_MOUNT_ABORT;
+		journal_abort(EXT3_SB(sb)->s_journal, -EIO);
+	}
+
+	if (ext3_error_behaviour(sb) == EXT3_ERRORS_PANIC) 
+		panic ("EXT3-fs (device %s): panic forced after error\n",
+		       bdevname(sb->s_dev));
+
+	if (ext3_error_behaviour(sb) == EXT3_ERRORS_RO) {
+		printk (KERN_CRIT "Remounting filesystem read-only\n");
+		sb->s_flags |= MS_RDONLY;
+	}
+
+	ext3_commit_super(sb, es, 1);
+}
+
+void ext3_error (struct super_block * sb, const char * function,
+		 const char * fmt, ...)
+{
+	va_list args;
+
+	va_start (args, fmt);
+	vsprintf (error_buf, fmt, args);
+	va_end (args);
+
+	printk (KERN_CRIT "EXT3-fs error (device %s): %s: %s\n",
+		bdevname(sb->s_dev), function, error_buf);
+
+	ext3_handle_error(sb);
+}
+
+const char *ext3_decode_error(struct super_block * sb, int errno, char nbuf[16])
+{
+	char *errstr = NULL;
+	
+	switch (errno) {
+	case -EIO:
+		errstr = "IO failure";
+		break;
+	case -ENOMEM:
+		errstr = "Out of memory";
+		break;
+	case -EROFS:
+		if (!sb || EXT3_SB(sb)->s_journal->j_flags & JFS_ABORT)
+			errstr = "Journal has aborted";
+		else
+			errstr = "Readonly filesystem";
+		break;
+	default:
+		/* If the caller passed in an extra buffer for unknown
+		 * errors, textualise them now.  Else we just return
+		 * NULL. */
+		if (nbuf) {
+			/* Check for truncated error codes... */
+			if (snprintf(nbuf, 16, "error %d", -errno) >= 0)
+				errstr = nbuf;
+		}
+		
+		break;
+	}
+
+	return errstr;
+}
+
+/* __ext3_std_error decodes expected errors from journaling functions
+ * automatically and invokes the appropriate error response.  */
+
+void __ext3_std_error (struct super_block * sb, const char * function,
+		       int errno)
+{
+	char nbuf[16];
+	const char *errstr = ext3_decode_error(sb, errno, nbuf);
+
+	printk (KERN_CRIT "EXT3-fs error (device %s) in %s: %s\n",
+		bdevname(sb->s_dev), function, errstr);
+	
+	ext3_handle_error(sb);
+}
+
+/*
+ * ext3_abort is a much stronger failure handler than ext3_error.  The
+ * abort function may be used to deal with unrecoverable failures such
+ * as journal IO errors or ENOMEM at a critical moment in log management.
+ *
+ * We unconditionally force the filesystem into an ABORT|READONLY state,
+ * unless the error response on the fs has been set to panic in which
+ * case we take the easy way out and panic immediately.
+ */
+
+void ext3_abort (struct super_block * sb, const char * function,
+		 const char * fmt, ...)
+{
+	va_list args;
+
+	printk (KERN_CRIT "ext3_abort called.\n");
+
+	va_start (args, fmt);
+	vsprintf (error_buf, fmt, args);
+	va_end (args);
+
+	if (ext3_error_behaviour(sb) == EXT3_ERRORS_PANIC)
+		panic ("EXT3-fs panic (device %s): %s: %s\n",
+		       bdevname(sb->s_dev), function, error_buf);
+
+	printk (KERN_CRIT "EXT3-fs abort (device %s): %s: %s\n",
+		bdevname(sb->s_dev), function, error_buf);
+
+	if (sb->s_flags & MS_RDONLY)
+		return;
+	
+	printk (KERN_CRIT "Remounting filesystem read-only\n");
+	sb->u.ext3_sb.s_mount_state |= EXT3_ERROR_FS;
+	sb->s_flags |= MS_RDONLY;
+	sb->u.ext3_sb.s_mount_opt |= EXT3_MOUNT_ABORT;
+	journal_abort(EXT3_SB(sb)->s_journal, -EIO);
+}
+
+/* Deal with the reporting of failure conditions while running, such as
+ * inconsistencies in operation or invalid system states.
+ *
+ * Use ext3_error() for cases of invalid filesystem states, as that will
+ * record an error on disk and force a filesystem check on the next boot.
+ */
+NORET_TYPE void ext3_panic (struct super_block * sb, const char * function,
+			    const char * fmt, ...)
+{
+	va_list args;
+
+	va_start (args, fmt);
+	vsprintf (error_buf, fmt, args);
+	va_end (args);
+
+	/* this is to prevent panic from syncing this filesystem */
+	/* AKPM: is this sufficient? */
+	sb->s_flags |= MS_RDONLY;
+	panic ("EXT3-fs panic (device %s): %s: %s\n",
+	       bdevname(sb->s_dev), function, error_buf);
+}
+
+void ext3_warning (struct super_block * sb, const char * function,
+		   const char * fmt, ...)
+{
+	va_list args;
+
+	va_start (args, fmt);
+	vsprintf (error_buf, fmt, args);
+	va_end (args);
+	printk (KERN_WARNING "EXT3-fs warning (device %s): %s: %s\n",
+		bdevname(sb->s_dev), function, error_buf);
+}
+
+void ext3_update_dynamic_rev(struct super_block *sb)
+{
+	struct ext3_super_block *es = EXT3_SB(sb)->s_es;
+
+	if (le32_to_cpu(es->s_rev_level) > EXT3_GOOD_OLD_REV)
+		return;
+
+	ext3_warning(sb, __FUNCTION__,
+		     "updating to rev %d because of new feature flag, "
+		     "running e2fsck is recommended",
+		     EXT3_DYNAMIC_REV);
+
+	es->s_first_ino = cpu_to_le32(EXT3_GOOD_OLD_FIRST_INO);
+	es->s_inode_size = cpu_to_le16(EXT3_GOOD_OLD_INODE_SIZE);
+	es->s_rev_level = cpu_to_le32(EXT3_DYNAMIC_REV);
+	/* leave es->s_feature_*compat flags alone */
+	/* es->s_uuid will be set by e2fsck if empty */
+
+	/*
+	 * The rest of the superblock fields should be zero, and if not it
+	 * means they are likely already in use, so leave them alone.  We
+	 * can leave it up to e2fsck to clean up any inconsistencies there.
+	 */
+}
+
+/*
+ * Open the external journal device
+ */
+static struct block_device *ext3_blkdev_get(kdev_t dev)
+{
+	struct block_device *bdev;
+	int err = -ENODEV;
+
+	bdev = bdget(kdev_t_to_nr(dev));
+	if (bdev == NULL)
+		goto fail;
+	err = blkdev_get(bdev, FMODE_READ|FMODE_WRITE, 0, BDEV_FS);
+	if (err < 0)
+		goto fail;
+	return bdev;
+
+fail:
+	printk(KERN_ERR "EXT3: failed to open journal device %s: %d\n",
+			bdevname(dev), err);
+	return NULL;
+}
+
+/*
+ * Release the journal device
+ */
+static int ext3_blkdev_put(struct block_device *bdev)
+{
+	return blkdev_put(bdev, BDEV_FS);
+}
+
+static int ext3_blkdev_remove(struct ext3_sb_info *sbi)
+{
+	struct block_device *bdev;
+	int ret = -ENODEV;
+
+	bdev = sbi->journal_bdev;
+	if (bdev) {
+		ret = ext3_blkdev_put(bdev);
+		sbi->journal_bdev = 0;
+	}
+	return ret;
+}
+
+#define orphan_list_entry(l) list_entry((l), struct inode, u.ext3_i.i_orphan)
+
+static void dump_orphan_list(struct super_block *sb, struct ext3_sb_info *sbi)
+{
+	struct list_head *l;
+	
+	printk(KERN_ERR "sb orphan head is %d\n", 
+	       le32_to_cpu(sbi->s_es->s_last_orphan));
+	
+	printk(KERN_ERR "sb_info orphan list:\n");
+	list_for_each(l, &sbi->s_orphan) {
+		struct inode *inode = orphan_list_entry(l);
+		printk(KERN_ERR "  "
+		       "inode 0x%04x:%ld at %p: mode %o, nlink %d, next %d\n",
+		       inode->i_dev, inode->i_ino, inode,
+		       inode->i_mode, inode->i_nlink, 
+		       le32_to_cpu(NEXT_ORPHAN(inode)));
+	}
+}
+
+void ext3_put_super (struct super_block * sb)
+{
+	struct ext3_sb_info *sbi = EXT3_SB(sb);
+	struct ext3_super_block *es = sbi->s_es;
+	kdev_t j_dev = sbi->s_journal->j_dev;
+	int i;
+
+	journal_destroy(sbi->s_journal);
+	if (!(sb->s_flags & MS_RDONLY)) {
+		EXT3_CLEAR_INCOMPAT_FEATURE(sb, EXT3_FEATURE_INCOMPAT_RECOVER);
+		es->s_state = le16_to_cpu(sbi->s_mount_state);
+		BUFFER_TRACE(sbi->s_sbh, "marking dirty");
+		mark_buffer_dirty(sbi->s_sbh);
+		ext3_commit_super(sb, es, 1);
+	}
+
+	for (i = 0; i < sbi->s_gdb_count; i++)
+		brelse(sbi->s_group_desc[i]);
+	kfree(sbi->s_group_desc);
+	for (i = 0; i < EXT3_MAX_GROUP_LOADED; i++)
+		brelse(sbi->s_inode_bitmap[i]);
+	for (i = 0; i < EXT3_MAX_GROUP_LOADED; i++)
+		brelse(sbi->s_block_bitmap[i]);
+	brelse(sbi->s_sbh);
+
+	/* Debugging code just in case the in-memory inode orphan list
+	 * isn't empty.  The on-disk one can be non-empty if we've
+	 * detected an error and taken the fs readonly, but the
+	 * in-memory list had better be clean by this point. */
+	if (!list_empty(&sbi->s_orphan))
+		dump_orphan_list(sb, sbi);
+	J_ASSERT(list_empty(&sbi->s_orphan));
+
+	invalidate_buffers(sb->s_dev);
+	if (j_dev != sb->s_dev) {
+		/*
+		 * Invalidate the journal device's buffers.  We don't want them
+		 * floating about in memory - the physical journal device may
+		 * hotswapped, and it breaks the `ro-after' testing code.
+		 */
+		fsync_no_super(j_dev);
+		invalidate_buffers(j_dev);
+		ext3_blkdev_remove(sbi);
+	}
+	clear_ro_after(sb);
+
+	return;
+}
+
+static struct super_operations ext3_sops = {
+	read_inode:	ext3_read_inode,	/* BKL held */
+	write_inode:	ext3_write_inode,	/* BKL not held.  Don't need */
+	dirty_inode:	ext3_dirty_inode,	/* BKL not held.  We take it */
+	put_inode:	ext3_put_inode,		/* BKL not held.  Don't need */
+	delete_inode:	ext3_delete_inode,	/* BKL not held.  We take it */
+	put_super:	ext3_put_super,		/* BKL held */
+	write_super:	ext3_write_super,	/* BKL held */
+	write_super_lockfs: ext3_write_super_lockfs, /* BKL not held. Take it */
+	unlockfs:	ext3_unlockfs,		/* BKL not held.  We take it */
+	statfs:		ext3_statfs,		/* BKL held */
+	remount_fs:	ext3_remount,		/* BKL held */
+};
+
+static int want_value(char *value, char *option)
+{
+	if (!value || !*value) {
+		printk(KERN_NOTICE "EXT3-fs: the %s option needs an argument\n",
+		       option);
+		return -1;
+	}
+	return 0;
+}
+
+static int want_null_value(char *value, char *option)
+{
+	if (*value) {
+		printk(KERN_NOTICE "EXT3-fs: Invalid %s argument: %s\n",
+		       option, value);
+		return -1;
+	}
+	return 0;
+}
+
+static int want_numeric(char *value, char *option, unsigned long *number)
+{
+	if (want_value(value, option))
+		return -1;
+	*number = simple_strtoul(value, &value, 0);
+	if (want_null_value(value, option))
+		return -1;
+	return 0;
+}
+
+/*
+ * This function has been shamelessly adapted from the msdos fs
+ */
+static int parse_options (char * options, unsigned long * sb_block,
+			  struct ext3_sb_info *sbi,
+			  unsigned long * inum,
+			  int is_remount)
+{
+	unsigned long *mount_options = &sbi->s_mount_opt;
+	uid_t *resuid = &sbi->s_resuid;
+	gid_t *resgid = &sbi->s_resgid;
+	char * this_char;
+	char * value;
+
+	if (!options)
+		return 1;
+	for (this_char = strtok (options, ",");
+	     this_char != NULL;
+	     this_char = strtok (NULL, ",")) {
+		if ((value = strchr (this_char, '=')) != NULL)
+			*value++ = 0;
+		if (!strcmp (this_char, "bsddf"))
+			clear_opt (*mount_options, MINIX_DF);
+		else if (!strcmp (this_char, "nouid32")) {
+			set_opt (*mount_options, NO_UID32);
+		}
+		else if (!strcmp (this_char, "abort"))
+			set_opt (*mount_options, ABORT);
+		else if (!strcmp (this_char, "check")) {
+			if (!value || !*value || !strcmp (value, "none"))
+				clear_opt (*mount_options, CHECK);
+			else
+#ifdef CONFIG_EXT3_CHECK
+				set_opt (*mount_options, CHECK);
+#else
+				printk(KERN_ERR 
+				       "EXT3 Check option not supported\n");
+#endif
+		}
+		else if (!strcmp (this_char, "debug"))
+			set_opt (*mount_options, DEBUG);
+		else if (!strcmp (this_char, "errors")) {
+			if (want_value(value, "errors"))
+				return 0;
+			if (!strcmp (value, "continue")) {
+				clear_opt (*mount_options, ERRORS_RO);
+				clear_opt (*mount_options, ERRORS_PANIC);
+				set_opt (*mount_options, ERRORS_CONT);
+			}
+			else if (!strcmp (value, "remount-ro")) {
+				clear_opt (*mount_options, ERRORS_CONT);
+				clear_opt (*mount_options, ERRORS_PANIC);
+				set_opt (*mount_options, ERRORS_RO);
+			}
+			else if (!strcmp (value, "panic")) {
+				clear_opt (*mount_options, ERRORS_CONT);
+				clear_opt (*mount_options, ERRORS_RO);
+				set_opt (*mount_options, ERRORS_PANIC);
+			}
+			else {
+				printk (KERN_ERR
+					"EXT3-fs: Invalid errors option: %s\n",
+					value);
+				return 0;
+			}
+		}
+		else if (!strcmp (this_char, "grpid") ||
+			 !strcmp (this_char, "bsdgroups"))
+			set_opt (*mount_options, GRPID);
+		else if (!strcmp (this_char, "minixdf"))
+			set_opt (*mount_options, MINIX_DF);
+		else if (!strcmp (this_char, "nocheck"))
+			clear_opt (*mount_options, CHECK);
+		else if (!strcmp (this_char, "nogrpid") ||
+			 !strcmp (this_char, "sysvgroups"))
+			clear_opt (*mount_options, GRPID);
+		else if (!strcmp (this_char, "resgid")) {
+			unsigned long v;
+			if (want_numeric(value, "resgid", &v))
+				return 0;
+			*resgid = v;
+		}
+		else if (!strcmp (this_char, "resuid")) {
+			unsigned long v;
+			if (want_numeric(value, "resuid", &v))
+				return 0;
+			*resuid = v;
+		}
+		else if (!strcmp (this_char, "sb")) {
+			if (want_numeric(value, "sb", sb_block))
+				return 0;
+		}
+#ifdef CONFIG_JBD_DEBUG
+		else if (!strcmp (this_char, "ro-after")) {
+			unsigned long v;
+			if (want_numeric(value, "ro-after", &v))
+				return 0;
+			ext3_ro_after = v;
+		}
+#endif
+		/* Silently ignore the quota options */
+		else if (!strcmp (this_char, "grpquota")
+		         || !strcmp (this_char, "noquota")
+		         || !strcmp (this_char, "quota")
+		         || !strcmp (this_char, "usrquota"))
+			/* Don't do anything ;-) */ ;
+		else if (!strcmp (this_char, "journal")) {
+			/* @@@ FIXME */
+			/* Eventually we will want to be able to create
+                           a journal file here.  For now, only allow the
+                           user to specify an existing inode to be the
+                           journal file. */
+			if (is_remount) {
+				printk(KERN_ERR "EXT3-fs: cannot specify "
+				       "journal on remount\n");
+				return 0;
+			}
+
+			if (want_value(value, "journal"))
+				return 0;
+			if (!strcmp (value, "update"))
+				set_opt (*mount_options, UPDATE_JOURNAL);
+			else if (want_numeric(value, "journal", inum))
+				return 0;
+		}
+		else if (!strcmp (this_char, "noload"))
+			set_opt (*mount_options, NOLOAD);
+		else if (!strcmp (this_char, "data")) {
+			int data_opt = 0;
+
+			if (want_value(value, "data"))
+				return 0;
+			if (!strcmp (value, "journal"))
+				data_opt = EXT3_MOUNT_JOURNAL_DATA;
+			else if (!strcmp (value, "ordered"))
+				data_opt = EXT3_MOUNT_ORDERED_DATA;
+			else if (!strcmp (value, "writeback"))
+				data_opt = EXT3_MOUNT_WRITEBACK_DATA;
+			else {
+				printk (KERN_ERR 
+					"EXT3-fs: Invalid data option: %s\n",
+					value);
+				return 0;
+			}
+			if (is_remount) {
+				if ((*mount_options & EXT3_MOUNT_DATA_FLAGS) !=
+							data_opt) {
+					printk(KERN_ERR
+					       "EXT3-fs: cannot change data "
+					       "mode on remount\n");
+					return 0;
+				}
+			} else {
+				*mount_options &= ~EXT3_MOUNT_DATA_FLAGS;
+				*mount_options |= data_opt;
+			}
+		} else {
+			printk (KERN_ERR 
+				"EXT3-fs: Unrecognized mount option %s\n",
+				this_char);
+			return 0;
+		}
+	}
+	return 1;
+}
+
+static int ext3_setup_super(struct super_block *sb, struct ext3_super_block *es,
+			    int read_only)
+{
+	struct ext3_sb_info *sbi = EXT3_SB(sb);
+	int res = 0;
+
+	if (le32_to_cpu(es->s_rev_level) > EXT3_MAX_SUPP_REV) {
+		printk (KERN_ERR "EXT3-fs warning: revision level too high, "
+			"forcing read-only mode\n");
+		res = MS_RDONLY;
+	}
+	if (read_only)
+		return res;
+	if (!(sbi->s_mount_state & EXT3_VALID_FS))
+		printk (KERN_WARNING "EXT3-fs warning: mounting unchecked fs, "
+			"running e2fsck is recommended\n");
+	else if ((sbi->s_mount_state & EXT3_ERROR_FS))
+		printk (KERN_WARNING
+			"EXT3-fs warning: mounting fs with errors, "
+			"running e2fsck is recommended\n");
+	else if ((__s16) le16_to_cpu(es->s_max_mnt_count) >= 0 &&
+		 le16_to_cpu(es->s_mnt_count) >=
+		 (unsigned short) (__s16) le16_to_cpu(es->s_max_mnt_count))
+		printk (KERN_WARNING
+			"EXT3-fs warning: maximal mount count reached, "
+			"running e2fsck is recommended\n");
+	else if (le32_to_cpu(es->s_checkinterval) &&
+		(le32_to_cpu(es->s_lastcheck) +
+			le32_to_cpu(es->s_checkinterval) <= CURRENT_TIME))
+		printk (KERN_WARNING
+			"EXT3-fs warning: checktime reached, "
+			"running e2fsck is recommended\n");
+#if 0
+		/* @@@ We _will_ want to clear the valid bit if we find
+                   inconsistencies, to force a fsck at reboot.  But for
+                   a plain journaled filesystem we can keep it set as
+                   valid forever! :) */
+	es->s_state = cpu_to_le16(le16_to_cpu(es->s_state) & ~EXT3_VALID_FS);
+#endif
+	if (!(__s16) le16_to_cpu(es->s_max_mnt_count))
+		es->s_max_mnt_count =
+			(__s16) cpu_to_le16(EXT3_DFL_MAX_MNT_COUNT);
+	es->s_mnt_count=cpu_to_le16(le16_to_cpu(es->s_mnt_count) + 1);
+	es->s_mtime = cpu_to_le32(CURRENT_TIME);
+	ext3_update_dynamic_rev(sb);
+	EXT3_SET_INCOMPAT_FEATURE(sb, EXT3_FEATURE_INCOMPAT_RECOVER);
+	ext3_commit_super (sb, es, 1);
+	if (test_opt (sb, DEBUG))
+		printk (KERN_INFO
+			"[EXT3 FS %s, %s, bs=%lu, gc=%lu, "
+			"bpg=%lu, ipg=%lu, mo=%04lx]\n",
+			EXT3FS_VERSION, EXT3FS_DATE, sb->s_blocksize,
+			sbi->s_groups_count,
+			EXT3_BLOCKS_PER_GROUP(sb),
+			EXT3_INODES_PER_GROUP(sb),
+			sbi->s_mount_opt);
+	printk(KERN_INFO "EXT3 FS " EXT3FS_VERSION ", " EXT3FS_DATE " on %s, ",
+				bdevname(sb->s_dev));
+	if (EXT3_SB(sb)->s_journal->j_inode == NULL) {
+		printk("external journal on %s\n",
+				bdevname(EXT3_SB(sb)->s_journal->j_dev));
+	} else {
+		printk("internal journal\n");
+	}
+#ifdef CONFIG_EXT3_CHECK
+	if (test_opt (sb, CHECK)) {
+		ext3_check_blocks_bitmap (sb);
+		ext3_check_inodes_bitmap (sb);
+	}
+#endif
+	setup_ro_after(sb);
+	return res;
+}
+
+static int ext3_check_descriptors (struct super_block * sb)
+{
+	struct ext3_sb_info *sbi = EXT3_SB(sb);
+	unsigned long block = le32_to_cpu(sbi->s_es->s_first_data_block);
+	struct ext3_group_desc * gdp = NULL;
+	int desc_block = 0;
+	int i;
+
+	ext3_debug ("Checking group descriptors");
+
+	for (i = 0; i < sbi->s_groups_count; i++)
+	{
+		if ((i % EXT3_DESC_PER_BLOCK(sb)) == 0)
+			gdp = (struct ext3_group_desc *)
+					sbi->s_group_desc[desc_block++]->b_data;
+		if (le32_to_cpu(gdp->bg_block_bitmap) < block ||
+		    le32_to_cpu(gdp->bg_block_bitmap) >=
+				block + EXT3_BLOCKS_PER_GROUP(sb))
+		{
+			ext3_error (sb, "ext3_check_descriptors",
+				    "Block bitmap for group %d"
+				    " not in group (block %lu)!",
+				    i, (unsigned long)
+					le32_to_cpu(gdp->bg_block_bitmap));
+			return 0;
+		}
+		if (le32_to_cpu(gdp->bg_inode_bitmap) < block ||
+		    le32_to_cpu(gdp->bg_inode_bitmap) >=
+				block + EXT3_BLOCKS_PER_GROUP(sb))
+		{
+			ext3_error (sb, "ext3_check_descriptors",
+				    "Inode bitmap for group %d"
+				    " not in group (block %lu)!",
+				    i, (unsigned long)
+					le32_to_cpu(gdp->bg_inode_bitmap));
+			return 0;
+		}
+		if (le32_to_cpu(gdp->bg_inode_table) < block ||
+		    le32_to_cpu(gdp->bg_inode_table) + sbi->s_itb_per_group >=
+		    block + EXT3_BLOCKS_PER_GROUP(sb))
+		{
+			ext3_error (sb, "ext3_check_descriptors",
+				    "Inode table for group %d"
+				    " not in group (block %lu)!",
+				    i, (unsigned long)
+					le32_to_cpu(gdp->bg_inode_table));
+			return 0;
+		}
+		block += EXT3_BLOCKS_PER_GROUP(sb);
+		gdp++;
+	}
+	return 1;
+}
+
+
+/* ext3_orphan_cleanup() walks a singly-linked list of inodes (starting at
+ * the superblock) which were deleted from all directories, but held open by
+ * a process at the time of a crash.  We walk the list and try to delete these
+ * inodes at recovery time (only with a read-write filesystem).
+ *
+ * In order to keep the orphan inode chain consistent during traversal (in
+ * case of crash during recovery), we link each inode into the superblock
+ * orphan list_head and handle it the same way as an inode deletion during
+ * normal operation (which journals the operations for us).
+ *
+ * We only do an iget() and an iput() on each inode, which is very safe if we
+ * accidentally point at an in-use or already deleted inode.  The worst that
+ * can happen in this case is that we get a "bit already cleared" message from
+ * ext3_free_inode().  The only reason we would point at a wrong inode is if
+ * e2fsck was run on this filesystem, and it must have already done the orphan
+ * inode cleanup for us, so we can safely abort without any further action.
+ */
+static void ext3_orphan_cleanup (struct super_block * sb,
+				 struct ext3_super_block * es)
+{
+	unsigned int s_flags = sb->s_flags;
+	int nr_orphans = 0, nr_truncates = 0;
+	if (!es->s_last_orphan) {
+		jbd_debug(4, "no orphan inodes to clean up\n");
+		return;
+	}
+
+	if (s_flags & MS_RDONLY) {
+		printk(KERN_INFO "EXT3-fs: %s: orphan cleanup on readonly fs\n",
+		       bdevname(sb->s_dev));
+		sb->s_flags &= ~MS_RDONLY;
+	}
+
+	if (sb->u.ext3_sb.s_mount_state & EXT3_ERROR_FS) {
+		if (es->s_last_orphan)
+			jbd_debug(1, "Errors on filesystem, "
+				  "clearing orphan list.\n");
+		es->s_last_orphan = 0;
+		jbd_debug(1, "Skipping orphan recovery on fs with errors.\n");
+		return;
+	}
+
+	while (es->s_last_orphan) {
+		struct inode *inode;
+
+		if (!(inode =
+		      ext3_orphan_get(sb, le32_to_cpu(es->s_last_orphan)))) {
+			es->s_last_orphan = 0;
+			break;
+		}
+
+		list_add(&EXT3_I(inode)->i_orphan, &EXT3_SB(sb)->s_orphan);
+		if (inode->i_nlink) {
+			printk(KERN_DEBUG __FUNCTION__
+				": truncating inode %ld to %Ld bytes\n",
+				inode->i_ino, inode->i_size);
+			jbd_debug(2, "truncating inode %ld to %Ld bytes\n",
+				  inode->i_ino, inode->i_size);
+			ext3_truncate(inode);
+			nr_truncates++;
+		} else {
+			printk(KERN_DEBUG __FUNCTION__
+				": deleting unreferenced inode %ld\n",
+				inode->i_ino);
+			jbd_debug(2, "deleting unreferenced inode %ld\n",
+				  inode->i_ino);
+			nr_orphans++;
+		}
+		iput(inode);  /* The delete magic happens here! */
+	}
+
+#define PLURAL(x) (x), ((x)==1) ? "" : "s"
+
+	if (nr_orphans)
+		printk(KERN_INFO "EXT3-fs: %s: %d orphan inode%s deleted\n",
+		       bdevname(sb->s_dev), PLURAL(nr_orphans));
+	if (nr_truncates)
+		printk(KERN_INFO "EXT3-fs: %s: %d truncate%s cleaned up\n",
+		       bdevname(sb->s_dev), PLURAL(nr_truncates));
+	sb->s_flags = s_flags; /* Restore MS_RDONLY status */
+}
+
+#define log2(n) ffz(~(n))
+
+/*
+ * Maximal file size.  There is a direct, and {,double-,triple-}indirect
+ * block limit, and also a limit of (2^32 - 1) 512-byte sectors in i_blocks.
+ * We need to be 1 filesystem block less than the 2^32 sector limit.
+ */
+static loff_t ext3_max_size(int bits)
+{
+	loff_t res = EXT3_NDIR_BLOCKS;
+	res += 1LL << (bits-2);
+	res += 1LL << (2*(bits-2));
+	res += 1LL << (3*(bits-2));
+	res <<= bits;
+	if (res > (512LL << 32) - (1 << bits))
+		res = (512LL << 32) - (1 << bits);
+	return res;
+}
+
+struct super_block * ext3_read_super (struct super_block * sb, void * data,
+				      int silent)
+{
+	struct buffer_head * bh;
+	struct ext3_super_block *es = 0;
+	struct ext3_sb_info *sbi = EXT3_SB(sb);
+	unsigned long sb_block = 1;
+	unsigned long logic_sb_block = 1;
+	unsigned long offset = 0;
+	unsigned long journal_inum = 0;
+	kdev_t dev = sb->s_dev;
+	int blocksize;
+	int hblock;
+	int db_count;
+	int i;
+	int needs_recovery;
+
+#ifdef CONFIG_JBD_DEBUG
+	ext3_ro_after = 0;
+#endif
+	/*
+	 * See what the current blocksize for the device is, and
+	 * use that as the blocksize.  Otherwise (or if the blocksize
+	 * is smaller than the default) use the default.
+	 * This is important for devices that have a hardware
+	 * sectorsize that is larger than the default.
+	 */
+	blocksize = EXT3_MIN_BLOCK_SIZE;
+	hblock = get_hardsect_size(dev);
+	if (blocksize < hblock)
+		blocksize = hblock;
+
+	sbi->s_mount_opt = 0;
+	sbi->s_resuid = EXT3_DEF_RESUID;
+	sbi->s_resgid = EXT3_DEF_RESGID;
+	if (!parse_options ((char *) data, &sb_block, sbi, &journal_inum, 0)) {
+		sb->s_dev = 0;
+		goto out_fail;
+	}
+
+	sb->s_blocksize = blocksize;
+	set_blocksize (dev, blocksize);
+
+	/*
+	 * The ext3 superblock will not be buffer aligned for other than 1kB
+	 * block sizes.  We need to calculate the offset from buffer start.
+	 */
+	if (blocksize != EXT3_MIN_BLOCK_SIZE) {
+		logic_sb_block = (sb_block * EXT3_MIN_BLOCK_SIZE) / blocksize;
+		offset = (sb_block * EXT3_MIN_BLOCK_SIZE) % blocksize;
+	}
+
+	if (!(bh = sb_bread(sb, logic_sb_block))) {
+		printk (KERN_ERR "EXT3-fs: unable to read superblock\n");
+		goto out_fail;
+	}
+	/*
+	 * Note: s_es must be initialized as soon as possible because
+	 *       some ext3 macro-instructions depend on its value
+	 */
+	es = (struct ext3_super_block *) (((char *)bh->b_data) + offset);
+	sbi->s_es = es;
+	sb->s_magic = le16_to_cpu(es->s_magic);
+	if (sb->s_magic != EXT3_SUPER_MAGIC) {
+		if (!silent)
+			printk(KERN_ERR 
+			       "VFS: Can't find ext3 filesystem on dev %s.\n",
+			       bdevname(dev));
+		goto failed_mount;
+	}
+	if (le32_to_cpu(es->s_rev_level) == EXT3_GOOD_OLD_REV &&
+	    (EXT3_HAS_COMPAT_FEATURE(sb, ~0U) ||
+	     EXT3_HAS_RO_COMPAT_FEATURE(sb, ~0U) ||
+	     EXT3_HAS_INCOMPAT_FEATURE(sb, ~0U)))
+		printk(KERN_WARNING 
+		       "EXT3-fs warning: feature flags set on rev 0 fs, "
+		       "running e2fsck is recommended\n");
+	/*
+	 * Check feature flags regardless of the revision level, since we
+	 * previously didn't change the revision level when setting the flags,
+	 * so there is a chance incompat flags are set on a rev 0 filesystem.
+	 */
+	if ((i = EXT3_HAS_INCOMPAT_FEATURE(sb, ~EXT3_FEATURE_INCOMPAT_SUPP))) {
+		printk(KERN_ERR "EXT3-fs: %s: couldn't mount because of "
+		       "unsupported optional features (%x).\n",
+		       bdevname(dev), i);
+		goto failed_mount;
+	}
+	if (!(sb->s_flags & MS_RDONLY) &&
+	    (i = EXT3_HAS_RO_COMPAT_FEATURE(sb, ~EXT3_FEATURE_RO_COMPAT_SUPP))){
+		printk(KERN_ERR "EXT3-fs: %s: couldn't mount RDWR because of "
+		       "unsupported optional features (%x).\n",
+		       bdevname(dev), i);
+		goto failed_mount;
+	}
+	sb->s_blocksize_bits = le32_to_cpu(es->s_log_block_size) + 10;
+	sb->s_blocksize = 1 << sb->s_blocksize_bits;
+
+	if (sb->s_blocksize < EXT3_MIN_BLOCK_SIZE ||
+	    sb->s_blocksize > EXT3_MAX_BLOCK_SIZE) {
+		printk(KERN_ERR 
+		       "EXT3-fs: Unsupported filesystem blocksize %d on %s.\n",
+		       blocksize, bdevname(dev));
+		goto failed_mount;
+	}
+
+	sb->s_maxbytes = ext3_max_size(sb->s_blocksize_bits);
+
+	if (sb->s_blocksize != blocksize) {
+		blocksize = sb->s_blocksize;
+
+		/*
+		 * Make sure the blocksize for the filesystem is larger
+		 * than the hardware sectorsize for the machine.
+		 */
+		if (sb->s_blocksize < hblock) {
+			printk(KERN_ERR "EXT3-fs: blocksize %d too small for "
+			       "device blocksize %d.\n", blocksize, hblock);
+			goto failed_mount;
+		}
+
+		brelse (bh);
+		set_blocksize (dev, sb->s_blocksize);
+		logic_sb_block = (sb_block * EXT3_MIN_BLOCK_SIZE) / blocksize;
+		offset = (sb_block * EXT3_MIN_BLOCK_SIZE) % blocksize;
+		bh = sb_bread(sb, logic_sb_block);
+		if (!bh) {
+			printk(KERN_ERR 
+			       "EXT3-fs: Can't read superblock on 2nd try.\n");
+			return NULL;
+		}
+		es = (struct ext3_super_block *)(((char *)bh->b_data) + offset);
+		sbi->s_es = es;
+		if (es->s_magic != le16_to_cpu(EXT3_SUPER_MAGIC)) {
+			printk (KERN_ERR 
+				"EXT3-fs: Magic mismatch, very weird !\n");
+			goto failed_mount;
+		}
+	}
+
+	if (le32_to_cpu(es->s_rev_level) == EXT3_GOOD_OLD_REV) {
+		sbi->s_inode_size = EXT3_GOOD_OLD_INODE_SIZE;
+		sbi->s_first_ino = EXT3_GOOD_OLD_FIRST_INO;
+	} else {
+		sbi->s_inode_size = le16_to_cpu(es->s_inode_size);
+		sbi->s_first_ino = le32_to_cpu(es->s_first_ino);
+		if (sbi->s_inode_size != EXT3_GOOD_OLD_INODE_SIZE) {
+			printk (KERN_ERR
+				"EXT3-fs: unsupported inode size: %d\n",
+				sbi->s_inode_size);
+			goto failed_mount;
+		}
+	}
+	sbi->s_frag_size = EXT3_MIN_FRAG_SIZE <<
+				   le32_to_cpu(es->s_log_frag_size);
+	if (blocksize != sbi->s_frag_size) {
+		printk(KERN_ERR
+		       "EXT3-fs: fragsize %lu != blocksize %u (unsupported)\n",
+		       sbi->s_frag_size, blocksize);
+		goto failed_mount;
+	}
+	sbi->s_frags_per_block = 1;
+	sbi->s_blocks_per_group = le32_to_cpu(es->s_blocks_per_group);
+	sbi->s_frags_per_group = le32_to_cpu(es->s_frags_per_group);
+	sbi->s_inodes_per_group = le32_to_cpu(es->s_inodes_per_group);
+	sbi->s_inodes_per_block = blocksize / EXT3_INODE_SIZE(sb);
+	sbi->s_itb_per_group = sbi->s_inodes_per_group /sbi->s_inodes_per_block;
+	sbi->s_desc_per_block = blocksize / sizeof(struct ext3_group_desc);
+	sbi->s_sbh = bh;
+	if (sbi->s_resuid == EXT3_DEF_RESUID)
+		sbi->s_resuid = le16_to_cpu(es->s_def_resuid);
+	if (sbi->s_resgid == EXT3_DEF_RESGID)
+		sbi->s_resgid = le16_to_cpu(es->s_def_resgid);
+	sbi->s_mount_state = le16_to_cpu(es->s_state);
+	sbi->s_addr_per_block_bits = log2(EXT3_ADDR_PER_BLOCK(sb));
+	sbi->s_desc_per_block_bits = log2(EXT3_DESC_PER_BLOCK(sb));
+
+	if (sbi->s_blocks_per_group > blocksize * 8) {
+		printk (KERN_ERR
+			"EXT3-fs: #blocks per group too big: %lu\n",
+			sbi->s_blocks_per_group);
+		goto failed_mount;
+	}
+	if (sbi->s_frags_per_group > blocksize * 8) {
+		printk (KERN_ERR
+			"EXT3-fs: #fragments per group too big: %lu\n",
+			sbi->s_frags_per_group);
+		goto failed_mount;
+	}
+	if (sbi->s_inodes_per_group > blocksize * 8) {
+		printk (KERN_ERR
+			"EXT3-fs: #inodes per group too big: %lu\n",
+			sbi->s_inodes_per_group);
+		goto failed_mount;
+	}
+
+	sbi->s_groups_count = (le32_to_cpu(es->s_blocks_count) -
+			       le32_to_cpu(es->s_first_data_block) +
+			       EXT3_BLOCKS_PER_GROUP(sb) - 1) /
+			      EXT3_BLOCKS_PER_GROUP(sb);
+	db_count = (sbi->s_groups_count + EXT3_DESC_PER_BLOCK(sb) - 1) /
+		   EXT3_DESC_PER_BLOCK(sb);
+	sbi->s_group_desc = kmalloc(db_count * sizeof (struct buffer_head *),
+				    GFP_KERNEL);
+	if (sbi->s_group_desc == NULL) {
+		printk (KERN_ERR "EXT3-fs: not enough memory\n");
+		goto failed_mount;
+	}
+	for (i = 0; i < db_count; i++) {
+		sbi->s_group_desc[i] = sb_bread(sb, logic_sb_block + i + 1);
+		if (!sbi->s_group_desc[i]) {
+			printk (KERN_ERR "EXT3-fs: "
+				"can't read group descriptor %d\n", i);
+			db_count = i;
+			goto failed_mount2;
+		}
+	}
+	if (!ext3_check_descriptors (sb)) {
+		printk (KERN_ERR "EXT3-fs: group descriptors corrupted !\n");
+		goto failed_mount2;
+	}
+	for (i = 0; i < EXT3_MAX_GROUP_LOADED; i++) {
+		sbi->s_inode_bitmap_number[i] = 0;
+		sbi->s_inode_bitmap[i] = NULL;
+		sbi->s_block_bitmap_number[i] = 0;
+		sbi->s_block_bitmap[i] = NULL;
+	}
+	sbi->s_loaded_inode_bitmaps = 0;
+	sbi->s_loaded_block_bitmaps = 0;
+	sbi->s_gdb_count = db_count;
+	get_random_bytes(&sbi->s_next_generation, sizeof(u32));
+	/*
+	 * set up enough so that it can read an inode
+	 */
+	sb->s_op = &ext3_sops;
+	INIT_LIST_HEAD(&sbi->s_orphan); /* unlinked but open files */
+
+	sb->s_root = 0;
+
+	needs_recovery = (es->s_last_orphan != 0 ||
+			  EXT3_HAS_INCOMPAT_FEATURE(sb,
+				    EXT3_FEATURE_INCOMPAT_RECOVER));
+
+	/*
+	 * The first inode we look at is the journal inode.  Don't try
+	 * root first: it may be modified in the journal!
+	 */
+	if (!test_opt(sb, NOLOAD) &&
+	    EXT3_HAS_COMPAT_FEATURE(sb, EXT3_FEATURE_COMPAT_HAS_JOURNAL)) {
+		if (ext3_load_journal(sb, es))
+			goto failed_mount2;
+	} else if (journal_inum) {
+		if (ext3_create_journal(sb, es, journal_inum))
+			goto failed_mount2;
+	} else {
+		if (!silent)
+			printk (KERN_ERR
+				"ext3: No journal on filesystem on %s\n",
+				bdevname(dev));
+		goto failed_mount2;
+	}
+
+	/* We have now updated the journal if required, so we can
+	 * validate the data journaling mode. */
+	switch (test_opt(sb, DATA_FLAGS)) {
+	case 0:
+		/* No mode set, assume a default based on the journal
+                   capabilities: ORDERED_DATA if the journal can
+                   cope, else JOURNAL_DATA */
+		if (journal_check_available_features
+		    (sbi->s_journal, 0, 0, JFS_FEATURE_INCOMPAT_REVOKE))
+			set_opt(sbi->s_mount_opt, ORDERED_DATA);
+		else
+			set_opt(sbi->s_mount_opt, JOURNAL_DATA);
+		break;
+
+	case EXT3_MOUNT_ORDERED_DATA:
+	case EXT3_MOUNT_WRITEBACK_DATA:
+		if (!journal_check_available_features
+		    (sbi->s_journal, 0, 0, JFS_FEATURE_INCOMPAT_REVOKE)) {
+			printk(KERN_ERR "EXT3-fs: Journal does not support "
+			       "requested data journaling mode\n");
+			goto failed_mount3;
+		}
+	default:
+		break;
+	}
+
+	/*
+	 * The journal_load will have done any necessary log recovery,
+	 * so we can safely mount the rest of the filesystem now.
+	 */
+
+	sb->s_root = d_alloc_root(iget(sb, EXT3_ROOT_INO));
+	if (!sb->s_root || !S_ISDIR(sb->s_root->d_inode->i_mode) ||
+	    !sb->s_root->d_inode->i_blocks || !sb->s_root->d_inode->i_size) {
+		if (sb->s_root) {
+			dput(sb->s_root);
+			sb->s_root = NULL;
+			printk(KERN_ERR
+			       "EXT3-fs: corrupt root inode, run e2fsck\n");
+		} else
+			printk(KERN_ERR "EXT3-fs: get root inode failed\n");
+		goto failed_mount3;
+	}
+
+	ext3_setup_super (sb, es, sb->s_flags & MS_RDONLY);
+	/*
+	 * akpm: core read_super() calls in here with the superblock locked.
+	 * That deadlocks, because orphan cleanup needs to lock the superblock
+	 * in numerous places.  Here we just pop the lock - it's relatively
+	 * harmless, because we are now ready to accept write_super() requests,
+	 * and aviro says that's the only reason for hanging onto the
+	 * superblock lock.
+	 */
+	EXT3_SB(sb)->s_mount_state |= EXT3_ORPHAN_FS;
+	unlock_super(sb);	/* akpm: sigh */
+	ext3_orphan_cleanup(sb, es);
+	lock_super(sb);
+	EXT3_SB(sb)->s_mount_state &= ~EXT3_ORPHAN_FS;
+	if (needs_recovery)
+		printk (KERN_INFO "EXT3-fs: recovery complete.\n");
+	ext3_mark_recovery_complete(sb, es);
+	printk (KERN_INFO "EXT3-fs: mounted filesystem with %s data mode.\n",
+		test_opt(sb,DATA_FLAGS) == EXT3_MOUNT_JOURNAL_DATA ? "journal":
+		test_opt(sb,DATA_FLAGS) == EXT3_MOUNT_ORDERED_DATA ? "ordered":
+		"writeback");
+
+	return sb;
+
+failed_mount3:
+	journal_destroy(sbi->s_journal);
+failed_mount2:
+	for (i = 0; i < db_count; i++)
+		brelse(sbi->s_group_desc[i]);
+	kfree(sbi->s_group_desc);
+failed_mount:
+	ext3_blkdev_remove(sbi);
+	brelse(bh);
+out_fail:
+	return NULL;
+}
+
+static journal_t *ext3_get_journal(struct super_block *sb, int journal_inum)
+{
+	struct inode *journal_inode;
+	journal_t *journal;
+
+	/* First, test for the existence of a valid inode on disk.  Bad
+	 * things happen if we iget() an unused inode, as the subsequent
+	 * iput() will try to delete it. */
+
+	journal_inode = iget(sb, journal_inum);
+	if (!journal_inode) {
+		printk(KERN_ERR "EXT3-fs: no journal found.\n");
+		return NULL;
+	}
+	if (!journal_inode->i_nlink) {
+		make_bad_inode(journal_inode);
+		iput(journal_inode);
+		printk(KERN_ERR "EXT3-fs: journal inode is deleted.\n");
+		return NULL;
+	}
+
+	jbd_debug(2, "Journal inode found at %p: %Ld bytes\n",
+		  journal_inode, journal_inode->i_size);
+	if (is_bad_inode(journal_inode) || !S_ISREG(journal_inode->i_mode)) {
+		printk(KERN_ERR "EXT3-fs: invalid journal inode.\n");
+		iput(journal_inode);
+		return NULL;
+	}
+
+	journal = journal_init_inode(journal_inode);
+	if (!journal) {
+		printk(KERN_ERR "EXT3-fs: Could not load journal inode\n");
+		iput(journal_inode);
+	}
+	
+	return journal;
+}
+
+static journal_t *ext3_get_dev_journal(struct super_block *sb,
+				       int dev)
+{
+	struct buffer_head * bh;
+	journal_t *journal;
+	int start;
+	int len;
+	int hblock, blocksize;
+	unsigned long sb_block;
+	unsigned long offset;
+	kdev_t journal_dev = to_kdev_t(dev);
+	struct ext3_super_block * es;
+	struct block_device *bdev;
+
+	bdev = ext3_blkdev_get(journal_dev);
+	if (bdev == NULL)
+		return NULL;
+
+	blocksize = sb->s_blocksize;
+	hblock = get_hardsect_size(journal_dev);
+	if (blocksize < hblock) {
+		printk(KERN_ERR
+			"EXT3-fs: blocksize too small for journal device.\n");
+		goto out_bdev;
+	}
+	
+	sb_block = EXT3_MIN_BLOCK_SIZE / blocksize;
+	offset = EXT3_MIN_BLOCK_SIZE % blocksize;
+	set_blocksize(dev, blocksize);
+	if (!(bh = bread(dev, sb_block, blocksize))) {
+		printk(KERN_ERR "EXT3-fs: couldn't read superblock of "
+		       "external journal\n");
+		goto out_bdev;
+	}
+
+	es = (struct ext3_super_block *) (((char *)bh->b_data) + offset);
+	if ((le16_to_cpu(es->s_magic) != EXT3_SUPER_MAGIC) ||
+	    !(le32_to_cpu(es->s_feature_incompat) &
+	      EXT3_FEATURE_INCOMPAT_JOURNAL_DEV)) {
+		printk(KERN_ERR "EXT3-fs: external journal has "
+					"bad superblock\n");
+		brelse(bh);
+		goto out_bdev;
+	}
+
+	if (memcmp(EXT3_SB(sb)->s_es->s_journal_uuid, es->s_uuid, 16)) {
+		printk(KERN_ERR "EXT3-fs: journal UUID does not match\n");
+		brelse(bh);
+		goto out_bdev;
+	}
+
+	len = le32_to_cpu(es->s_blocks_count);
+	start = sb_block + 1;
+	brelse(bh);	/* we're done with the superblock */
+
+	journal = journal_init_dev(journal_dev, sb->s_dev, 
+					start, len, blocksize);
+	if (!journal) {
+		printk(KERN_ERR "EXT3-fs: failed to create device journal\n");
+		goto out_bdev;
+	}
+	ll_rw_block(READ, 1, &journal->j_sb_buffer);
+	wait_on_buffer(journal->j_sb_buffer);
+	if (!buffer_uptodate(journal->j_sb_buffer)) {
+		printk(KERN_ERR "EXT3-fs: I/O error on journal device\n");
+		goto out_journal;
+	}
+	if (ntohl(journal->j_superblock->s_nr_users) != 1) {
+		printk(KERN_ERR "EXT3-fs: External journal has more than one "
+					"user (unsupported) - %d\n",
+			ntohl(journal->j_superblock->s_nr_users));
+		goto out_journal;
+	}
+	EXT3_SB(sb)->journal_bdev = bdev;
+	return journal;
+out_journal:
+	journal_destroy(journal);
+out_bdev:
+	ext3_blkdev_put(bdev);
+	return NULL;
+}
+
+static int ext3_load_journal(struct super_block * sb,
+			     struct ext3_super_block * es)
+{
+	journal_t *journal;
+	int journal_inum = le32_to_cpu(es->s_journal_inum);
+	int journal_dev = le32_to_cpu(es->s_journal_dev);
+	int err = 0;
+	int really_read_only;
+
+	really_read_only = is_read_only(sb->s_dev);
+
+	/*
+	 * Are we loading a blank journal or performing recovery after a
+	 * crash?  For recovery, we need to check in advance whether we
+	 * can get read-write access to the device.
+	 */
+
+	if (EXT3_HAS_INCOMPAT_FEATURE(sb, EXT3_FEATURE_INCOMPAT_RECOVER)) {
+		if (sb->s_flags & MS_RDONLY) {
+			printk(KERN_INFO "EXT3-fs: INFO: recovery "
+					"required on readonly filesystem.\n");
+			if (really_read_only) {
+				printk(KERN_ERR "EXT3-fs: write access "
+					"unavailable, cannot proceed.\n");
+				return -EROFS;
+			}
+			printk (KERN_INFO "EXT3-fs: write access will "
+					"be enabled during recovery.\n");
+		}
+	}
+
+	if (journal_inum && journal_dev) {
+		printk(KERN_ERR "EXT3-fs: filesystem has both journal "
+		       "and inode journals!\n");
+		return -EINVAL;
+	}
+
+	if (journal_inum) {
+		if (!(journal = ext3_get_journal(sb, journal_inum)))
+			return -EINVAL;
+	} else {
+		if (!(journal = ext3_get_dev_journal(sb, journal_dev)))
+			return -EINVAL;
+	}
+	
+
+	if (!really_read_only && test_opt(sb, UPDATE_JOURNAL)) {
+		err = journal_update_format(journal);
+		if (err)  {
+			printk(KERN_ERR "EXT3-fs: error updating journal.\n");
+			journal_destroy(journal);
+			return err;
+		}
+	}
+
+	if (!EXT3_HAS_INCOMPAT_FEATURE(sb, EXT3_FEATURE_INCOMPAT_RECOVER))
+		err = journal_wipe(journal, !really_read_only);
+	if (!err)
+		err = journal_load(journal);
+
+	if (err) {
+		printk(KERN_ERR "EXT3-fs: error loading journal.\n");
+		journal_destroy(journal);
+		return err;
+	}
+
+	EXT3_SB(sb)->s_journal = journal;
+	ext3_clear_journal_err(sb, es);
+	return 0;
+}
+
+static int ext3_create_journal(struct super_block * sb,
+			       struct ext3_super_block * es,
+			       int journal_inum)
+{
+	journal_t *journal;
+
+	if (sb->s_flags & MS_RDONLY) {
+		printk(KERN_ERR "EXT3-fs: readonly filesystem when trying to "
+				"create journal.\n");
+		return -EROFS;
+	}
+
+	if (!(journal = ext3_get_journal(sb, journal_inum)))
+		return -EINVAL;
+
+	printk(KERN_INFO "EXT3-fs: creating new journal on inode %d\n",
+	       journal_inum);
+
+	if (journal_create(journal)) {
+		printk(KERN_ERR "EXT3-fs: error creating journal.\n");
+		journal_destroy(journal);
+		return -EIO;
+	}
+
+	EXT3_SB(sb)->s_journal = journal;
+
+	ext3_update_dynamic_rev(sb);
+	EXT3_SET_INCOMPAT_FEATURE(sb, EXT3_FEATURE_INCOMPAT_RECOVER);
+	EXT3_SET_COMPAT_FEATURE(sb, EXT3_FEATURE_COMPAT_HAS_JOURNAL);
+
+	es->s_journal_inum = cpu_to_le32(journal_inum);
+	sb->s_dirt = 1;
+
+	/* Make sure we flush the recovery flag to disk. */
+	ext3_commit_super(sb, es, 1);
+
+	return 0;
+}
+
+static void ext3_commit_super (struct super_block * sb,
+			       struct ext3_super_block * es,
+			       int sync)
+{
+	es->s_wtime = cpu_to_le32(CURRENT_TIME);
+	BUFFER_TRACE(sb->u.ext3_sb.s_sbh, "marking dirty");
+	mark_buffer_dirty(sb->u.ext3_sb.s_sbh);
+	if (sync) {
+		ll_rw_block(WRITE, 1, &sb->u.ext3_sb.s_sbh);
+		wait_on_buffer(sb->u.ext3_sb.s_sbh);
+	}
+}
+
+
+/*
+ * Have we just finished recovery?  If so, and if we are mounting (or
+ * remounting) the filesystem readonly, then we will end up with a
+ * consistent fs on disk.  Record that fact.
+ */
+static void ext3_mark_recovery_complete(struct super_block * sb,
+					struct ext3_super_block * es)
+{
+	journal_flush(EXT3_SB(sb)->s_journal);
+	if (EXT3_HAS_INCOMPAT_FEATURE(sb, EXT3_FEATURE_INCOMPAT_RECOVER) &&
+	    sb->s_flags & MS_RDONLY) {
+		EXT3_CLEAR_INCOMPAT_FEATURE(sb, EXT3_FEATURE_INCOMPAT_RECOVER);
+		sb->s_dirt = 0;
+		ext3_commit_super(sb, es, 1);
+	}
+}
+
+/*
+ * If we are mounting (or read-write remounting) a filesystem whose journal
+ * has recorded an error from a previous lifetime, move that error to the
+ * main filesystem now.
+ */
+static void ext3_clear_journal_err(struct super_block * sb,
+				   struct ext3_super_block * es)
+{
+	journal_t *journal;
+	int j_errno;
+	const char *errstr;
+	
+	journal = EXT3_SB(sb)->s_journal;
+
+	/*
+	 * Now check for any error status which may have been recorded in the
+	 * journal by a prior ext3_error() or ext3_abort()
+	 */
+
+	j_errno = journal_errno(journal);
+	if (j_errno) {
+		char nbuf[16];
+		
+		errstr = ext3_decode_error(sb, j_errno, nbuf);
+		ext3_warning(sb, __FUNCTION__, "Filesystem error recorded "
+			     "from previous mount: %s", errstr);
+		ext3_warning(sb, __FUNCTION__, "Marking fs in need of "
+			     "filesystem check.");
+		
+		sb->u.ext3_sb.s_mount_state |= EXT3_ERROR_FS;
+		es->s_state |= cpu_to_le16(EXT3_ERROR_FS);
+		ext3_commit_super (sb, es, 1);
+
+		journal_clear_err(journal);
+	}
+}
+
+/*
+ * Force the running and committing transactions to commit,
+ * and wait on the commit.
+ */
+int ext3_force_commit(struct super_block *sb)
+{
+	journal_t *journal;
+	int ret;
+
+	if (sb->s_flags & MS_RDONLY)
+		return 0;
+
+	journal = EXT3_SB(sb)->s_journal;
+	sb->s_dirt = 0;
+	lock_kernel();	/* important: lock down j_running_transaction */
+	ret = ext3_journal_force_commit(journal);
+	unlock_kernel();
+	return ret;
+}
+
+/*
+ * Ext3 always journals updates to the superblock itself, so we don't
+ * have to propagate any other updates to the superblock on disk at this
+ * point.  Just start an async writeback to get the buffers on their way
+ * to the disk.
+ *
+ * This implicitly triggers the writebehind on sync().
+ */
+
+static int do_sync_supers = 0;
+MODULE_PARM(do_sync_supers, "i");
+MODULE_PARM_DESC(do_sync_supers, "Write superblocks synchronously");
+
+void ext3_write_super (struct super_block * sb)
+{
+	tid_t target;
+	
+	if (down_trylock(&sb->s_lock) == 0)
+		BUG();		/* aviro detector */
+	sb->s_dirt = 0;
+	target = log_start_commit(EXT3_SB(sb)->s_journal, NULL);
+
+	if (do_sync_supers) {
+		unlock_super(sb);
+		log_wait_commit(EXT3_SB(sb)->s_journal, target);
+		lock_super(sb);
+	}
+}
+
+/*
+ * LVM calls this function before a (read-only) snapshot is created.  This
+ * gives us a chance to flush the journal completely and mark the fs clean.
+ */
+void ext3_write_super_lockfs(struct super_block *sb)
+{
+	sb->s_dirt = 0;
+
+	lock_kernel();		/* 2.4.5 forgot to do this for us */
+	if (!(sb->s_flags & MS_RDONLY)) {
+		journal_t *journal = EXT3_SB(sb)->s_journal;
+
+		/* Now we set up the journal barrier. */
+		journal_lock_updates(journal);
+		journal_flush(journal);
+
+		/* Journal blocked and flushed, clear needs_recovery flag. */
+		EXT3_CLEAR_INCOMPAT_FEATURE(sb, EXT3_FEATURE_INCOMPAT_RECOVER);
+		ext3_commit_super(sb, EXT3_SB(sb)->s_es, 1);
+	}
+	unlock_kernel();
+}
+
+/*
+ * Called by LVM after the snapshot is done.  We need to reset the RECOVER
+ * flag here, even though the filesystem is not technically dirty yet.
+ */
+void ext3_unlockfs(struct super_block *sb)
+{
+	if (!(sb->s_flags & MS_RDONLY)) {
+		lock_kernel();
+		lock_super(sb);
+		/* Reser the needs_recovery flag before the fs is unlocked. */
+		EXT3_SET_INCOMPAT_FEATURE(sb, EXT3_FEATURE_INCOMPAT_RECOVER);
+		ext3_commit_super(sb, EXT3_SB(sb)->s_es, 1);
+		unlock_super(sb);
+		journal_unlock_updates(EXT3_SB(sb)->s_journal);
+		unlock_kernel();
+	}
+}
+
+int ext3_remount (struct super_block * sb, int * flags, char * data)
+{
+	struct ext3_super_block * es;
+	struct ext3_sb_info *sbi = EXT3_SB(sb);
+	unsigned long tmp;
+
+	clear_ro_after(sb);
+
+	/*
+	 * Allow the "check" option to be passed as a remount option.
+	 */
+	if (!parse_options(data, &tmp, sbi, &tmp, 1))
+		return -EINVAL;
+
+	if (sbi->s_mount_opt & EXT3_MOUNT_ABORT)
+		ext3_abort(sb, __FUNCTION__, "Abort forced by user");
+
+	es = sbi->s_es;
+
+	if ((*flags & MS_RDONLY) != (sb->s_flags & MS_RDONLY)) {
+		if (sbi->s_mount_opt & EXT3_MOUNT_ABORT)
+			return -EROFS;
+
+		if (*flags & MS_RDONLY) {
+			/*
+			 * First of all, the unconditional stuff we have to do
+			 * to disable replay of the journal when we next remount
+			 */
+			sb->s_flags |= MS_RDONLY;
+
+			/*
+			 * OK, test if we are remounting a valid rw partition
+			 * readonly, and if so set the rdonly flag and then
+			 * mark the partition as valid again.
+			 */
+			if (!(es->s_state & cpu_to_le16(EXT3_VALID_FS)) &&
+			    (sbi->s_mount_state & EXT3_VALID_FS))
+				es->s_state = cpu_to_le16(sbi->s_mount_state);
+
+			ext3_mark_recovery_complete(sb, es);
+		} else {
+			int ret;
+			if ((ret = EXT3_HAS_RO_COMPAT_FEATURE(sb,
+					~EXT3_FEATURE_RO_COMPAT_SUPP))) {
+				printk(KERN_WARNING "EXT3-fs: %s: couldn't "
+				       "remount RDWR because of unsupported "
+				       "optional features (%x).\n",
+				       bdevname(sb->s_dev), ret);
+				return -EROFS;
+			}
+			/*
+			 * Mounting a RDONLY partition read-write, so reread
+			 * and store the current valid flag.  (It may have
+			 * been changed by e2fsck since we originally mounted
+			 * the partition.)
+			 */
+			ext3_clear_journal_err(sb, es);
+			sbi->s_mount_state = le16_to_cpu(es->s_state);
+			if (!ext3_setup_super (sb, es, 0))
+				sb->s_flags &= ~MS_RDONLY;
+		}
+	}
+	setup_ro_after(sb);
+	return 0;
+}
+
+int ext3_statfs (struct super_block * sb, struct statfs * buf)
+{
+	struct ext3_super_block *es = EXT3_SB(sb)->s_es;
+	unsigned long overhead;
+	int i;
+
+	if (test_opt (sb, MINIX_DF))
+		overhead = 0;
+	else {
+		/*
+		 * Compute the overhead (FS structures)
+		 */
+
+		/*
+		 * All of the blocks before first_data_block are
+		 * overhead
+		 */
+		overhead = le32_to_cpu(es->s_first_data_block);
+
+		/*
+		 * Add the overhead attributed to the superblock and
+		 * block group descriptors.  If the sparse superblocks
+		 * feature is turned on, then not all groups have this.
+		 */
+		for (i = 0; i < EXT3_SB(sb)->s_groups_count; i++)
+			overhead += ext3_bg_has_super(sb, i) +
+				ext3_bg_num_gdb(sb, i);
+
+		/*
+		 * Every block group has an inode bitmap, a block
+		 * bitmap, and an inode table.
+		 */
+		overhead += (EXT3_SB(sb)->s_groups_count *
+			     (2 + EXT3_SB(sb)->s_itb_per_group));
+	}
+
+	buf->f_type = EXT3_SUPER_MAGIC;
+	buf->f_bsize = sb->s_blocksize;
+	buf->f_blocks = le32_to_cpu(es->s_blocks_count) - overhead;
+	buf->f_bfree = ext3_count_free_blocks (sb);
+	buf->f_bavail = buf->f_bfree - le32_to_cpu(es->s_r_blocks_count);
+	if (buf->f_bfree < le32_to_cpu(es->s_r_blocks_count))
+		buf->f_bavail = 0;
+	buf->f_files = le32_to_cpu(es->s_inodes_count);
+	buf->f_ffree = ext3_count_free_inodes (sb);
+	buf->f_namelen = EXT3_NAME_LEN;
+	return 0;
+}
+
+static DECLARE_FSTYPE_DEV(ext3_fs_type, "ext3", ext3_read_super);
+
+static int __init init_ext3_fs(void)
+{
+        return register_filesystem(&ext3_fs_type);
+}
+
+static void __exit exit_ext3_fs(void)
+{
+	unregister_filesystem(&ext3_fs_type);
+}
+
+EXPORT_NO_SYMBOLS;
+
+MODULE_AUTHOR("Remy Card, Stephen Tweedie, Andrew Morton, Andreas Dilger, Theodore Ts'o and others");
+MODULE_DESCRIPTION("Second Extended Filesystem with journaling extensions");
+MODULE_LICENSE("GPL");
+module_init(init_ext3_fs)
+module_exit(exit_ext3_fs)
diff -rup --new-file linux.mcp2/fs/ext3/symlink.c linux_tmp/fs/ext3/symlink.c
--- linux.mcp2/fs/ext3/symlink.c	1969-12-31 16:00:00.000000000 -0800
+++ linux_tmp/fs/ext3/symlink.c	2001-11-09 14:25:04.000000000 -0800
@@ -0,0 +1,39 @@
+/*
+ *  linux/fs/ext3/symlink.c
+ *
+ * Only fast symlinks left here - the rest is done by generic code. AV, 1999
+ *
+ * Copyright (C) 1992, 1993, 1994, 1995
+ * Remy Card (card@masi.ibp.fr)
+ * Laboratoire MASI - Institut Blaise Pascal
+ * Universite Pierre et Marie Curie (Paris VI)
+ *
+ *  from
+ *
+ *  linux/fs/minix/symlink.c
+ *
+ *  Copyright (C) 1991, 1992  Linus Torvalds
+ *
+ *  ext3 symlink handling code
+ */
+
+#include <linux/fs.h>
+#include <linux/jbd.h>
+#include <linux/ext3_fs.h>
+
+static int ext3_readlink(struct dentry *dentry, char *buffer, int buflen)
+{
+	char *s = (char *)dentry->d_inode->u.ext3_i.i_data;
+	return vfs_readlink(dentry, buffer, buflen, s);
+}
+
+static int ext3_follow_link(struct dentry *dentry, struct nameidata *nd)
+{
+	char *s = (char *)dentry->d_inode->u.ext3_i.i_data;
+	return vfs_follow_link(nd, s);
+}
+
+struct inode_operations ext3_fast_symlink_inode_operations = {
+	readlink:	ext3_readlink,		/* BKL not held.  Don't need */
+	follow_link:	ext3_follow_link,	/* BKL not held.  Don't need */
+};
